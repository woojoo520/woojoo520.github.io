<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="a study blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Celery Fairy">
<meta property="og:url" content="https://woojoo520.github.io/page/7/index.html">
<meta property="og:site_name" content="Celery Fairy">
<meta property="og:description" content="a study blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Celery Fairy">
<meta name="twitter:description" content="a study blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://woojoo520.github.io/page/7/">





  <title>Celery Fairy</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <a href="https://github.com/woojoo520" class="github-corner" aria-label="View source on GitHub">
      <svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/>
      </svg>
    </a>
    <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Celery Fairy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Celery's Blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/07/Inception/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/Inception/" itemprop="url">Inception</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-07T12:55:01+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Inception-v1"><a href="#Inception-v1" class="headerlink" title="Inception-v1"></a>Inception-v1</h3><p>在这篇论文之前，卷积神经网络的性能高都是依赖于提高网络的深度和宽度，而这篇论文是从网络结构上入手，改变了网络结构</p>
<h4 id="为什么提出Inception？"><a href="#为什么提出Inception？" class="headerlink" title="为什么提出Inception？"></a>为什么提出Inception？</h4><p>提高网络最简单粗暴的方法就是提高网络的深度和宽度，即增加隐层以及各层神经元数目。但这种简单粗暴的方法存在一些问题：</p>
<ul>
<li>会导致更大的参数空间，更容易过拟合</li>
<li>需要更多的计算资源</li>
<li>网络越深，梯度容易消失，优化困难（这时还没有提出BN，网络优化极其困难）</li>
</ul>
<p>基于此，我们的目标就是，提高网络计算资源的利用率，在计算率不变的情况下，提高网络的宽度和深度</p>
<p>作者认为，解决这种困难的方法就是，把全连接改成稀疏连接，卷积层也是稀疏连接，但是不对称稀疏数据数值计算效率低下，因为硬件全是针对密集矩阵优化的，所以，我们要找到卷积网络可以近似优化的最优局部稀疏结构，并且该结构下可以用现有的密度矩阵计算硬件实现，产生的结果就是Inception</p>
<h4 id="Inception结构"><a href="#Inception结构" class="headerlink" title="Inception结构"></a>Inception结构</h4><p><img src="https://img-blog.csdn.net/20180709151519421?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk1MzUwMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>首先看第一个结构，有四个通道，有1 * 1、3 * 3、5 * 5卷积核，该结构有几个特点：</p>
<ul>
<li>使用这些大小卷积核，没有什么特殊含义，主要方便对齐，只要padding = 0、1、2，就可以得到相同大小的特征图，可以顺利concat</li>
<li>采用大小不同的卷积核，以为着感受野的大小不同，就可以得到不同尺度的特征</li>
<li>采用比较大的卷积核，即5 * 5，意味着有些相关性可能隔得比较远，用大的卷积核才能学到此特征</li>
</ul>
<p>当时，这个结构有个缺点，5 * 5的卷积核的计算量太大。那么作者想到的第二个结构，用1 * 1的卷积核进行降维。</p>
<p>这个1 * 1的卷积核，它的作用就是：</p>
<ul>
<li>降低维度，减少计算瓶颈</li>
<li>增加网络层数，提高网络的表达能力。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/07/softmax/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/softmax/" itemprop="url">softmax</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-07T10:52:19+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="softmax交叉熵损失函数求导"><a href="#softmax交叉熵损失函数求导" class="headerlink" title="softmax交叉熵损失函数求导"></a>softmax交叉熵损失函数求导</h3><p>softmax经常被添加在分类任务的神经网络的输出层中，神经网络的反向传播中关键的步骤就是求导。</p>
<p>softmax函数：</p>
<p>一般在神经网络中，softmax可以作为分类任务的输出层。其实可以认为softmax输出的是几个类别选择的概率，比如我有一个分类任务，要分为三个类，softmax函数可以根据他们的相对大小，输出三个类别选取的概率，并且概率和为1。</p>
<p>公式：$S_{i}=\frac{e^{z_{i}}}{\sum_{k} e^{z_{k}}}$</p>
<p>$S_{}i$ 代表的是第i个神经元的输出，其实就是在输出后面套一个这个函数</p>
<p>首先是一个神经元的输出，一个神经元如下图：</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1565146619562.png" alt="1565146619562"></p>
<p>神经元的输出设为：$z_{i}=\sum_{j} w_{i j} x_{i j}+b$</p>
<p>其中 $w_{ij}$ 是第i个神经元的第j个权重，b是偏移值， $z_{i}$ 表示该网络的第i个输出</p>
<p>给这个输出加上一个softmax函数，就是$a_{i}=\frac{e^{z_{i}}}{\sum_{k} e^{z_{k}}}$</p>
<p>其中 $a_{i}$ 代表softmax的第i个输出值，右侧就是套用了softmax函数</p>
<h4 id="损失函数-loss-function"><a href="#损失函数-loss-function" class="headerlink" title="损失函数 loss function"></a>损失函数 loss function</h4><p>在神经网络反向传播中，要求一个损失函数，这个损失函数其实表示的是真实值与网络的估计值的误差，知道误差了，才能知道怎样去修改网络中的权重。</p>
<p>损失函数可以有很多形式，这里使用的是交叉熵函数，主要是由于这个求导结果比较简单，易于计算，并且交叉熵解决某些损失函数学习缓慢的问题。交叉熵的函数是这样的：$C=-\sum_{i} y_{i} \ln a_{i}$</p>
<p>其中 $y_{i}$ 表示真实的分类结果。</p>
<h4 id="推导过程："><a href="#推导过程：" class="headerlink" title="推导过程："></a>推导过程：</h4><p>首先，我们要明确一下我们要求什么，我们要求的是我们的loss对于神经元输出( $ z_{i}$ )的梯度，即：$\frac{\partial C}{\partial z_{i}}$</p>
<p>根据复合函数求导法则：$\frac{\partial C}{\partial z_{i}}=\sum_{j}\left(\frac{\partial C_{j}}{\partial a_{j}} \frac{\partial a_{j}}{\partial z_{i}}\right)$</p>
<p>这里为什么是 $a_{j}$ 而不是 $a_{i}$ ，这里要看一下softmax的公式了，因为softmax公式的特性，它的分母包含了所有神经元的输出，所以，对于不等于i的其他输出里面，也包含着 $z_{i}$，所有的a都要纳入到计算范围中，并且后面的计算可以看到需要分为 $i = j$ 和 $i \not= j$ 两种情况求导。</p>
<p>下面我们一个一个推：</p>
<p>$\frac{\partial C_{j}}{\partial a_{j}}=\frac{\partial\left(-y_{j} \ln a_{j}\right)}{\partial a_{j}}=-y_{j} \frac{1}{a_{j}}$</p>
<p>第二个稍微复杂一点，我们先把他分为两种情况：</p>
<ol>
<li><p>如果 $i = j$ :</p>
<p>$\frac{\partial a_{i}}{\partial z_{i}}=\frac{\partial\left(\frac{e^{z_{i}}}{\sum_{k} e^{z_{k}}}\right)}{\partial z_{i}}=\frac{\sum_{k} e^{z_{k}} e^{z_{i}}-\left(e^{z_{i}}\right)^{2}}{\left(\sum_{k} e^{z_{k}}\right)^{2}}=\left(\frac{e^{z_{i}}}{\sum_{k} e^{z_{k}}}\right)\left(1-\frac{e^{z_{i}}}{\sum_{k} e^{z_{k}}}\right)=a_{i}\left(1-a_{i}\right)$</p>
</li>
<li><p>如果 $i \not= j$ :</p>
<p>$\frac{\partial a_{j}}{\partial z_{i}}=\frac{\partial\left(\frac{e^{z_{j}}}{\sum_{k} e^{z_{k}}}\right)}{\partial z_{i}}=-e^{z_{j}}\left(\frac{1}{\sum_{k} e^{z_{k}}}\right)^{2} e^{z_{i}}=-a_{i} a_{j}$</p>
</li>
</ol>
<p>接下来，只需要组合两个式子：</p>
<p>$\frac{\partial C}{\partial z_{i}}=\sum_{j}\left(\frac{\partial C_{j}}{\partial a_{j}} \frac{\partial a_{j}}{\partial z_{i}}\right)=\sum_{j=\dot{\psi}}\left(\frac{\partial C_{j}}{\partial a_{j}} \frac{\partial a_{j}}{\partial z_{i}}\right)+\sum_{i=j}\left(\frac{\partial C_{j}}{\partial a_{j}} \frac{\partial a_{j}}{\partial z_{i}}\right)$</p>
<p>$=\sum_{j=\dot{y}}-y_{j} \frac{1}{a_{j}}\left(-a_{i} a_{j}\right)+\left(-y_{i} \frac{1}{a_{i}}\right)\left(a_{i}\left(1-a_{i}\right)\right)$</p>
<p>$=\sum_{j=i} a_{i} y_{j}+\left(-y_{i}\left(1-a_{i}\right)\right)$</p>
<p>$=\sum_{j=\dot{\psi}} a_{i} y_{j}+a_{i} y_{i}-y_{i}$</p>
<p>$=a_{i} \sum_{j} y_{j}-y_{i}$</p>
<p>最后的结果看起来简单了很多，最后，针对分类问题，我们给定的结果 $y_{i}$ 最终只会哟一个类别是1，其他类别都是0，因此对于分类问题，这个梯度等于：</p>
<p>$\frac{\partial C}{\partial z_{i}}=a_{i}-y_{i}$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/07/29/OverTheWall/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/29/OverTheWall/" itemprop="url">OverTheWall</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-29T19:38:03+08:00">
                2019-07-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="匿名术"><a href="#匿名术" class="headerlink" title="匿名术"></a>匿名术</h3><ul>
<li><p>网络层面：隐藏公网IP</p>
</li>
<li><p>操作系统层面</p>
</li>
<li><p>个人软件层面</p>
</li>
<li><p>通讯工具层面</p>
</li>
<li><p>通讯工具层面</p>
</li>
</ul>
<h3 id="DNS欺骗（DNS污染）"><a href="#DNS欺骗（DNS污染）" class="headerlink" title="DNS欺骗（DNS污染）"></a>DNS欺骗（DNS污染）</h3><p>DNS欺骗又称DNS Spoofing，DNS污染又称为“域名缓存投毒”。如果你使用的是翻墙外的DNS服务器，那么，你每次进行DNS查询，都会要经过国际出口。这时候GFW会通过技术手段伪造DNS的查询结果——使得你查询到的网站IP是错误的。如此一来，你自然就无法访问该网站。</p>
<p>DNS劫持不同于DNS污染。DNS劫持是指DNS服务器上的记录被认为修改成错误的，把某些敏感网站的记录修改成错误的。</p>
<h3 id="代理软件"><a href="#代理软件" class="headerlink" title="代理软件"></a>代理软件</h3><h4 id="代理的工作原理"><a href="#代理的工作原理" class="headerlink" title="代理的工作原理"></a>代理的工作原理</h4><p>假设你想通过翻墙代理访问某个被墙的网站，这时候会经历如下几个步骤：</p>
<ul>
<li>你的上网软件（通常是浏览器）会把数据发送给你电脑中的代理工具</li>
<li>该工具把数据进行<strong>加密</strong>，然后发送给<strong>国外</strong>的某个代理服务器</li>
<li>该代理服务器吧数据解密，然后发送给你要访问的网站</li>
<li>从该网站回传的数据，也是经过上述过程，最终回到你的浏览器</li>
</ul>
<h4 id="代理的分类"><a href="#代理的分类" class="headerlink" title="代理的分类"></a>代理的分类</h4><p>按照是否加密，代理软件可分为加密代理和不加密代理两种</p>
<p>按照协议类型，常见的有HTTP和SOCKS代理。如果你纯粹用浏览器翻墙，HTTP代理就够用了。如果还需要使用其他软件翻墙，那就得使用SOCKS代理</p>
<h4 id="代理工具的获取"><a href="#代理工具的获取" class="headerlink" title="代理工具的获取"></a>代理工具的获取</h4>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/07/29/论文中的Python知识点/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/29/论文中的Python知识点/" itemprop="url">论文中的Python知识点</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-29T09:51:31+08:00">
                2019-07-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1-元类"><a href="#1-元类" class="headerlink" title="1. 元类"></a>1. 元类</h3><p>首先理解python中的类，用class修饰的都可以叫做类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Class</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">c = Class()</span><br><span class="line">Class.b = <span class="number">2</span></span><br><span class="line">print(c.b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># out 2</span></span><br></pre></td></tr></table></figure>

<p>我们平时用的类都是实例化以后的类，可以在任何时候动态的创建类，通常情况我们都是这样c=Class(),python解释器会将它认为是创建类，可是解释器本身是如何创建类的，答案是利用type</p>
<p>type平时我们可能认为是查看对象的类型，例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(type(c))</span><br><span class="line"><span class="comment"># out:</span></span><br><span class="line"><span class="comment"># &lt;class '__main__.Class'&gt;</span></span><br><span class="line">print(type(Class))</span><br><span class="line"><span class="comment"># out:</span></span><br><span class="line"><span class="comment"># &lt;class 'type'&gt;</span></span><br></pre></td></tr></table></figure>

<p>所以，Class的类型是type，我们可以用type直接生成一个类</p>
<p>type(类名, 父类的元组（针对继承的情况，可以为空），包含属性的字典（名称和值）)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Class_type = type(<span class="string">'Class_type'</span>, (), &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;)</span><br><span class="line">c_t = Class_type()</span><br><span class="line">print(c_t.a)</span><br><span class="line"><span class="comment"># out：1</span></span><br></pre></td></tr></table></figure>

<p><strong>元类</strong>：就是用来创建这些类（对象）的类，元类就是类的类</p>
<p>type就是所有类的元类，可以理解为所有的类都是由type创建的，我们也可以创建自己的元类，这个类需要继承在type</p>
<p>__metaclass__属性</p>
<p>Class中有<em>\</em>metaclass__这个属性吗？如果是，Python会在内存中通过<em>\</em>metaclass__创建一个名字为Test的类对象<br> 如果Python没有找到<em>\</em>metaclass__，它会继续在Base（父类）中寻找<em>\</em>metaclass__属性，并尝试做和前面同样的操作。<br> 如果Python在任何父类中都找不到<em>\</em>metaclass__，它就会在模块层次中去寻找<em>\</em>metaclass__，并尝试做同样的操作。<br> 如果还是找不到<em>\</em>metaclass__,Python就会用内置的type来创建这个类对象</p>
<p>那么<em>\</em>metaclass__是什么？</p>
<p>答：就是可以创建类的东西，类是由type创建的，所以<em>\</em>metaclass__内部一定要返回一个类，它可以是一个函数，也可以是一个类，而这个类就是我们自定义的元类，这个类必须继承自type</p>
<p>通常元类用来创建API是非常好的选择，使用元类的编写很复杂，但使用者可以非常简洁的调用API</p>
<h4 id="abc-ABCMeta："><a href="#abc-ABCMeta：" class="headerlink" title="abc.ABCMeta："></a>abc.ABCMeta：</h4><p>简单的说ABCMeta就是让你的类变成一个纯虚类，子类必须实现某个方法，这个方法在父类中用@abc.abstractmethod修饰</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> abc</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span><span class="params">(object,metaclass=abc.ABCMeta)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_a</span><span class="params">(self,data)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param data:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"><span class="meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_b</span><span class="params">(self,data,out)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param data:</span></span><br><span class="line"><span class="string">        :param out:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_d</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'func_d in base'</span>)</span><br></pre></td></tr></table></figure>

<p>你可以实现这两个虚方法，也可以不实现<br>这样在Base的子类中就必须实现func_a，func_b2个函数，否则就会报错</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sub</span><span class="params">(Base)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_a</span><span class="params">(self,data)</span>:</span></span><br><span class="line">        print(<span class="string">'over write func_a'</span>,data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_b</span><span class="params">(self,data,out)</span>:</span></span><br><span class="line">        print(<span class="string">'over write func_b'</span>)</span><br></pre></td></tr></table></figure>

<p>如果还想调用虚类的方法用super</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func_b</span><span class="params">(self,data,out)</span>:</span></span><br><span class="line">        super(Sub,self).func_b(data,out)</span><br><span class="line">        print(<span class="string">'over write func_b'</span>)</span><br></pre></td></tr></table></figure>

<p>还有一种方法是，注册虚子类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Register</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_c</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'func_c in third class'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_a</span><span class="params">(self,data)</span>:</span></span><br><span class="line">        print(<span class="string">'func_a in third class'</span>,data)</span><br><span class="line">Base.register(Register)</span><br></pre></td></tr></table></figure>

<p>这样调用issubclass(), issubinstance()进行判断时仍然返回真值</p>
<h3 id="2-tensorflow中的“tf-name-scope-”有什么用？"><a href="#2-tensorflow中的“tf-name-scope-”有什么用？" class="headerlink" title="2. tensorflow中的“tf.name_scope()”有什么用？"></a>2. tensorflow中的“tf.name_scope()”有什么用？</h3><h4 id="2-1-tf-name-scope-命名空间的实际作用"><a href="#2-1-tf-name-scope-命名空间的实际作用" class="headerlink" title="2.1. tf.name_scope()命名空间的实际作用"></a><strong>2.1. tf.name_scope()命名空间的实际作用</strong></h4><p>（1）在某个tf.name_scope()指定的区域中定义的所有对象及各种操作，他们的“name”属性上会增加该命名区的区域名，用以区别对象属于哪个区域； </p>
<p>（2）将不同的对象及操作放在由tf.name_scope()指定的区域中，便于在tensorboard中展示清晰的逻辑关系图，这点在复杂关系图中特别重要。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf;  </span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 无tf.name_scope()</span></span><br><span class="line">a = tf.constant(<span class="number">1</span>,name=<span class="string">'my_a'</span>) <span class="comment">#定义常量</span></span><br><span class="line">b = tf.Variable(<span class="number">2</span>,name=<span class="string">'my_b'</span>) <span class="comment">#定义变量</span></span><br><span class="line">c = tf.add(a,b,name=<span class="string">'my_add'</span>) <span class="comment">#二者相加（操作）</span></span><br><span class="line">print(<span class="string">"a.name = "</span>+a.name)</span><br><span class="line">print(<span class="string">"b.name = "</span>+b.name)</span><br><span class="line">print(<span class="string">"c.name = "</span>+c.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 有tf.name_scope()</span></span><br><span class="line"><span class="comment"># with tf.name_scope('cgx_name_scope'): #定义一块名为cgx_name_scope的区域，并在其中工作</span></span><br><span class="line"><span class="comment">#     a = tf.constant(1,name='my_a')</span></span><br><span class="line"><span class="comment">#     b = tf.Variable(2,name='my_b')</span></span><br><span class="line"><span class="comment">#     c = tf.add(a,b,name='my_add')</span></span><br><span class="line"><span class="comment"># print("a.name = "+a.name)</span></span><br><span class="line"><span class="comment"># print("b.name = "+b.name)</span></span><br><span class="line"><span class="comment"># print("c.name = "+c.name)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存graph用于tensorboard绘图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"./test"</span>,sess.graph)</span><br><span class="line">    print(sess.run(c))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># 无tf.name_scope()</span></span><br><span class="line">a.name = my_a:<span class="number">0</span></span><br><span class="line">b.name = my_b:<span class="number">0</span></span><br><span class="line">c.name = my_add:<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有tf.name_scope()</span></span><br><span class="line">a.name = cgx_name_scope/my_a:<span class="number">0</span></span><br><span class="line">b.name = cgx_name_scope/my_b:<span class="number">0</span></span><br><span class="line">c.name = cgx_name_scope/my_add:<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>从输出结果可以看出，在tf.name_scope()下的所有对象和操作，其name属性前都加了cgx_name_scope，用以表示这些内容全在其范围下。<br>下图展示了两种情况的tensorboard差异，差别一目了然。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/14029140-b8d46d738bf1230c.jpg?imageMogr2/auto-orient/" alt="img"></p>
<h4 id="2-2-name-scope-只决定“对象”属于哪个范围，并不会对“对象”的“作用域”产生任何影响。"><a href="#2-2-name-scope-只决定“对象”属于哪个范围，并不会对“对象”的“作用域”产生任何影响。" class="headerlink" title="2.2. name_scope()只决定“对象”属于哪个范围，并不会对“对象”的“作用域”产生任何影响。"></a><strong>2.2. name_scope()只决定“对象”属于哪个范围，并不会对“对象”的“作用域”产生任何影响。</strong></h4><p>tf.name_scope()只是规定了对象和操作属于哪个区域，但这并不意味着他们的作用域也只限于该区域（with的这种写法很容易让人产生这种误会），不要将其和“全局变量、局部变量”的概念搞混淆，两者完全不是一回事。在name_scope中定义的对象，从被定义的位置开始，直到后来某个地方对该对象重新定义，中间任何地方都可以使用该对象。本质上name_scope只对对象的name属性进行圈定，并不会对其作用域产生任何影响。这就好比甲、乙、丙、丁属于陈家，这里“陈家”就是一个name_scope划定的区域，虽然他们只属于陈家，但他们依然可以去全世界的任何地方，并不会只将他们限制在陈家范围。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'cgx_1'</span>):</span><br><span class="line">    a = tf.Variable(tf.constant(<span class="number">4</span>), name=<span class="string">'my_a'</span>)</span><br><span class="line">    print(<span class="string">'case1: a.name = '</span> + a.name)</span><br><span class="line">print(<span class="string">"case2: a.name = "</span> + a.name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'cgx_2'</span>):</span><br><span class="line">    print(<span class="string">"case3: a.name = "</span> + a.name)</span><br><span class="line">    a = tf.Variable(tf.constant(<span class="number">4</span>), name=<span class="string">'my_a'</span>)</span><br><span class="line">    print(<span class="string">"case4: a.name = "</span> + a.name)</span><br><span class="line">print(<span class="string">"case5: a.name = "</span> + a.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># out：</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">case1: a.name = cgx_1/my_a:0</span></span><br><span class="line"><span class="string">case2: a.name = cgx_1/my_a:0</span></span><br><span class="line"><span class="string">case3: a.name = cgx_1/my_a:0</span></span><br><span class="line"><span class="string">case4: a.name = cgx_2/my_a:0</span></span><br><span class="line"><span class="string">case5: a.name = cgx_2/my_a:0</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p>（1）程序首先指定了命名区域cgx_1，并在其中定义了变量a，紧接着case1直接在cgx_1中输出a.name = cgx_1/my_a:0，这很好理解，跟想象的一样；<br> （2）case2在cgx_1之外的公共区域也输出了相同的a.name，<strong>这就说明a的作用范围并没有被限制在cgx_1中</strong>；<br> （3）接着程序又新指定了命名区域cgx_2，并在其中执行case3，输出a.name，结果还是和case1和case2完全相同，实际上还是最前面定义的那个a，这更进一步说明<strong>name_scope不会对对象的作用域产生影响</strong>；<br> （4）★★接着在cgx_2中<strong>重新定义了变量“a”</strong>，紧接着就执行case4，输出a.name = cgx_2/my_a:0，可见此时的结果与前面三个case就不同了，说明这里<strong>新定义的a覆盖了前面的a，即使他们在两个完全独立的name_scope中</strong>；<br> （5）case5输出的结果与case4结果相同，这已经无须解释了。</p>
<h4 id="2-3-tf-name-scope-‘cgx-scope’-语句重复执行几次，就会生成几个独立的命名空间，尽管表面上看起来都是“cgx-scope”，实际上tensorflow在每一次执行相同语句都会在后面加上“-序数”，加以区别。"><a href="#2-3-tf-name-scope-‘cgx-scope’-语句重复执行几次，就会生成几个独立的命名空间，尽管表面上看起来都是“cgx-scope”，实际上tensorflow在每一次执行相同语句都会在后面加上“-序数”，加以区别。" class="headerlink" title="2.3 tf.name_scope(‘cgx_scope’)语句重复执行几次，就会生成几个独立的命名空间，尽管表面上看起来都是“cgx_scope”，实际上tensorflow在每一次执行相同语句都会在后面加上“_序数”，加以区别。"></a><strong>2.3 tf.name_scope(‘cgx_scope’)语句重复执行几次，就会生成几个独立的命名空间，尽管表面上看起来都是“cgx_scope”，实际上tensorflow在每一次执行相同语句都会在后面加上“_序数”，加以区别。</strong></h4><p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'cgx_scope'</span>):</span><br><span class="line">    a = tf.Variable(<span class="number">1</span>, name=<span class="string">'my_a'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'cgx_scope'</span>):</span><br><span class="line">    b = tf.Variable(<span class="number">2</span>, name=<span class="string">'my_b'</span>)</span><br><span class="line"></span><br><span class="line">c = tf.add(a, b, name=<span class="string">'my_add'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'a.name = '</span> + a.name)</span><br><span class="line">print(<span class="string">'b.name = '</span> + b.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># out:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">a.name = cgx_scope/my_a:0</span></span><br><span class="line"><span class="string">b.name = cgx_scope_1/my_b:0</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p>（1）指定了“<strong>cgx_scope</strong>”命名区域，并在其中定义变量a；<br> （2）又指定了相同名称的“<strong>cgx_scope</strong>”命名区域，并在其中定义变量b；<br> （3）输出a.name = cgx_scope/my_a:0和b.name = cgx_scope_1/my_b:0，<strong>可见b.name已经自动加了“_1”，这是tensorflow的特点，自动检测是否重复，有重复就自动增加数字作为标记</strong>。</p>
<h3 id="3-tf-shape"><a href="#3-tf-shape" class="headerlink" title="3. tf.shape()"></a>3. tf.shape()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.shape(</span><br><span class="line">    input,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    out_type=tf.int32</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ul>
<li>将矩阵的维度输出为一个维度矩阵</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line">A = np.array([[[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]],</span><br><span class="line">              [[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]],</span><br><span class="line">              [[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]])</span><br><span class="line"></span><br><span class="line">t = tf.shape(A)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(t))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># out: [3 2 3]</span></span><br></pre></td></tr></table></figure>

<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><ul>
<li>input：张量或稀疏张量</li>
<li>name：op 的名字，用于tensorboard中</li>
<li>out_type：默认为tf.int32</li>
</ul>
<h4 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h4><ul>
<li>返回out_type类型的张量</li>
</ul>
<h3 id="4-tf-reshape"><a href="#4-tf-reshape" class="headerlink" title="4. tf.reshape()"></a>4. tf.reshape()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.reshape(tensor,shape,name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><ul>
<li><p>tensor：输入张量</p>
</li>
<li><p>shape：列表形式，可以存在-1</p>
<p>-1 代表的含义是不用我们自己指定这一维的大小，函数会自动计算，但列表中只能存在一个-1</p>
</li>
<li><p>name：命名</p>
</li>
</ul>
<h4 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h4><p>将tensor变换为参数shape的形式</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line">print(<span class="string">'a = '</span>, a)</span><br><span class="line"></span><br><span class="line">b = a.reshape((<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">print(<span class="string">'b = '</span>, b)</span><br><span class="line"></span><br><span class="line">c = a.reshape((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">print(<span class="string">'c = '</span>, c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># out：</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">a =  [0 1 2 3 4 5 6 7]</span></span><br><span class="line"><span class="string">b =  [[0 1 2 3]</span></span><br><span class="line"><span class="string"> [4 5 6 7]]</span></span><br><span class="line"><span class="string">c =  [[[0 1]</span></span><br><span class="line"><span class="string">  [2 3]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[4 5]</span></span><br><span class="line"><span class="string">  [6 7]]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<h3 id="5-tf-control-dependencies"><a href="#5-tf-control-dependencies" class="headerlink" title="5. tf.control_dependencies()"></a>5. tf.control_dependencies()</h3><p>在有些机器学习程序中我们想要指定某些操作执行的依赖关系，这时我们可以使用tf.control_dependencies()来实现。 </p>
<p>control_dependencies(control_inputs)返回一个控制依赖的上下文管理器，使用with关键字可以让在这个上下文环境中的操作都在control_inputs 执行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b, c]):</span><br><span class="line">  <span class="comment"># `d` and `e` will only run after `a`, `b`, and `c` have executed.</span></span><br><span class="line">  d = ...</span><br><span class="line">  e = ...</span><br></pre></td></tr></table></figure>

<p>可以嵌套<code>control_dependencies</code> 使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</span><br><span class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></span><br><span class="line">  <span class="keyword">with</span> g.control_dependencies([c, d]):</span><br><span class="line">    <span class="comment"># Ops constructed here run after `a`, `b`, `c`, and `d`.</span></span><br></pre></td></tr></table></figure>

<p>可以传入<code>None</code> 来消除依赖：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</span><br><span class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></span><br><span class="line">  <span class="keyword">with</span> g.control_dependencies(<span class="literal">None</span>):</span><br><span class="line">    <span class="comment"># Ops constructed here run normally, not waiting for either `a` or `b`.</span></span><br><span class="line">    <span class="keyword">with</span> g.control_dependencies([c, d]):</span><br><span class="line">      <span class="comment"># Ops constructed here run after `c` and `d`, also not waiting</span></span><br><span class="line">      <span class="comment"># for either `a` or `b`.</span></span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：<br>控制依赖只对那些在上下文环境中<strong>建立</strong>的操作有效，仅仅在context中<strong>使用</strong>一个操作或张量是没用的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># WRONG</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></span><br><span class="line">  t = tf.matmul(tensor, tensor)</span><br><span class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">    <span class="comment"># The matmul op is created outside the context, so no control</span></span><br><span class="line">    <span class="comment"># dependency will be added.</span></span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># RIGHT</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">    <span class="comment"># The matmul op is created in the context, so a control dependency</span></span><br><span class="line">    <span class="comment"># will be added.</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(tensor, tensor)</span><br></pre></td></tr></table></figure>

<p>例子：<br>在训练模型时我们每步训练可能要执行两种操作，<code>op a, b</code> 这时我们就可以使用如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.control_dependencies([a, b]):</span><br><span class="line">    c= tf.no_op(name=<span class="string">'train'</span>)<span class="comment">#tf.no_op；什么也不做</span></span><br><span class="line">sess.run(c)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">c= tf.no_op提供一个什么都不做的节点，该节点属于整个执行的流程图，但是操作a和操作b不是流程图中的一部分，但是为了确保操作a和操作b在某一个环节（此时可能是个未知环节）之前执行，所以提供一个什么都不做的环节c(前面称为节点)，确保操作a和操作b在c之前能够完成。而环节c可以插入流程图中。在整个流程图运行起来时，当运行到c时，就确保a,b操作先执行。 我只是根据官方文档以及常用用法猜测，不一定对。</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p>在这样简单的要求下，可以将上面代码替换为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c= tf.group([a, b])</span><br><span class="line">sess.run(c)</span><br></pre></td></tr></table></figure>

<h3 id="set-shape-与reshape"><a href="#set-shape-与reshape" class="headerlink" title="set_shape()与reshape()"></a>set_shape()与reshape()</h3><p>set_shape() 方法更新张量对象的静态形状，通常用于在无法直接推断时提供其他形状信息。它不会改变张量的动态形状</p>
<p>reshape()操作创建一个具有不同动态形状的新张量</p>
<h3 id="tf-image-resize-images"><a href="#tf-image-resize-images" class="headerlink" title="tf.image.resize_images()"></a>tf.image.resize_images()</h3><p>改变图片尺寸的大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> img_resized = tf.image.resize_images(image_data, [<span class="number">300</span>, <span class="number">300</span>], method=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 第一个参数为袁术图像的大小</span></span><br><span class="line"><span class="comment"># 第二三个分别为调整后图像的大小</span></span><br><span class="line"><span class="comment"># method参数给出了调整图像大小的方向</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">method = 0, 双线性插值法</span></span><br><span class="line"><span class="string">method = 1, 最近邻居法</span></span><br><span class="line"><span class="string">method = 2, 双三次插值法</span></span><br><span class="line"><span class="string">method = 3, 面积插值法</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<h3 id="xreadline-与-readlines"><a href="#xreadline-与-readlines" class="headerlink" title="xreadline() 与 readlines()"></a>xreadline() 与 readlines()</h3><p>xreadlines返回的是一个生成器类型</p>
<p>readlines()返回的是一个列表</p>
<p>但是使用时是相同的</p>
<h3 id="os-path-join"><a href="#os-path-join" class="headerlink" title="os.path.join()"></a>os.path.join()</h3><p>连接两个或更多的路径名组件</p>
<ul>
<li>如果各组件名首字母不包含‘/’，则函数会自动加上</li>
<li>如果有一个组件是一个绝对路径，则在它之前的所有组件均会被舍弃</li>
<li>如果最后䘝组件为空，则生成的路径以一个‘/’分隔符结尾</li>
</ul>
<h3 id="tf-train-slice-input-producer"><a href="#tf-train-slice-input-producer" class="headerlink" title="tf.train.slice_input_producer()"></a>tf.train.slice_input_producer()</h3><p>是一个tensor生成器，作用是按照设定，每次从一个tensor列表中按顺序或者随机抽取出一个tensor放入文件名队列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slice_input_producer(tensor_list, num_epochs=<span class="literal">None</span>, shuffle=<span class="literal">True</span>, seed=<span class="literal">None</span>, capacity=<span class="number">32</span>, shared_name=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<h3 id="tf-read-file-amp-tf-image-decode-jpeg-处理图片"><a href="#tf-read-file-amp-tf-image-decode-jpeg-处理图片" class="headerlink" title="tf.read_file() &amp; tf.image.decode_jpeg()处理图片"></a>tf.read_file() &amp; tf.image.decode_jpeg()处理图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file_contents = tf.read_file(filename)</span><br><span class="line">image = tf.image.decode_png(file_contents)	<span class="comment"># 解码png格式</span></span><br></pre></td></tr></table></figure>

<h3 id="os-path-splitext"><a href="#os-path-splitext" class="headerlink" title="os.path.splitext()"></a>os.path.splitext()</h3><p><code>os.path.splitext(“文件路径”)</code>分离文件名与扩展名；默认返回<code>(frame,fextension)</code>元组，可做分片操作</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">path_01=<span class="string">'D:/User/wgy/workplace/data/notMNIST_large.tar.gar'</span></span><br><span class="line">path_02=<span class="string">'D:/User/wgy/workplace/data/notMNIST_large'</span></span><br><span class="line">root_01=os.path.splitext(path_01)</span><br><span class="line">root_02=os.path.splitext(path_02)</span><br><span class="line">print(root_01)</span><br><span class="line">print(root_02)</span><br><span class="line"></span><br><span class="line"><span class="comment"># out：</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">('D:/User/wgy/workplace/data/notMNIST_large.tar', '.gar')</span></span><br><span class="line"><span class="string">('D:/User/wgy/workplace/data/notMNIST_large', '')</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<h3 id="tf-concat"><a href="#tf-concat" class="headerlink" title="tf.concat()"></a>tf.concat()</h3><p><code>tf.concat([tensor1, tensor2, tensor3, ...], axis)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t1 = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]  </span><br><span class="line">t2 = [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]  </span><br><span class="line">tf.concat([t1, t2], <span class="number">0</span>)  <span class="comment"># [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]  </span></span><br><span class="line">tf.concat([t1, t2], <span class="number">1</span>)  <span class="comment"># [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]</span></span><br><span class="line"><span class="comment"># axis = 0, 代表在第0个维度拼接</span></span><br><span class="line"><span class="comment"># axis = 1, 代表在第1个维度拼接</span></span><br></pre></td></tr></table></figure>

<p>对于一个二维矩阵，第0个维度代表最外层方括号所框下的子集，第一个维度代表内部方括号所框下的子集。<strong>维度越高，括号越小</strong></p>
<p>对于[ [ ], [ ]]和[[ ], [ ]]，低维拼接等于拿掉最外面括号，高维拼接是拿掉里面的括号(保证其他维度不变)。</p>
<p><strong>注意：tf.concat()拼接的张量只会改变一个维度，其他维度是保存不变的。</strong></p>
<p>比如两个shape为[2,3]的矩阵拼接，要么通过axis=0变成[4,3]，要么通过axis=1变成[2,6]。<strong>改变的维度索引对应axis的值。</strong></p>
<h3 id="tf-contrib-layers-batch-norm"><a href="#tf-contrib-layers-batch-norm" class="headerlink" title="tf.contrib.layers.batch_norm()"></a>tf.contrib.layers.batch_norm()</h3><h3 id="tf-contrib-layers-conv2d"><a href="#tf-contrib-layers-conv2d" class="headerlink" title="tf.contrib.layers.conv2d()"></a>tf.contrib.layers.conv2d()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.layers.conv2d(</span><br><span class="line">    inputs,			<span class="comment"># [batch_size] + input_spatial_shape + [in_channels]如果data_format不以“NC”（默认值）[batch_size, in_channels] + input_spatial_shape开头，或者 data_format以“NC”开头，则为形状等级N + 2的张量。</span></span><br><span class="line">    num_outputs,	<span class="comment"># 整数，输出过滤器的数量。</span></span><br><span class="line">    kernel_size,	<span class="comment"># N个正整数的序列，指定过滤器的空间维度。可以是单个整数，以指定所有空间维度的相同值。</span></span><br><span class="line">    stride=<span class="number">1</span>,	</span><br><span class="line">    padding=<span class="string">'SAME'</span>,</span><br><span class="line">    data_format=<span class="literal">None</span>,</span><br><span class="line">    rate=<span class="number">1</span>,</span><br><span class="line">    activation_fn=tf.nn.relu,</span><br><span class="line">    normalizer_fn=<span class="literal">None</span>,	<span class="comment"># 使用标准化功能代替biases。如果 normalizer_fn提供biases_initializer， biases_regularizer则忽略并且biases不创建也不添加。没有规范化器功能，默认设置为“无”</span></span><br><span class="line">    normalizer_params=<span class="literal">None</span>,		<span class="comment"># 规范化函数参数。</span></span><br><span class="line">    weights_initializer=initializers.xavier_initializer(),	<span class="comment"># 权重的初始化程序。</span></span><br><span class="line">    weights_regularizer=<span class="literal">None</span>,	<span class="comment"># 可选的权重正则化器。</span></span><br><span class="line">    biases_initializer=tf.zeros_initializer(),	<span class="comment"># 偏移量的初始化程序。如果没有跳过偏移量。</span></span><br><span class="line">    biases_regularizer=<span class="literal">None</span>,	<span class="comment"># 偏移量的可选正则化器。</span></span><br><span class="line">    reuse=<span class="literal">None</span>,					<span class="comment"># 是否应重用图层及其变量。必须给出能够重用层范围的能力</span></span><br><span class="line">    variables_collections=<span class="literal">None</span>,	<span class="comment"># 所有变量的集合的可选列表或包含每个变量的不同集合列表的字典。</span></span><br><span class="line">    outputs_collections=<span class="literal">None</span>,	<span class="comment"># 用于添加输出的集合。</span></span><br><span class="line">    trainable=<span class="literal">True</span>,				<span class="comment"># 如果True还将变量添加到图表集合中 GraphKeys.TRAINABLE_VARIABLES（请参阅tf.Variable）。</span></span><br><span class="line">    scope=<span class="literal">None</span>					<span class="comment"># 可选范围variable_scope。</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="tf-contrib-layers-variance-scaling-initializer"><a href="#tf-contrib-layers-variance-scaling-initializer" class="headerlink" title="tf.contrib.layers.variance_scaling_initializer()"></a>tf.contrib.layers.variance_scaling_initializer()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">variance_scaling_initializer(</span><br><span class="line">    factor=<span class="number">2.0</span>,</span><br><span class="line">    mode=<span class="string">'FAN_IN'</span>,</span><br><span class="line">    uniform=<span class="literal">False</span>,</span><br><span class="line">    seed=<span class="literal">None</span>,</span><br><span class="line">    dtype=tf.float32</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>方差缩放初始化</p>
<p>这种初始化方法比常规高斯分布初始化、阶段高斯分布初始化及Xavier初始化的泛华/缩放性能更好。粗略地说，方差缩放初始化根据每一层输入或输出的数量(在 TensorFlow 中默认为输入的数量)来调整初始随机权重的方差，从而帮助信号在不需要其他技巧(如梯度裁剪或批归一化)的情况下在网络中更深入地传播。</p>
<h3 id="tf-constant-initializer"><a href="#tf-constant-initializer" class="headerlink" title="tf.constant_initializer()"></a>tf.constant_initializer()</h3><p>初始化为常数，这个非常有用，通常偏置项就是用它初始化的。</p>
<p>由它衍生出的两个初始化方法：</p>
<ul>
<li>tf.zeros_initializer()， 也可以简写为tf.Zeros()</li>
<li>tf.ones_initializer(), 也可以简写为tf.Ones()</li>
</ul>
<h3 id="tf-contrib-layers-convolution"><a href="#tf-contrib-layers-convolution" class="headerlink" title="tf.contrib.layers.convolution()"></a>tf.contrib.layers.convolution()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def convolution(inputs,</span><br><span class="line">                num_outputs,</span><br><span class="line">                kernel_size,</span><br><span class="line">                stride=1,</span><br><span class="line">                padding=&apos;SAME&apos;,</span><br><span class="line">                data_format=None,</span><br><span class="line">                rate=1,</span><br><span class="line">                activation_fn=nn.relu,</span><br><span class="line">                normalizer_fn=None,</span><br><span class="line">                normalizer_params=None,</span><br><span class="line">                weights_initializer=initializers.xavier_initializer(),</span><br><span class="line">                weights_regularizer=None,</span><br><span class="line">                biases_initializer=init_ops.zeros_initializer(),</span><br><span class="line">                biases_regularizer=None,</span><br><span class="line">                reuse=None,</span><br><span class="line">                variables_collections=None,</span><br><span class="line">                outputs_collections=None,</span><br><span class="line">                trainable=True,</span><br><span class="line">                scope=None):</span><br></pre></td></tr></table></figure>

<h3 id="x-get-shape-as-list"><a href="#x-get-shape-as-list" class="headerlink" title="x.get_shape().as_list()"></a>x.get_shape().as_list()</h3><p>x.get_shape()，只有tensor才可以使用这种方法，返回的是一个元组</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">a_array = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">b_list = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]]</span><br><span class="line">c_tensor = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">print(c_tensor.get_shape())</span><br><span class="line">print(c_tensor.get_shape().as_list())</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.shape(a_array)))</span><br><span class="line">    print(sess.run(tf.shape(b_list)))</span><br><span class="line">    print(sess.run(tf.shape(c_tensor)))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># out:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">(2, 3)</span></span><br><span class="line"><span class="string">[2, 3]</span></span><br><span class="line"><span class="string">[2 3]</span></span><br><span class="line"><span class="string">[2 3]</span></span><br><span class="line"><span class="string">[2 3]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p>只能用于tensor来返回shape，但是是一个元组，需要通过as_list()的操作转换成list.</p>
<h3 id="tf-image-rgb-to-grayscale"><a href="#tf-image-rgb-to-grayscale" class="headerlink" title="tf.image.rgb_to_grayscale()"></a>tf.image.rgb_to_grayscale()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.image.rgb_to_grayscale(</span><br><span class="line">    images,		<span class="comment"># 要转换的RGB张量，最后一个维度的大小必须为3，并且应该包含RGB值</span></span><br><span class="line">    name=<span class="literal">None</span>	<span class="comment"># 操作的名称（可选）</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 返回：该函数返回转换后的灰度图像</span></span><br></pre></td></tr></table></figure>

<p>将一个或多个图像从RGB转化为灰度</p>
<p>输出与images具有相同DType和等级的张量，最后一个维度大小为1，包含像素的灰度值。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/07/29/双边网格/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/29/双边网格/" itemprop="url">双边网格</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-29T08:42:33+08:00">
                2019-07-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1-双边滤波器的快速近似"><a href="#1-双边滤波器的快速近似" class="headerlink" title="1. 双边滤波器的快速近似"></a>1. 双边滤波器的快速近似</h3><p>在对双边网格做总结之前，先介绍一下双边滤波器的快速近似方法（<a href="http://www.cis.rit.edu/~cnspci/references/dip/filtering/paris2006.pdf），它是双边网格的雏形，是一个将双边滤波器扩展大高维空间进行线性卷积的方法。而双边网格的坐着在这篇文章的基础上，将高维空间数据映射成3D" target="_blank" rel="noopener">http://www.cis.rit.edu/~cnspci/references/dip/filtering/paris2006.pdf），它是双边网格的雏形，是一个将双边滤波器扩展大高维空间进行线性卷积的方法。而双边网格的坐着在这篇文章的基础上，将高维空间数据映射成3D</a> array，进而提出了双边网格</p>
<h4 id="1-1-双边滤波器"><a href="#1-1-双边滤波器" class="headerlink" title="1.1 双边滤波器"></a>1.1 双边滤波器</h4><p>简单的说，双边滤波器是一种高斯滤波器的扩展。传统的高斯滤波器有一个高斯核，通过对空间相邻像素点以高斯函数位权值取均值，实现对图像的平滑。由于传统高斯滤波器只考虑了空域的信息，尽管它能实现图像的有效平滑，但同时也模糊了边缘信息。双边滤波器的原理即在传统高斯滤波器的基础上添加了一个表征亮度差异的高斯核，即既考虑了空间的信息，又考虑了值域的信息。在灰度差别不大的范围的范围，表征亮度高斯核的权值较大，接近于1，因此双边滤波器退化为传统的高斯滤波器，实现对图像的平滑；在边缘部分，尽管相邻像素点的空间接近，空间距离小，但由于边缘部分，灰度差别比较大，因此在值域上距离较大，所以新加入的高斯核使得在该部分不执行平滑和粗粒，保留图像的边缘。所以说，双边滤波是一种非线性（两个高斯核的乘积）的铝箔方法，是结合图像的空间邻近度和像素值相似度的一种折中处理，同时考虑空域信息和灰度相似性，达到保边去噪的目的</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/07/28/Loss-Function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/28/Loss-Function/" itemprop="url">Loss Function</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-28T21:30:13+08:00">
                2019-07-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>损失函数（Loss Function）是用来估计模型的预测值f(x)与真实值Y的不一致程度；他是一个非负实值函数，通常使用 L(Y,f(x)) 来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。模型的结构风险函数包括了经验风险和正则项，通常可以表示成如下式子：</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1564324156129.png" alt="1564324156129"></p>
<p>其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数</p>
<h3 id="一-LogLoss对数损失函数（逻辑回归，交叉熵损失）"><a href="#一-LogLoss对数损失函数（逻辑回归，交叉熵损失）" class="headerlink" title="一. LogLoss对数损失函数（逻辑回归，交叉熵损失）"></a>一. LogLoss对数损失函数（逻辑回归，交叉熵损失）</h3><p>平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推到得到的，而逻辑回归得到的不是平方损失。在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是吧极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即 $maxF(y, F(x)) -&gt; min -F(y, f(x))$ ）从损失函数的角度来看，它就成log损失函数了</p>
<p>log损失函数的标准形式：</p>
<p>$L(Y, P(Y|X)) = -logP(Y|X)$</p>
<p>取对数是为了方便计算极大似然值，因为MLE（最大似然估计）中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数 $L(Y, P(Y|X))$ 表达的是样本X在分类Y的情况下，使概率 $P(Y|X)$ 达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的擦书才能使我们观测到目前这组数据的概率最大）。因为log函数是单调递增的，所以 $logP(Y|X)$ 也会达到最大值，因此在前面加上负号之后，最大化 $P(Y|X)$ 就等价于最小化L了</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1564360345156.png" alt="1564360345156"></p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1564360352849.png" alt="1564360352849"></p>
<p><strong>注意：softmax使用的即为交叉熵损失函数，binary_cossentropy为二分类交叉熵损失，categorical_crossentropy为多分类交叉熵损失，当使用多分类交叉熵损失函数时，标签应该为多分类模式，即使用one-hot编码的向量。</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/07/27/Tkinter/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/27/Tkinter/" itemprop="url">Tkinter</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-27T16:18:34+08:00">
                2019-07-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Label-amp-Button"><a href="#Label-amp-Button" class="headerlink" title="Label &amp; Button"></a>Label &amp; Button</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/07/25/ResNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/25/ResNet/" itemprop="url">ResNet</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-25T20:48:56+08:00">
                2019-07-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="ResNet（Residual-Neural-Network）"><a href="#ResNet（Residual-Neural-Network）" class="headerlink" title="ResNet（Residual Neural Network）"></a>ResNet（Residual Neural Network）</h2><h3 id="一-简要概括"><a href="#一-简要概括" class="headerlink" title="一. 简要概括"></a>一. 简要概括</h3><p>ResNet的结构可以极快的加速神经网络的训练，模型的准确率也有比较大的提升。同时ResNe的推广性非常好，甚至可以直接用到InceptionNet网络中</p>
<p>ResNet的主要思想是在网络中增加了直连通道，即Highway Network的思想。此前的网络结构是性能输入做一个非线性变换，而Highway Network则允许保留之前网络层的一定比例的输出。ResNet的思想和way Network的思想也十分类似，允许原始输入信息直接传到后面的层中：</p>
<p><img src="https://img-blog.csdn.net/20180710193536899?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMxODE1OTU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>这样的话这一层的神经网络可以不用学习整个的输出，而是学习上一个网络输出的残差，因此ResNet又叫做残差网络</p>
<h3 id="二-创新点"><a href="#二-创新点" class="headerlink" title="二. 创新点"></a>二. 创新点</h3><p>提出残差学习的思想。传统的卷积网络或者全连接网络在信息传递的时候或多或少会存在信息丢失，损耗等问题，同时还有导致梯度消失或者梯度爆炸，导致很深的网络无法训练。ResNet在一定程度上解决了这个问题，通过直接将输入信息绕道传到输出，保护信息的完整性，整个网络只需要学习输入、输出差别的那一部分，简化学习目标和难度。VGGNet和ResNet的对比：</p>
<p><img src="https://img-blog.csdn.net/20180710193619121?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMxODE1OTU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>ResNet最大的区别在于有很多的旁路将输入直接连接到后面的层，这种结构也被称为shortcut或者skip connection</p>
<h3 id="三-深度残差学习（Deep-Residual-Learning）"><a href="#三-深度残差学习（Deep-Residual-Learning）" class="headerlink" title="三. 深度残差学习（Deep Residual Learning）"></a>三. 深度残差学习（Deep Residual Learning）</h3><h4 id="残差学习"><a href="#残差学习" class="headerlink" title="残差学习"></a>残差学习</h4><p>若将输入设为x，将某一有参网络层设为H，那么以X为输入的此层的输出将为H(X)。一般的CNN网络如Alexnet/VGG等会直接通过训练学习出参数函数H的表达，从而直接学习 X -&gt; H(X)</p>
<p>而残差学习则是致力于使用多个有参网络层来学习输入、输出之间的残差，即H(X) - X，即学习 X -&gt; (H(X) - X) + X。其中X这一部分为直接的Identity mapping，而H(X) - X则为有参网络层要学习的输入输出间残差。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/5971313-ba5e9ef4e622908f.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/430" alt="img"></p>
<h4 id="Identity-mapping"><a href="#Identity-mapping" class="headerlink" title="Identity mapping"></a>Identity mapping</h4><p>残差学习单元通过Identity mapping的引入在输入、输出之间建立了一条直接的关联通道，从而使得强大的有参层集中精力学习输入、输出之间的残差。一般我们用 $F(X,W_i)$ 来表示残差映射，那么输出即为： $Y=F(X,W_i)+X$。 当输入、输出通道数相同时，我们自然可以如此直接使用X进行相加。而当他们之间的通道数目不同时，我们就需要考虑建立一种有效的Identity mapping函数从而可以使得处理后的输入X与输出Y的通道数目相同即 $Y=F(X,W_i)+W_s*X$。</p>
<p>当X与Y通道数目不同时，作者尝试了两种Identity mapping的方式。一种即简单地将X相对Y确实的通道直接补零从而时期能够相对器的方式，另一种则是通过使用1 * 1的conv来表示 $W_s$ 映射从而使得最终输入与输出的通道达到一致的方式。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/07/25/YOLO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/25/YOLO/" itemprop="url">YOLO</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-25T20:37:09+08:00">
                2019-07-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="《You-Only-Look-Once-Unified-Real-Time-Object-Detection》——YOLO"><a href="#《You-Only-Look-Once-Unified-Real-Time-Object-Detection》——YOLO" class="headerlink" title="《You Only Look Once: Unified, Real-Time Object Detection》——YOLO"></a>《You Only Look Once: Unified, Real-Time Object Detection》——YOLO</h2><p>目前，基于深度学习算法的一系列目标检测算法大致可以分为两个流派：</p>
<ol>
<li>两步走（two-stage）算法：先生产候选区域然后再进行CNN分类（RCNN系列）</li>
<li>一步走（one-stage）算法：直接对输入图像应用算法并输出类别和相应的定位（YOLO系列）</li>
</ol>
<p>之前的R-CNN系列虽然准确率比较高，但是即使是发展到Faster R-CNN，检测一张图片如下图所示也要7pfd（原文为5fps），为了使得检测的工作能够应用到实时的场景中，提出了YOLO</p>
<p><img src="https://upload-images.jianshu.io/upload_images/6983308-92902dcbd7c8262d.png?imageMogr2/auto-orient/" alt="img"></p>
<p>WOLO的检测思想不同于R-CNN系列的思想，它将目标检测作为回归任务来解决。</p>
<p>YOLO的整体结构：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/6983308-d54136f1eb0cd733.png?imageMogr2/auto-orient/" alt="img"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/6983308-0d1e8d42480e1c1a.png?imageMogr2/auto-orient/" alt="img"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/07/25/Something-about-TensorFlow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/25/Something-about-TensorFlow/" itemprop="url">Something about TensorFlow</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-25T17:16:29+08:00">
                2019-07-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1-TensorFlow中tf-train-slice-input-producer和tf-train-batch函数"><a href="#1-TensorFlow中tf-train-slice-input-producer和tf-train-batch函数" class="headerlink" title="1. TensorFlow中tf.train.slice_input_producer和tf.train.batch函数"></a>1. TensorFlow中tf.train.slice_input_producer和tf.train.batch函数</h3><h4 id="TensorFlow数据读取机制"><a href="#TensorFlow数据读取机制" class="headerlink" title="TensorFlow数据读取机制"></a>TensorFlow数据读取机制</h4><p>TensorFlow中为了充分利用GPU，减少GPU等待数据的空闲时间，使用了两个线程分别执行数据读入和数据计算。</p>
<p>具体来说就是使用一个线程源源不断的将硬盘中的图片数据读入到一个内存队列中，另一个线程负责计算任务，所需数据直接从内存队列中获取。</p>
<p>tf在内存队列之前，还设立了一个文件名队列，文件名队列存放的是参与训练的文件名，要训练N个epoch，则文件名队列中就含有N个批次的所有文件名。</p>
<p><img src="https://img-blog.csdn.net/20180401131308152" alt="img"></p>
<p>在N个epoch的文件名最后是一个结束标志，当tf读到这个结束标志的时候，会抛出一个AutoRange的异常，外部捕获到之歌异常之后皆可以结束进程了。而创建tf的文件名队列就需要使用到tf.train.slice_input_producer函数</p>
<h4 id="tf-train-slice-input-producer"><a href="#tf-train-slice-input-producer" class="headerlink" title="tf.train.slice_input_producer"></a>tf.train.slice_input_producer</h4><p>tf.train.slice_input_producer是一个tensor生成器，作用是按照规定，每次从一个tensor列表中按顺序或者随机抽取出一个tensor放入文件名队列。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None,                         capacity=32, shared_name=None, name=None)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>第一个参数 tensor_list：包含一系列tensor的列表，表中tensor的第一维度的值必须相等，即个数必须相等，有多少个图像，就应该有多少个对应的标签。</p>
</li>
<li><p>第二个参数<strong>num_epochs: 可选参数，是一个整数值，代表迭代的次数</strong>，如果设置 num_epochs=None,生成器可以无限次遍历tensor列表，如果设置为 num_epochs=N，生成器只能遍历tensor列表N次。</p>
</li>
<li><p>第三个参数shuffle： bool类型，设置是否打乱样本的顺序。<strong>一般情况下，如果shuffle=True，生成的样本顺序就被打乱了，在批处理的时候不需要再次打乱样本，使用 tf.train.batch函数就可以了;如果shuffle=False,就需要在批处理时候使用 tf.train.shuffle_batch函数打乱样本。</strong></p>
</li>
<li><p>第四个参数seed: <strong>可选的整数，是生成随机数的种子，在第三个参数设置为shuffle=True的情况下才有用。</strong></p>
</li>
<li><p>第五个参数capacity：设置tensor列表的容量。</p>
</li>
<li><p>第六个参数shared_name：可选参数，如果设置一个‘shared_name’，则在不同的上下文环境（Session）中可以通过这个名字共享生成的tensor。</p>
</li>
<li><p>第七个参数name：可选，设置操作的名称。</p>
</li>
</ul>
<p>tf.train.slice_input_producer定义了样本放入文件名队列的方式，包括迭代次数，是否乱序等，<strong>要真正将文件放入文件名队列，还需要调用tf.train.start_queue_runners 函数来启动执行文件名队列填充的线程，之后计算单元才可以把数据读出来，否则文件名队列为空的，计算单元就会处于一直等待状态，导致系统阻塞。</strong></p>
<h4 id="tf-contrib-layers-batch-norm"><a href="#tf-contrib-layers-batch-norm" class="headerlink" title="tf.contrib.layers.batch_norm"></a><strong>tf.contrib.layers.batch_norm</strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tf.contrib.layers.batch_norm(</span><br><span class="line">    inputs,</span><br><span class="line">    decay=0.999,</span><br><span class="line">    center=True,</span><br><span class="line">    scale=False,</span><br><span class="line">    epsilon=0.001,</span><br><span class="line">    activation_fn=None,</span><br><span class="line">    param_initializers=None,</span><br><span class="line">    param_regularizers=None,</span><br><span class="line">    updates_collections=tf.GraphKeys.UPDATE_OPS,</span><br><span class="line">    is_training=True,</span><br><span class="line">    reuse=None,</span><br><span class="line">    variables_collections=None,</span><br><span class="line">    outputs_collections=None,</span><br><span class="line">    trainable=True,</span><br><span class="line">    batch_weights=None,</span><br><span class="line">    fused=None,</span><br><span class="line">    data_format=DATA_FORMAT_NHWC,</span><br><span class="line">    zero_debias_moving_mean=False,</span><br><span class="line">    scope=None,</span><br><span class="line">    renorm=False,</span><br><span class="line">    renorm_clipping=None,</span><br><span class="line">    renorm_decay=0.99,</span><br><span class="line">    adjustment=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>Batch Normalization 通过减少内部使用协变量加速神经网络的训练</p>
<p>可以用作conv2d或fully_connected的标准化函数</p>
<p>参数：</p>
<p>1 inputs： 输入</p>
<p>2 decay ：衰减系数。合适的衰减系数值接近1.0,特别是含多个9的值：0.999,0.99,0.9。如果训练集表现很好而验证/测试集表现得不好，选择小的系数（推荐使用0.9）。如果想要提高稳定性，zero_debias_moving_mean设为True</p>
<p>3 center：如果为True，有beta偏移量；如果为False，无beta偏移量</p>
<p>4 scale：如果为True，则乘以gamma。如果为False，gamma则不使用。当下一层是线性的时（例如nn.relu），由于缩放可以由下一层完成，所以可以禁用该层。</p>
<p>5 epsilon：避免被零除</p>
<p>6 activation_fn：用于激活，默认为线性激活函数</p>
<p>7 param_initializers ： beta, gamma, moving mean and moving variance的优化初始化</p>
<p>8 param_regularizers ： beta and gamma正则化优化</p>
<p>9 updates_collections ：Collections来收集计算的更新操作。updates_ops需要使用train_op来执行。如果为None，则会添加控件依赖项以确保更新已计算到位。</p>
<p>10 is_training:图层是否处于训练模式。在训练模式下，它将积累转入的统计量moving_mean并 moving_variance使用给定的指数移动平均值 decay。当它不是在训练模式，那么它将使用的数值moving_mean和moving_variance。<br>11 scope：可选范围variable_scope</p>
<h4 id="tf-contrib-layers-convolution2d"><a href="#tf-contrib-layers-convolution2d" class="headerlink" title="tf.contrib.layers.convolution2d"></a>tf.contrib.layers.convolution2d</h4><h4 id="tf-reduce-mean"><a href="#tf-reduce-mean" class="headerlink" title="tf.reduce_mean"></a>tf.reduce_mean</h4><p>用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的平均值，主要用作降维或者计算tensor（图像）的平均值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">reduce_mean(</span><br><span class="line">	input_tensor， </span><br><span class="line">    axis=<span class="literal">None</span>, </span><br><span class="line">	keep_dims=<span class="literal">False</span>,</span><br><span class="line">	name=<span class="literal">None</span>, </span><br><span class="line">	reduction_indices=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>参数：<ul>
<li>axis：指定的轴，如果不指定，则计算所有元素的均值</li>
<li>keep_dims：是否降维度，设置为True， 则输出的结果保持输入tensor的结果，设置为False， 输出结果会降低维度</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"></span><br><span class="line">x = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">     [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br><span class="line"></span><br><span class="line">xx = tf.cast(x, tf.float32)</span><br><span class="line">mean_all = tf.reduce_mean(xx, keep_dims=<span class="literal">False</span>)</span><br><span class="line">mean_0 = tf.reduce_mean(xx, axis=<span class="number">0</span>, keep_dims=<span class="literal">False</span>)</span><br><span class="line">mean_1 = tf.reduce_mean(xx, axis=<span class="number">1</span>, keep_dims=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    m_a, m_0, m_1 = sess.run([mean_all, mean_0, mean_1])</span><br><span class="line"></span><br><span class="line">print(mean_a)</span><br><span class="line">print(mean_0)</span><br><span class="line">print(mean_1)    </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string"> 2.0</span></span><br><span class="line"><span class="string">[ 1. 2. 3.]</span></span><br><span class="line"><span class="string">[ 2. 2.]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">如果keep_dims=True，则结果为</span></span><br><span class="line"><span class="string">[[ 2.]]</span></span><br><span class="line"><span class="string">[[ 1. 2. 3.]]</span></span><br><span class="line"><span class="string">[[ 2.], [ 2.]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Mariana</p>
              <p class="site-description motion-element" itemprop="description">a study blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">77</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mariana</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
