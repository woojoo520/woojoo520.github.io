<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>Celery Fairy</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="a study blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Celery Fairy">
<meta property="og:url" content="https://woojoo520.github.io/page/4/index.html">
<meta property="og:site_name" content="Celery Fairy">
<meta property="og:description" content="a study blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Celery Fairy">
<meta name="twitter:description" content="a study blog">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Celery Fairy</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://woojoo520.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-NormFace" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/09/NormFace/" class="article-date">
  <time datetime="2019-08-09T08:29:48.000Z" itemprop="datePublished">2019-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/09/NormFace/">NormFace</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="人脸识别：NormFace"><a href="#人脸识别：NormFace" class="headerlink" title="人脸识别：NormFace"></a>人脸识别：NormFace</h2><p>参考链接：<a href="https://blog.csdn.net/wfei101/article/details/82890444" target="_blank" rel="noopener">https://blog.csdn.net/wfei101/article/details/82890444</a></p>
<h3 id="提出问题"><a href="#提出问题" class="headerlink" title="提出问题"></a>提出问题</h3><p>之前的人脸识别工作，在特征比较阶段，通常使用的都是特征的余弦距离</p>
<p>$cos \theta = \frac {a * b} {||a|| ||b||}$</p>
<p>而余弦距离等价于L2归一化后的内积，也等价于L2归一化后的欧氏距离（欧氏距离表示超球面上的弦长，两个向量之间的夹角越大，弦长也越大）</p>
<p>然而，实际上训练的时候用的都是没有L2归一化的内积</p>
<p>关于这一点，可以这样解释，softmax函数是：</p>
<p>$P_{k}= \frac {e^{W_{k}x}} {\sum_{j=0}^{d} e^{W_{j}x}}$</p>
<p>可以理解为$W_{k}$ 和特征向量 $x$的内积越大，x属于第k类概率也就越大，训练过程就是最大化x与其标签项所对应项的权值$W_{label(x)}$的过程</p>
<p><img src="https://img-blog.csdn.net/20180318154156748?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L0ZpcmVfTGlnaHRf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>也就是说在训练时使用的距离度量与在测试时使用的度量是不一样的</p>
<h3 id="测试时是否需要归一化？"><a href="#测试时是否需要归一化？" class="headerlink" title="测试时是否需要归一化？"></a>测试时是否需要归一化？</h3><p>事实证明，进行人脸验证时，使用归一化后的内积或者欧氏距离效果会优于直接计算两个特征向量的内积或者欧氏距离，实验结果如下</p>
<p><img src="https://img-blog.csdn.net/20180318154219273?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L0ZpcmVfTGlnaHRf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<ul>
<li>注意这个Normalization不同于batch normalization，一个是对L2翻书进行归一化，一个是均值归0，方差归1</li>
</ul>
<h3 id="那么是否可以直接在训练时也对特征向量归一化？"><a href="#那么是否可以直接在训练时也对特征向量归一化？" class="headerlink" title="那么是否可以直接在训练时也对特征向量归一化？"></a>那么是否可以直接在训练时也对特征向量归一化？</h3><p>针对上面的问题，作者设计实验，通过归一化Softmax所有的特征和权重来创建一个cosine layer，实验结果是<strong>网络不收敛了</strong>。</p>
<h3 id="本论文要解决的四大问题："><a href="#本论文要解决的四大问题：" class="headerlink" title="本论文要解决的四大问题："></a>本论文要解决的四大问题：</h3><ul>
<li>为什么在测试时必须要归一化？</li>
<li>为什么直接优化余弦相似度会导致网络不收敛？</li>
<li>怎么样使用softmaxloss优化余弦相似度？</li>
<li>既然softmax loss在优化余弦相似度时不能收敛，那么其他的损失函数可以收敛吗？</li>
</ul>
<h3 id="L2归一化"><a href="#L2归一化" class="headerlink" title="L2归一化"></a>L2归一化</h3><h4 id="为什么要归一化"><a href="#为什么要归一化" class="headerlink" title="为什么要归一化"></a>为什么要归一化</h4><p>全连接层特征降至二维的MNIST特征图</p>
<p>左图中，f2f3是同一类的两个特征，但是可以看到f1和f2的距离明显小于f2和f3的距离，因此，加入不对特征进行归一化再比较距离的话，可能就会误判f1f2为同一类</p>
<p><img src="https://img-blog.csdn.net/20180318154315787?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L0ZpcmVfTGlnaHRf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<h4 id="为什么特征会呈辐射状分布？"><a href="#为什么特征会呈辐射状分布？" class="headerlink" title="为什么特征会呈辐射状分布？"></a>为什么特征会呈辐射状分布？</h4><p>Softmax实际上是一种（Soft）软的max（最大化）操作,考虑Softmax的概率</p>
<p>$P_{i}(f)= \frac {e^{W_{i}^{T}f}} {\sum_{j=1}^{n} e^{W_{j}^{T}f}}$</p>
<p>假设是一个十个分类问题，那么每个类都会对应一个权值向量W0,W1…W9,某个特征f会被分为哪一类，<strong>取决f和哪一个权值向量的内积最大</strong>。</p>
<p>也就是说，靠近W0的向量会被归为第一类，靠近W1的向量会归为第二类，以此类推。网络在训练过程中，为了使得各个分类更明显，会让各个权值向量W逐渐分散开，相互之间有一定的角度，而<strong>靠近某一权值向量的特征就会被归为相应的类别</strong>，因此特征最终会呈辐射状分布。</p>
<h4 id="如果添加了偏置，结果会是怎么样的呢？"><a href="#如果添加了偏置，结果会是怎么样的呢？" class="headerlink" title="如果添加了偏置，结果会是怎么样的呢？"></a>如果添加了偏置，结果会是怎么样的呢？</h4><p>$L_{s}= - \frac{1}{m} \sum_{i = 1}^{m} log\frac {e^{W_{y_{i}}^{T} f_{i} + b_{y_{i}}}} {\sum_{j=1}^{n} e^{W_{j}^{T}f_{i} + b_{j}}}$</p>
<p>如果添加了骗纸，不同类的b不同，则会造成有的类w角度近似相等，而依据b来区分的情况。如下图：</p>
<p><img src="https://img-blog.csdn.net/20180318154405545?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L0ZpcmVfTGlnaHRf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>在这种情况下，如果再对w进行归一化，那个中间这些类会散步在单位圆上各个方向，造成错误分类。</p>
<p>所以添加偏置对我们通过余弦距离来分类没有帮助，弱化了网络的学习能力，所以我们不添加偏置</p>
<h4 id="网络为何不收敛？"><a href="#网络为何不收敛？" class="headerlink" title="网络为何不收敛？"></a>网络为何不收敛？</h4>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://woojoo520.github.io/2019/08/09/NormFace/" data-id="ck035zyc7000rp0ujtgtb4191" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-卷积" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/09/卷积/" class="article-date">
  <time datetime="2019-08-09T07:09:44.000Z" itemprop="datePublished">2019-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/09/卷积/">卷积在图像处理中的应用</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>卷积的用途：</p>
<p>将图像相邻子区域的像素值与卷积核执行“卷积”操作，可以获取相邻数据之间的统计关系，从而可挖掘图像中的某些重要特征。</p>
<p>比较抽象…所以特征到底是什么？用图像来形象的说明一下</p>
<p><img src="https://pic4.zhimg.com/v2-b0f198aa46872eb91cb7f1f593073f13_b.jpg" alt="img"></p>
<p>下面我们简单介绍一下常用的“久经考验”的卷积核。<br>（1）同一化核（Identity）。从图13-6可见，这个滤波器什么也没有做，卷积后得到的图像和原图一样。因为这个核只有中心点的值是1。邻域点的权值都是0，所以对滤波后的取值没有任何影响。<br>（2）边缘检测核（Edge Detection），也称为高斯-拉普拉斯算子。需要注意的是，这个核矩阵的元素总和为0（即中间元素为8，而周围8个元素之和为-8），所以滤波后的图像会很暗，而只有边缘位置是有亮度的。<br>（3）图像锐化核（Sharpness Filter）。图像的锐化和边缘检测比较相似。首先找到边缘，然后再把边缘加到原来的图像上面，如此一来，就强化了图像的边缘，使得图像看起来更加锐利。<br>（4）均值模糊（Box Blur /Averaging）。这个核矩阵的每个元素值都是1，它将当前像素和它的四邻域的像素一起取平均，然后再除以9。均值模糊比较简单，但图像处理得不够平滑。因此，还可以采用高斯模糊核（Gaussian Blur），这个核被广泛用在图像降噪上。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://woojoo520.github.io/2019/08/09/卷积/" data-id="ck035zycp0017p0ujtyaw62vi" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-center-loss" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/09/center-loss/" class="article-date">
  <time datetime="2019-08-09T06:31:07.000Z" itemprop="datePublished">2019-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/09/center-loss/">center loss</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Center-Loss"><a href="#Center-Loss" class="headerlink" title="Center Loss"></a>Center Loss</h2><h3 id="一-简介"><a href="#一-简介" class="headerlink" title="一. 简介"></a>一. 简介</h3><p>论文链接：<a href="http://ydwen.github.io/papers/WenECCV16.pdf" target="_blank" rel="noopener">http://ydwen.github.io/papers/WenECCV16.pdf</a> </p>
<h3 id="二-为什么要使用Center-Loss？"><a href="#二-为什么要使用Center-Loss？" class="headerlink" title="二. 为什么要使用Center Loss？"></a>二. 为什么要使用Center Loss？</h3><p>简单的来说，我们在做分类的时候，不光需要学得separable的特征，更想要这些特征是discriminative的，这就意味着我们需要在loss上做更多的约束。</p>
<p>仅仅使用softmax作为监督信号的输出处理就只能做到seperable而不是discriminative，如下图:</p>
<p><img src="https://img-blog.csdn.net/20180727140845416?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<h3 id="三-如何使学到的特征差异化更大——Center-Loss"><a href="#三-如何使学到的特征差异化更大——Center-Loss" class="headerlink" title="三. 如何使学到的特征差异化更大——Center Loss"></a>三. 如何使学到的特征差异化更大——Center Loss</h3><p>融合Softmax Loss与Center loss</p>
<p><strong>Softmax Loss（保证类之间的feature距离最大）与Center Loss（保证类内的feature距离最小，更接近于类中心）</strong></p>
<p><img src="https://img-blog.csdn.net/20180727150130651?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p><img src="https://img-blog.csdn.net/2018072715022915?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>m是mini-batch、n是class。在Lc公式中有一个缺陷，就是$C_{y_{i}}$是i这个样本对应的类别yi所属于的类中心C∈ Rd，d代表d维。</p>
<p>理想情况下，Cyi需要随着学到的feature变化而实时更新，也就是要在每一次迭代中用整个数据集的feature来算每个类的中心。</p>
<p>但这显然不现实，做以下两个修改：</p>
<p>1、由整个训练集更新center改为mini-batch更改center  </p>
<p>2、避免错误分类的样本的干扰，使用scalar α 来控制center的学习率  </p>
<p>因此求算梯度的公式如下：</p>
<p><img src="https://img-blog.csdn.net/20180727153311160?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>即：当yi = j，也就是mini-batch中某一个sample是对应要更新的那一个类的center的时候就累加起来除以某类的个数+1。</p>
<p><img src="https://img-blog.csdn.net/2018072715370270?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>最终loss联立起来如上图，λ用于平衡softmax loss与center loss，越大则区分度 越大，如下图效果：</p>
<p><img src="https://img-blog.csdn.net/20180727150702547?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<h3 id="四-Center-Loss的实现"><a href="#四-Center-Loss的实现" class="headerlink" title="四. Center Loss的实现"></a>四. Center Loss的实现</h3><p>pytorch实现：<a href="https://github.com/jxgu1016/MNIST_center_loss_pytorch" target="_blank" rel="noopener">https://github.com/jxgu1016/MNIST_center_loss_pytorch</a></p>
<ul>
<li>网络结构</li>
</ul>
<p><img src="https://img-blog.csdn.net/20180727162522989?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>即在特征层输出（classification前最后一层）引入center loss：</p>
<p><img src="https://img-blog.csdn.net/20180727162755579?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<h4 id="fully-connected-和-local-connected"><a href="#fully-connected-和-local-connected" class="headerlink" title="fully-connected 和 local-connected"></a>fully-connected 和 local-connected</h4><h5 id="判断fully-connected的方法："><a href="#判断fully-connected的方法：" class="headerlink" title="判断fully-connected的方法："></a>判断fully-connected的方法：</h5><ul>
<li><p>对于neuron的链接（点对点的链接）都是fully connected（这里其实就是MLP）</p>
</li>
<li><p>对于有filter的network，不是看filter的size，而是看output的feature map的size。如果output feature map的size还是1 * 1 * N的话，这个layer就是fully connected layer</p>
<p><strong>解释第二个判断方法：</strong></p>
<ul>
<li>1 * 1的filter size不一定是fully connected。比如input size是10 * 10 * 100， filter size是1 * 1 * 100， 重复50次，则该layer的总weights是：1 * 1 * 100 * 50</li>
<li>1 * 1的filter size如果要是 fully connected， 则input size必须是1 * 1</li>
<li>input size是10x10的时候却是fully connected的情况：这里我们的output size肯定是1x1，且我们的filter size肯定是10x10。</li>
</ul>
<p><strong>总结：filter size等于input size则是fully connected</strong></p>
</li>
</ul>
<p>综上：</p>
<ul>
<li>fully connected没有weight share</li>
<li>对于neuron的连接（点对点的链接）都是fully connected（MLP——多层感知器）</li>
<li>Convolution中当filter size等于input size时，就是fully connected，此时的output size为1 * 1 * N</li>
<li>当1 *1不等于input size时，1 * 1一样具备weights share的能力。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://woojoo520.github.io/2019/08/09/center-loss/" data-id="ck035zycc000up0ujvx1q9294" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-softmax-and-softmax-loss-and-BP" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/09/softmax-and-softmax-loss-and-BP/" class="article-date">
  <time datetime="2019-08-09T05:57:43.000Z" itemprop="datePublished">2019-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/09/softmax-and-softmax-loss-and-BP/">softmax and softmax-loss and BP</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/" target="_blank" rel="noopener">http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/</a> </p>
<p><img src="https://img-blog.csdn.net/20170504203817251?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>联想逻辑回归的重要公式就是<img src="https://img-blog.csdn.net/20170504203942767?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img">，得到预测结果，然后再经过sigmoid转换成0到1的概率值，而在softmax中则通过取exponential的方式并进行归一化得到某个样本属于某类的概率。非负的意义不用说，就是避免正负值抵消。</p>
<p><img src="https://img-blog.csdn.net/20170504204409994?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>逻辑回归的推导可以用最大似然或最小损失函数，本质是一样的，可以简单理解成加了一个负号，这里的y指的是真实类别。注意下softmax-loss可以看做是softmax和multinomial logistic loss两步，正如上述所写公式，把变量展开即softmax-loss。</p>
<p><img src="https://img-blog.csdn.net/20170504204847454?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>原博客的重点在于介绍softmax-loss是分成两步还是一步到位比较好，而我这则重点说下BP。上面这个神经网络的图应该不陌生，这个公式也是在逻辑回归的核心（通过迭代得到w，然后在测试时按照上面这个公式计算类别概率）</p>
<p><img src="https://img-blog.csdn.net/20170504205237865?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>这里第一个公式是损失函数对权重w求导，其实就是梯度，红色那部分可以看前面O是怎么算出来的，就知道其导数的形式非常简单，就是输入I。蓝色部分就是BP的核心，回传就是通过这个达到的，回传的东西就是损失函数对该层输出的导数，只有把这个往前回传，才能计算前面的梯度。所以回传的不是对权重的求导，对每层权重的求导的结果会保留在该层，等待权重更新时候使用。具体看上面最后一个公式。</p>
<p><img src="https://img-blog.csdn.net/20170504210106340?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>这部分的求导：l(y,z)函数是log函数，log（x）函数求导是取1/x，后面的那个数是zy对zk的导数，当k=y时，那就是1，k不等于y时就是两个不同的常数求导，就是0。</p>
<p><img src="https://img-blog.csdn.net/20170504210625060?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>这一部分就是把softmax-loss分成两步来做，第一个求导可以先找到最前面l(y,o)的公式，也是log函数，所以求导比较简单。第二个求导也是查看前面Oi的公式，分母取平方的那种求导。最后链式相乘的结果和原来合并算的结果一样。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://woojoo520.github.io/2019/08/09/softmax-and-softmax-loss-and-BP/" data-id="ck035zyci0011p0ujdfyr0cf3" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-PyTorch-中的-dim" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/09/PyTorch-中的-dim/" class="article-date">
  <time datetime="2019-08-09T02:24:23.000Z" itemprop="datePublished">2019-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/09/PyTorch-中的-dim/">PyTorch 中的 dim</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="PyTorch中的dim"><a href="#PyTorch中的dim" class="headerlink" title="PyTorch中的dim"></a>PyTorch中的dim</h2><h3 id="dim概念"><a href="#dim概念" class="headerlink" title="dim概念"></a>dim概念</h3><p>dim的不同值表示不同维度。特别的在dim=0表示二维中的行，dim=1在二维矩阵中表示行。广泛的来说，我们不管一个矩阵是几维的，比如一个矩阵维度如下：${d_{0},d_{1},…,d_{n-1}}$，那么dim=0就表示对应到$d_{0}$也就是第一个维度，dim=1,表示对应到$d_{1}$也就是第二个维度，以此类推</p>
<h3 id="dim在函数中的作用"><a href="#dim在函数中的作用" class="headerlink" title="dim在函数中的作用"></a>dim在函数中的作用</h3><h4 id="例一-torch-argmax"><a href="#例一-torch-argmax" class="headerlink" title="例一. torch.argmax()"></a>例一. torch.argmax()</h4><p>函数中dim表示该维度会消失。</p>
<p>这个消失是什么意思？官方英文解释是：dim (int) – the dimension to reduce.</p>
<p>我们知道argmax就是得到最大值的序号索引，对于一个维度为$(d_0,d_1)$的矩阵来说，我们想要求每一行中最大数的在该行中的列号，最后我们得到的就是一个维度为$(d_0,1)$的一矩阵。这时候，列就要消失了。</p>
<p>因此，我们想要求每一行最大的列标号，我们就要指定dim=1，表示我们不要列了，保留行的size就可以了。<br>假如我们想求每一列的最大行标，就可以指定dim=0，表示我们不要行了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">a = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a.size())</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">b = torch.argmax(a, dim=<span class="number">1</span>)</span><br><span class="line">print(b)</span><br><span class="line">print(b.size())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.Size([3, 4])</span></span><br><span class="line"><span class="string">tensor([[0.9120, 0.4805, 0.6701, 0.5446],</span></span><br><span class="line"><span class="string">        [0.6273, 0.1295, 0.3416, 0.2213],</span></span><br><span class="line"><span class="string">        [0.6068, 0.8448, 0.8452, 0.4931]])</span></span><br><span class="line"><span class="string">tensor([0, 0, 2])</span></span><br><span class="line"><span class="string">torch.Size([3])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 可以看到，指定dim=1时，列的size没有了</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.argmax(input, dim=<span class="literal">None</span>, keepdim=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>返回指定维度最大的序号</p>
<p>dim给定的定义是：the dimention to reduce.也就是吧dim这个维度的，变成这个维度的最大值</p>
<p>如果上面的代码改成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">b = torch.argmax(a, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output：</span></span><br><span class="line"><span class="string">tensor([[0],</span></span><br><span class="line"><span class="string">		[0],</span></span><br><span class="line"><span class="string">        [2]])</span></span><br><span class="line"><span class="string">torch.Size([3, 1])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://woojoo520.github.io/2019/08/09/PyTorch-中的-dim/" data-id="ck035zyc4000pp0ujwo0k4l5b" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-model-train-and-model-eval" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/09/model-train-and-model-eval/" class="article-date">
  <time datetime="2019-08-09T01:46:19.000Z" itemprop="datePublished">2019-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/09/model-train-and-model-eval/">model.train() and model.eval()</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>model.train()     model.eval()</p>
<p>一般在模型训练和评价的时候会加上这两句</p>
<p>主要是针对model在训练时和评价时不同的 <strong>Batch Normalization</strong> 和 <strong>Dropout</strong> 方法模式</p>
<p><strong>注意：</strong>使用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval， eval()时， 框架会自动把BN和DropOut固定住，不会取平均，而是用训练好的值，不然的话，一旦test的batch_size过小，很容易就会被BN层导致生成图片颜色失真极大！</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://woojoo520.github.io/2019/08/09/model-train-and-model-eval/" data-id="ck035zyce000wp0uj9g9v8u0q" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Python格式化输出" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/09/Python格式化输出/" class="article-date">
  <time datetime="2019-08-09T01:25:45.000Z" itemprop="datePublished">2019-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/09/Python格式化输出/">Python格式化输出</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Python格式化输出"><a href="#Python格式化输出" class="headerlink" title="Python格式化输出"></a>Python格式化输出</h2><h3 id="格式化输出字符串"><a href="#格式化输出字符串" class="headerlink" title="格式化输出字符串"></a>格式化输出字符串</h3><h4 id="用法一："><a href="#用法一：" class="headerlink" title="用法一："></a>用法一：</h4><p>与%s类似，不同指出是将<code>%s</code>换乘了<code>‘{ }’</code>大括号，调用时依然需要按照顺序对应</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">"My name is &#123;&#125;, &#123;&#125; years old, and my hobby is &#123;&#125;"</span></span><br><span class="line">s1 = s.format(<span class="string">'MMMMMQ'</span>, <span class="string">'25'</span>, <span class="string">'watching TV'</span>)</span><br><span class="line">print(s1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">My name is MMMMMQ, 25 years old, My hobby is watching TV</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="用法二："><a href="#用法二：" class="headerlink" title="用法二："></a>用法二：</h4><p>通过<code>{n}</code>方式来指定接收参数的位置，将调用时传入的参数按照位置进行传入。相比%s可以减少参数的个数，实现了参数的复用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">"My name is &#123;0&#125;, &#123;1&#125; years old, and my name is still &#123;0&#125;"</span></span><br><span class="line">s1 = s.format(<span class="string">'MMMMQ'</span>, <span class="string">'25'</span>, <span class="string">'watching TV'</span>)</span><br><span class="line">print(s1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">My name is MMMMQ, 25 years old, and my name is still MMMMQ</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="用法三："><a href="#用法三：" class="headerlink" title="用法三："></a>用法三：</h4><p>通过<code>str{}</code>方式来指定名字，调用时使用<code>str=&#39;xxx&#39;</code>，确定参数传入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">"My name is &#123;name&#125;, &#123;age&#125; years old, and my hobby is &#123;hobby&#125;"</span></span><br><span class="line">s1 = s.format(age=<span class="number">25</span>, hobby=<span class="string">'watching TV'</span>, name=<span class="string">'MMMMQ'</span>)</span><br><span class="line">print(s1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">My name is MMMMQ, 25 years old, and my hobby is watching TV</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h3 id="保留n位小数："><a href="#保留n位小数：" class="headerlink" title="保留n位小数："></a>保留n位小数：</h3><h4 id="保留一个数字"><a href="#保留一个数字" class="headerlink" title="保留一个数字"></a>保留一个数字</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'This is a float number &#123;:.2f&#125;'</span>.format(<span class="number">123.432423432</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">This is a float number 123.43</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="保留两个数字"><a href="#保留两个数字" class="headerlink" title="保留两个数字"></a>保留两个数字</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'This are two float numbers &#123;:.2f&#125; and &#123;:.4f&#125;'</span>.format(<span class="number">123.432423432</span>, <span class="number">0.321423423</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">This are two float numbers 123.43 and 0.3214</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="保留数字和文字"><a href="#保留数字和文字" class="headerlink" title="保留数字和文字"></a>保留数字和文字</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'This are two float numbers and a string &#123;:.2f&#125; , &#123;:.4f&#125; and &#123;&#125; '</span>.format(<span class="number">123.432423432</span>, <span class="number">0.321423423</span>, <span class="string">'MMMMQ'</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">This are two float numbers and a string 123.43 , 0.3214 and MMMMQ </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://woojoo520.github.io/2019/08/09/Python格式化输出/" data-id="ck035zyc2000op0uj18gnp033" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Neural-Network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Neural-Network/" class="article-date">
  <time datetime="2019-08-08T07:45:06.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Neural-Network/">Neural Network</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>可以使用<code>torch.nn</code>包构造神经网络</p>
<p>nn取决于autograd定义的模型并区分它们。一个<code>nn.Module</code>包含层和一种方法<code>forward(input)</code>，它返回output</p>
<p>例如，查看对数字图像进行分类的网络</p>
<p><img src="https://pytorch.org/tutorials/_images/mnist.png" alt="convnet"></p>
<p>它是一个简单的前馈网络。它接受输入，一个接一个地通过及各层输入，然后最终给出输出</p>
<p><strong>神经网络的典型训练程序如下：</strong></p>
<ul>
<li><p>定义一些具有可学习参数（或权重）的神经网络</p>
</li>
<li><p>迭代输入数据集</p>
</li>
<li><p>通过网络处理输入</p>
</li>
<li><p>计算损失（输出距离正确多远）</p>
</li>
<li><p>将渐变传播回网络参数</p>
</li>
<li><p>通常使用简单的更新规则更新网络权重</p>
<p><code>weight = weight - learning_rate * gradient</code></p>
</li>
</ul>
<h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://woojoo520.github.io/2019/08/08/Neural-Network/" data-id="ck035zybw000lp0ujywcsggvb" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-什么是PyTorch" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/07/什么是PyTorch/" class="article-date">
  <time datetime="2019-08-07T08:37:37.000Z" itemprop="datePublished">2019-08-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/07/什么是PyTorch/">什么是PyTorch?</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="什么是PyTorch？"><a href="#什么是PyTorch？" class="headerlink" title="什么是PyTorch？"></a>什么是PyTorch？</h3><p>这是一个基于Python的科学计算软件包，针对两组受众：</p>
<ul>
<li>Numpy的替代品，可以使用GPU的强大功能</li>
<li>深入学习研究平台，提供最大的灵活性和速度</li>
</ul>
<h3 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h3><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><p>张量与Numpy的ndarray类似，另外还有Tensor也可用于GPU以加速计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<p>构造一个未初始化的5 * 3矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[1.0561e-38, 1.0653e-38, 4.1327e-39],</span></span><br><span class="line"><span class="string">        [8.9082e-39, 9.8265e-39, 9.4592e-39],</span></span><br><span class="line"><span class="string">        [1.0561e-38, 1.0653e-38, 1.0469e-38],</span></span><br><span class="line"><span class="string">        [9.5510e-39, 1.0378e-38, 8.9082e-39],</span></span><br><span class="line"><span class="string">        [9.6429e-39, 8.9082e-39, 9.1837e-39]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>构造一个随机初始化的矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[0.4904, 0.0543, 0.2698],</span></span><br><span class="line"><span class="string">        [0.4626, 0.9494, 0.6573],</span></span><br><span class="line"><span class="string">        [0.2933, 0.7157, 0.5195],</span></span><br><span class="line"><span class="string">        [0.3329, 0.3446, 0.3271],</span></span><br><span class="line"><span class="string">        [0.3962, 0.9864, 0.2137]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>构造一个0填充的矩阵 of dtype long：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>直接从数据构造Tensor：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([5.5000, 3.0000])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>或者根据现有的张量创建张量。除非用户提供新值，否则这些方法将重用与输入张量的属性，例如dtype</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)	<span class="comment"># new method take in size</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)	<span class="comment"># override dtype!</span></span><br><span class="line">print(x)	<span class="comment"># result has the same size</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]], dtype=torch.float64)</span></span><br><span class="line"><span class="string">tensor([[-1.5400,  0.6350,  1.6990],</span></span><br><span class="line"><span class="string">        [ 0.0767,  0.3982, -0.0827],</span></span><br><span class="line"><span class="string">        [ 0.1511,  0.8096,  0.0600],</span></span><br><span class="line"><span class="string">        [ 0.2931,  0.8122, -0.3534],</span></span><br><span class="line"><span class="string">        [ 0.7164, -0.0334,  0.2272]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>得到它的大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(x, size())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.Size([5, 3])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong><code>torch.Size()</code>实际上是一个元组，因此它支持所有元组操作</p>
<h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>操作有多种语法。下面的示例中，我们将查看添加操作：</p>
<p>增加：语法1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>增加：语法2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>增加：提供输出张量作为参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>增加：就地</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<p>任何使原张量变形的操作都是用 _ 后固定的。例如：x.copy_(y), x.t_(), 将改变x。</p>
<p>可以使用标准的NumPy索引与所有bells和whistles</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x[:, <span class="number">1</span>])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[0.3356, 0.3659, 0.7898],</span></span><br><span class="line"><span class="string">        [0.3474, 0.4648, 0.2795],</span></span><br><span class="line"><span class="string">        [0.0268, 0.8986, 0.5615],</span></span><br><span class="line"><span class="string">        [0.8278, 0.4778, 0.0131],</span></span><br><span class="line"><span class="string">        [0.2451, 0.6178, 0.0125]])</span></span><br><span class="line"><span class="string">tensor([0.3659, 0.4648, 0.8986, 0.4778, 0.6178])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>调整大小：如果要调整tensor/重塑tensor，可以使用<code>torch.view</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)	<span class="comment"># the size of -1 is inferred from other dimensions</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br><span class="line">print(z)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</span></span><br><span class="line"><span class="string">tensor([[0.3415, 0.9245, 0.5641, 0.9463],</span></span><br><span class="line"><span class="string">        [0.5833, 0.5632, 0.3456, 0.6338],</span></span><br><span class="line"><span class="string">        [0.2562, 0.6854, 0.1495, 0.0351],</span></span><br><span class="line"><span class="string">        [0.6777, 0.8511, 0.9998, 0.7963]])</span></span><br><span class="line"><span class="string">tensor([0.3415, 0.9245, 0.5641, 0.9463, 0.5833, 0.5632, 0.3456, 0.6338, 0.2562,</span></span><br><span class="line"><span class="string">        0.6854, 0.1495, 0.0351, 0.6777, 0.8511, 0.9998, 0.7963])</span></span><br><span class="line"><span class="string">tensor([[0.3415, 0.9245, 0.5641, 0.9463, 0.5833, 0.5632, 0.3456, 0.6338],</span></span><br><span class="line"><span class="string">        [0.2562, 0.6854, 0.1495, 0.0351, 0.6777, 0.8511, 0.9998, 0.7963]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>假如你有一个元素张量，可以用item()获取值作为python数字</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([-0.2729])</span></span><br><span class="line"><span class="string">-0.2729296088218689</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>torch包含用于多维张量的数据结构，并且定义了浙西额数学运算。此外还提供了徐国使用程序，用于搞笑序列化Tensor和任意类型，以及其他有用的实用程序。</p>
<p>它有一个CUDA对应物，能够在计算能力 &gt;= 3.0的NVIDIA GPU上运行张量计算</p>
<p>关于张量的操作：</p>
<h3 id="NumPy-Bridge："><a href="#NumPy-Bridge：" class="headerlink" title="NumPy Bridge："></a>NumPy Bridge：</h3><p>将Torch Tensor转换为NumPy阵列（反之亦然）是一件轻而易举的事</p>
<p>Torch Tensor和NumPy阵列将共享其底层内存位置（如果Torch Tensor在CPU上），更改一个将改变另一个。</p>
<h4 id="将Torch-Tensor转换为NumPy数组"><a href="#将Torch-Tensor转换为NumPy数组" class="headerlink" title="将Torch Tensor转换为NumPy数组"></a>将Torch Tensor转换为NumPy数组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([1., 1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">[1. 1. 1. 1. 1.]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>了解numpy数组的值如何变化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([2., 2., 2., 2., 2.])</span></span><br><span class="line"><span class="string">[2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="将NumPy数组转换为Torch-Tensor"><a href="#将NumPy数组转换为Torch-Tensor" class="headerlink" title="将NumPy数组转换为Torch Tensor"></a>将NumPy数组转换为Torch Tensor</h4><p>了解更改np阵列如何自动更改Torch Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">torch.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">[2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong>除了CharTensor之外，CPU上的所有Tensor都支持转换为NumPy并返回</p>
<h4 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h4><p>可以使用<code>.to</code>方法将张量移动到任何设备上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)			<span class="comment"># a CUDA device object </span></span><br><span class="line">   	y = torch.ones_like(x, device=device)	<span class="comment"># directly create a tensor on GPU</span></span><br><span class="line">    x = x.to(device)						<span class="comment"># or just use strings ``.to("cuda")``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))		<span class="comment"># ``.to`` can also change dtype together!</span></span><br><span class="line"> <span class="string">"""</span></span><br><span class="line"><span class="string"> output:</span></span><br><span class="line"><span class="string"> tensor([[-0.6816,  0.2219, -0.4269],</span></span><br><span class="line"><span class="string">        [-1.1169, -1.7020,  0.6161],</span></span><br><span class="line"><span class="string">        [-0.2312, -0.8429,  0.9232],</span></span><br><span class="line"><span class="string">        [ 0.8789, -0.4840,  0.8420],</span></span><br><span class="line"><span class="string">        [-1.4957, -0.8483,  0.0130]])</span></span><br><span class="line"><span class="string">tensor([[ 0.3184,  1.2219,  0.5731],</span></span><br><span class="line"><span class="string">        [-0.1169, -0.7020,  1.6161],</span></span><br><span class="line"><span class="string">        [ 0.7688,  0.1571,  1.9232],</span></span><br><span class="line"><span class="string">        [ 1.8789,  0.5160,  1.8420],</span></span><br><span class="line"><span class="string">        [-0.4957,  0.1517,  1.0130]], device='cuda:0')</span></span><br><span class="line"><span class="string">tensor([[ 0.3184,  1.2219,  0.5731],</span></span><br><span class="line"><span class="string">        [-0.1169, -0.7020,  1.6161],</span></span><br><span class="line"><span class="string">        [ 0.7688,  0.1571,  1.9232],</span></span><br><span class="line"><span class="string">        [ 1.8789,  0.5160,  1.8420],</span></span><br><span class="line"><span class="string">        [-0.4957,  0.1517,  1.0130]], dtype=torch.float64)</span></span><br><span class="line"><span class="string"> """</span></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://woojoo520.github.io/2019/08/07/什么是PyTorch/" data-id="ck035zych000zp0ujyjffbxic" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-PyTorch-Study" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/07/PyTorch-Study/" class="article-date">
  <time datetime="2019-08-07T06:57:07.000Z" itemprop="datePublished">2019-08-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/07/PyTorch-Study/">PyTorch Study</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="PyTorch搭建神经网络"><a href="#PyTorch搭建神经网络" class="headerlink" title="PyTorch搭建神经网络"></a>PyTorch搭建神经网络</h3><h4 id="1-导入库"><a href="#1-导入库" class="headerlink" title="1. 导入库"></a>1. 导入库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br></pre></td></tr></table></figure>

<h4 id="2-搭建卷积神经网络"><a href="#2-搭建卷积神经网络" class="headerlink" title="2. 搭建卷积神经网络"></a>2. 搭建卷积神经网络</h4><p>网络定义一般由两部分组成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span>		<span class="comment"># 用来定义网络节点参数</span></span><br></pre></td></tr></table></figure>

<p>将节点连接成图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br></pre></td></tr></table></figure>

<p>卷积计算规则：</p>
<p>对我们输入形状1， 1， 64， 64.四个维度分别是<code>(batch, channel, height, width)</code></p>
<p><code>new_height = (height - kernel_size) + s * padding / (stride[0]) + 1</code></p>
<p>即在周围补一圈0， stride默认为1.因此</p>
<p><code>new_height = new_width = (64 - 3) / 1 + 1 = 62</code></p>
<p>由于输出通道是6，所以通过卷积层后维度为<code>(1, 6, 62, 62)</code></p>
<p>经过pooling后，<code>(1, 6, 31, 31)</code></p>
<p><code>x.view(1, -1)</code>把x伸缩为(1 : ?)的维度。这样整个网络其实输入<code>(1, 1, 64, 64)</code>，输出为(1, 10)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.linear = nn.Leaner(<span class="number">5766</span>, <span class="number">10</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpooling = nn.MaxPool2d((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="comment"># print (x.shape)</span></span><br><span class="line">        x = self.maxpooling(x)</span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># print (x.shape)</span></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h4 id="3-添加训练数据"><a href="#3-添加训练数据" class="headerlink" title="3. 添加训练数据"></a>3. 添加训练数据</h4><p>optimizer：是优化器，即所谓的反向传播算法。<code>criterion = nn.MSELoss()</code>定义损失函数。</p>
<p><code>input = torch.randn(1， 1， 64， 64).cuda()</code></p>
<p><code>output = torch.ones(1, 10).cuda()</code></p>
<p>定义训练样本，注意如果实在gpu中训练，在pytorch中需要.cuda()把数据从cpu中导入到gpu中</p>
<p>网络的功能是给定随机噪声向量，输出是逼近1的单位向量</p>
<h4 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(epoch):</span><br><span class="line">    prediction = net(input)</span><br><span class="line">    loss = creterion(prediction, output)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 消除优化器梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 指自动求导</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># 根据自动求导反向传播优化参数</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"EPOCH: &#123;&#125;, Loss:&#123;:4f&#125;"</span>).format(step, loss))</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://woojoo520.github.io/2019/08/07/PyTorch-Study/" data-id="ck035zyc0000mp0ujtlmk4auz" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/page/3/">&laquo; __('prev')</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/5/">__('next') &raquo;</a>
  </nav>
</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Mariana<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<script src="/js/script.js"></script>

  </div>
</body>
</html>