<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Celery Fairy" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://woojoo520.github.io').hostname,
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="a study blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Celery Fairy">
<meta property="og:url" content="https://woojoo520.github.io/page/109/index.html">
<meta property="og:site_name" content="Celery Fairy">
<meta property="og:description" content="a study blog">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Celery Fairy">
<meta name="twitter:description" content="a study blog">

<link rel="canonical" href="https://woojoo520.github.io/page/109/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Celery Fairy</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Celery Fairy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Celery's Blog</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/08/Autograd-自动微分/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.gif">
      <meta itemprop="name" content="Woojoo">
      <meta itemprop="description" content="a study blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/08/Autograd-自动微分/" class="post-title-link" itemprop="url">Autograd 自动微分</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-08 11:45:28" itemprop="dateCreated datePublished" datetime="2019-08-08T11:45:28+08:00">2019-08-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-26 17:24:57" itemprop="dateModified" datetime="2019-11-26T17:24:57+08:00">2019-11-26</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/08/08/Autograd-自动微分/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/08/08/Autograd-自动微分/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Autograd：-自动微分"><a href="#Autograd：-自动微分" class="headerlink" title="Autograd： 自动微分"></a>Autograd： 自动微分</h2><p>autograd包中是PyTorch中所有神经网络的核心。</p>
<p>该autograd软件包为Tensors上的所有操作提供自动区分。它是一个逐个运行的框架，这意味着backprop由自己的代码运行方式定义，并且每个迭代都可以不同</p>
<h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><p><code>torch.Tensor</code>是包的核心类。如果将其属性设置<code>.requires_grad</code>为<code>True</code>，则会开始跟踪其上的所有操作。完成计算后，可以调用<code>.backward()</code>并自动计算所有渐变。该张量的梯度将累计到<code>.grad()</code>属性中。</p>
<p>要阻止张量跟踪历史记录，可以调用<code>.detach()</code>将它从计算历史记录中分离出来，并防止将来的计算被跟踪</p>
<p>要防止跟踪历史记录（和使用内存），还可以将代码块包装在其中。这在评估模型时尤其有用，因为模型可能具有可训练的参数，但我们不需要梯度。<code>with torch.no_grad():requires_grad=True</code></p>
<p>还有一个类对于autograd实现非常重要 <code>-a Function</code></p>
<p>Tensor和Function互相连接并构建一个非循环图，它编码完整的计算历史。每一个张量都有<code>.grad_fn</code>属性，该属性引用Function已创建的属性Tensor。<code>.grad_fn is None</code></p>
<p>如果你想计算任何导数，可以调用<code>.backward</code> a Tensor。如果Tensor是标量（即它包含一个元素数据），则不需要指定任何参数<code>backward()</code>，但是如果他有更多的额元素，则需要指定一个<code>gradient</code>匹配形状的张量的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)    <span class="comment"># 创建一个张量并设置requires_grad为跟踪计算</span></span><br><span class="line">print(x)</span><br><span class="line">y = x + <span class="number">2</span>   <span class="comment"># 做一个张量操作</span></span><br><span class="line">print(y)</span><br><span class="line">print(y.grad_fn)    <span class="comment"># y是作为一个操作的结果创建的，所以它有一个grad_fn</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line">print(z, out)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1.]], requires_grad=True)</span></span><br><span class="line"><span class="string">tensor([[3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">&lt;AddBackward0 object at 0x000002508C5D4128&gt;</span></span><br><span class="line"><span class="string">tensor([[27., 27.],</span></span><br><span class="line"><span class="string">        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong> <code>requires_grad_(...)</code> <code>requires_grad</code>就地改变现有的Tensor旗帜。如果没有的话，就默认为False。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(a * a)</span><br><span class="line">print(b)</span><br><span class="line">print(b.grad_fn)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">tensor([[-0.1373,  1.9251],</span></span><br><span class="line"><span class="string">        [ 1.0700,  5.3293]], requires_grad=True)</span></span><br><span class="line"><span class="string">tensor(33.2711, grad_fn=&lt;SumBackward0&gt;)</span></span><br><span class="line"><span class="string">&lt;SumBackward0 object at 0x000002CBCFFC7A58&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h4 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h4><p>Let’s backprop now! 因为out包含的是一个标量，所以<code>out.backward()</code> 和<code>out.backward(torch.tensor(1.))</code>是等价的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line">print(x.grad)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[4.5000, 4.5000],</span></span><br><span class="line"><span class="string">        [4.5000, 4.5000]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p><code>torch.autograd.backgrad(variables, grad_variables=None, retain_graph = None, create_graph=None, retain_variables=None)</code></p>
<ul>
<li><p><code>grad_variable</code>：形状与variable一致，对于<code>y.backward()</code>，<code>grad_variable</code>相当于链式法则：</p>
<p><code>dz/dx = dz/dy * dy/dx</code>中的<code>dz/dy</code>。grad_variables也可以是tensor或序列</p>
</li>
<li><p><code>retain_graph</code>：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就会被清空，可通过指定这个参数不清空缓存，用来多次反向传播。</p>
</li>
<li>create_graph：对反向传播过程再次构建计算图，可通过backward of backward实现求高阶导数</li>
</ul>
<p><strong>注意：</strong>variables和grad_variables都可以是sequence。对于scalar（标量，一维向量）来说可以不用填写grad_variables参数，若填写的话就相当于<strong>系数</strong>。若variables非标量则必须填写grad_variables参数。</p>
<p><strong>通式：</strong> $k.backward(p)$ 对于此式，x的梯度$x.grad = p * \frac {d_{k}}  {d_{x}}$</p>
<ul>
<li><p>scalar标量：注意参数requires_grad=True让其成为一个叶子节点，具有求导功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="keyword">as</span> v </span><br><span class="line">a = v(t.FloatTensor([<span class="number">2</span>, <span class="number">3</span>]), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = a + <span class="number">3</span></span><br><span class="line">c = b * b * <span class="number">3</span></span><br><span class="line">out = c.mean()</span><br><span class="line">out.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">print(a.grad.data)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output：</span></span><br><span class="line"><span class="string">tensor([15., 18.])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>手工求解过程</p>
<p>$a = (x_{1}, x_{2})$     $b = (x_{1} + 3, x_{2} + 3)$     $c = (3 <em> (x_{1} + 3)^2 , 3 </em> (x_{2} + 3)^2)$</p>
<p>$\text {out}=\frac{3 *\left(\left(x_{1}+3\right)^{2}+\left(x_{2}+3\right)^{2}\right)}{2}$</p>
<p>我们对其求导也很简单：</p>
<p>$\frac{\partial o u t}{\partial x_{1}}=\left.3\left(x_{1}+3\right)\right|_{x_{1}=2}=15$</p>
<p>$\frac{\partial o u t}{\partial x_{2}}=\left.3\left(x_{2}+3\right)\right|_{x_{2}=3}=18$</p>
<p>和上面的求解一致</p>
</li>
<li></li>
</ul>
<p>还可以通过<code>.requires_grad=True</code>  <code>with torch.no_grad()</code>包装代码来停止在Tensor上跟踪历史记录的autograd。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>例：</p>
<p>y =w * x + b</p>
<p>y.backgrad()</p>
<p>如果需要计算dy / dw</p>
<p>w.grad()</p>
<p><strong>注意：</strong>torch.normal()</p>
<p>返回一个张量，张量里面的随机数是从相互独立的正态分布中随机产生的。神经网络中的初始weight用normal生成可能效果会比较好。</p>
<p>nn.CrossEntropyLoss() 和 NLLLoss()</p>
<p>NLLLoss的输入时一个对数概率向量和一个目标标签，他不会为我们计算对数概率，适合网络的最后一层时log_softmax，损失函数nn.CrossEntropyLoss()与NLLLoss()相同，唯一的不同是他为我们去做softmax</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/108/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/108/">108</a><span class="page-number current">109</span><a class="page-number" href="/page/110/">110</a><span class="space">&hellip;</span><a class="page-number" href="/page/129/">129</a><a class="extend next" rel="next" href="/page/110/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Woojoo"
      src="/images/logo.gif">
  <p class="site-author-name" itemprop="name">Woojoo</p>
  <div class="site-description" itemprop="description">a study blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">129</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Woojoo</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.5.0
  </div>

        








        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
















  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'aKRj8pzPJm0LdONJb0Ci0U5L-gzGzoHsz',
    appKey: 'vA8nbcq2HgWrqovGq6LwXRG1',
    placeholder: "Just go go",
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn',
    path: location.pathname,
    recordIP: false,
    serverURLs: ''
  });
}, window.Valine);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
