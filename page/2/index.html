<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="a study blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Celery Fairy">
<meta property="og:url" content="https://woojoo520.github.io/page/2/index.html">
<meta property="og:site_name" content="Celery Fairy">
<meta property="og:description" content="a study blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Celery Fairy">
<meta name="twitter:description" content="a study blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://woojoo520.github.io/page/2/">





  <title>Celery Fairy</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Celery Fairy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Celery's Blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/model-train-and-model-eval/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/model-train-and-model-eval/" itemprop="url">model.train() and model.eval()</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T09:46:19+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>model.train()     model.eval()</p>
<p>一般在模型训练和评价的时候会加上这两句</p>
<p>主要是针对model在训练时和评价时不同的 <strong>Batch Normalization</strong> 和 <strong>Dropout</strong> 方法模式</p>
<p><strong>注意：</strong>使用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval， eval()时， 框架会自动把BN和DropOut固定住，不会取平均，而是用训练好的值，不然的话，一旦test的batch_size过小，很容易就会被BN层导致生成图片颜色失真极大！</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/Python格式化输出/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/Python格式化输出/" itemprop="url">Python格式化输出</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T09:25:45+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Python格式化输出"><a href="#Python格式化输出" class="headerlink" title="Python格式化输出"></a>Python格式化输出</h2><h3 id="格式化输出字符串"><a href="#格式化输出字符串" class="headerlink" title="格式化输出字符串"></a>格式化输出字符串</h3><h4 id="用法一："><a href="#用法一：" class="headerlink" title="用法一："></a>用法一：</h4><p>与%s类似，不同指出是将<code>%s</code>换乘了<code>‘{ }’</code>大括号，调用时依然需要按照顺序对应</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">"My name is &#123;&#125;, &#123;&#125; years old, and my hobby is &#123;&#125;"</span></span><br><span class="line">s1 = s.format(<span class="string">'MMMMMQ'</span>, <span class="string">'25'</span>, <span class="string">'watching TV'</span>)</span><br><span class="line">print(s1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">My name is MMMMMQ, 25 years old, My hobby is watching TV</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="用法二："><a href="#用法二：" class="headerlink" title="用法二："></a>用法二：</h4><p>通过<code>{n}</code>方式来指定接收参数的位置，将调用时传入的参数按照位置进行传入。相比%s可以减少参数的个数，实现了参数的复用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">"My name is &#123;0&#125;, &#123;1&#125; years old, and my name is still &#123;0&#125;"</span></span><br><span class="line">s1 = s.format(<span class="string">'MMMMQ'</span>, <span class="string">'25'</span>, <span class="string">'watching TV'</span>)</span><br><span class="line">print(s1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">My name is MMMMQ, 25 years old, and my name is still MMMMQ</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="用法三："><a href="#用法三：" class="headerlink" title="用法三："></a>用法三：</h4><p>通过<code>str{}</code>方式来指定名字，调用时使用<code>str=&#39;xxx&#39;</code>，确定参数传入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">"My name is &#123;name&#125;, &#123;age&#125; years old, and my hobby is &#123;hobby&#125;"</span></span><br><span class="line">s1 = s.format(age=<span class="number">25</span>, hobby=<span class="string">'watching TV'</span>, name=<span class="string">'MMMMQ'</span>)</span><br><span class="line">print(s1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">My name is MMMMQ, 25 years old, and my hobby is watching TV</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h3 id="保留n位小数："><a href="#保留n位小数：" class="headerlink" title="保留n位小数："></a>保留n位小数：</h3><h4 id="保留一个数字"><a href="#保留一个数字" class="headerlink" title="保留一个数字"></a>保留一个数字</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'This is a float number &#123;:.2f&#125;'</span>.format(<span class="number">123.432423432</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">This is a float number 123.43</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="保留两个数字"><a href="#保留两个数字" class="headerlink" title="保留两个数字"></a>保留两个数字</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'This are two float numbers &#123;:.2f&#125; and &#123;:.4f&#125;'</span>.format(<span class="number">123.432423432</span>, <span class="number">0.321423423</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">This are two float numbers 123.43 and 0.3214</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="保留数字和文字"><a href="#保留数字和文字" class="headerlink" title="保留数字和文字"></a>保留数字和文字</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'This are two float numbers and a string &#123;:.2f&#125; , &#123;:.4f&#125; and &#123;&#125; '</span>.format(<span class="number">123.432423432</span>, <span class="number">0.321423423</span>, <span class="string">'MMMMQ'</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">This are two float numbers and a string 123.43 , 0.3214 and MMMMQ </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/08/Neural-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/08/Neural-Network/" itemprop="url">Neural Network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-08T15:45:06+08:00">
                2019-08-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>可以使用<code>torch.nn</code>包构造神经网络</p>
<p>nn取决于autograd定义的模型并区分它们。一个<code>nn.Module</code>包含层和一种方法<code>forward(input)</code>，它返回output</p>
<p>例如，查看对数字图像进行分类的网络</p>
<p><img src="https://pytorch.org/tutorials/_images/mnist.png" alt="convnet"></p>
<p>它是一个简单的前馈网络。它接受输入，一个接一个地通过及各层输入，然后最终给出输出</p>
<p><strong>神经网络的典型训练程序如下：</strong></p>
<ul>
<li><p>定义一些具有可学习参数（或权重）的神经网络</p>
</li>
<li><p>迭代输入数据集</p>
</li>
<li><p>通过网络处理输入</p>
</li>
<li><p>计算损失（输出距离正确多远）</p>
</li>
<li><p>将渐变传播回网络参数</p>
</li>
<li><p>通常使用简单的更新规则更新网络权重</p>
<p><code>weight = weight - learning_rate * gradient</code></p>
</li>
</ul>
<h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/07/什么是PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/什么是PyTorch/" itemprop="url">什么是PyTorch?</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-07T16:37:37+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="什么是PyTorch？"><a href="#什么是PyTorch？" class="headerlink" title="什么是PyTorch？"></a>什么是PyTorch？</h3><p>这是一个基于Python的科学计算软件包，针对两组受众：</p>
<ul>
<li>Numpy的替代品，可以使用GPU的强大功能</li>
<li>深入学习研究平台，提供最大的灵活性和速度</li>
</ul>
<h3 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h3><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><p>张量与Numpy的ndarray类似，另外还有Tensor也可用于GPU以加速计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<p>构造一个未初始化的5 * 3矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[1.0561e-38, 1.0653e-38, 4.1327e-39],</span></span><br><span class="line"><span class="string">        [8.9082e-39, 9.8265e-39, 9.4592e-39],</span></span><br><span class="line"><span class="string">        [1.0561e-38, 1.0653e-38, 1.0469e-38],</span></span><br><span class="line"><span class="string">        [9.5510e-39, 1.0378e-38, 8.9082e-39],</span></span><br><span class="line"><span class="string">        [9.6429e-39, 8.9082e-39, 9.1837e-39]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>构造一个随机初始化的矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[0.4904, 0.0543, 0.2698],</span></span><br><span class="line"><span class="string">        [0.4626, 0.9494, 0.6573],</span></span><br><span class="line"><span class="string">        [0.2933, 0.7157, 0.5195],</span></span><br><span class="line"><span class="string">        [0.3329, 0.3446, 0.3271],</span></span><br><span class="line"><span class="string">        [0.3962, 0.9864, 0.2137]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>构造一个0填充的矩阵 of dtype long：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>直接从数据构造Tensor：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([5.5000, 3.0000])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>或者根据现有的张量创建张量。除非用户提供新值，否则这些方法将重用与输入张量的属性，例如dtype</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)	<span class="comment"># new method take in size</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)	<span class="comment"># override dtype!</span></span><br><span class="line">print(x)	<span class="comment"># result has the same size</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]], dtype=torch.float64)</span></span><br><span class="line"><span class="string">tensor([[-1.5400,  0.6350,  1.6990],</span></span><br><span class="line"><span class="string">        [ 0.0767,  0.3982, -0.0827],</span></span><br><span class="line"><span class="string">        [ 0.1511,  0.8096,  0.0600],</span></span><br><span class="line"><span class="string">        [ 0.2931,  0.8122, -0.3534],</span></span><br><span class="line"><span class="string">        [ 0.7164, -0.0334,  0.2272]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>得到它的大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(x, size())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.Size([5, 3])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong><code>torch.Size()</code>实际上是一个元组，因此它支持所有元组操作</p>
<h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>操作有多种语法。下面的示例中，我们将查看添加操作：</p>
<p>增加：语法1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>增加：语法2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>增加：提供输出张量作为参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>增加：就地</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<p>任何使原张量变形的操作都是用 _ 后固定的。例如：x.copy_(y), x.t_(), 将改变x。</p>
<p>可以使用标准的NumPy索引与所有bells和whistles</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x[:, <span class="number">1</span>])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[0.3356, 0.3659, 0.7898],</span></span><br><span class="line"><span class="string">        [0.3474, 0.4648, 0.2795],</span></span><br><span class="line"><span class="string">        [0.0268, 0.8986, 0.5615],</span></span><br><span class="line"><span class="string">        [0.8278, 0.4778, 0.0131],</span></span><br><span class="line"><span class="string">        [0.2451, 0.6178, 0.0125]])</span></span><br><span class="line"><span class="string">tensor([0.3659, 0.4648, 0.8986, 0.4778, 0.6178])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>调整大小：如果要调整tensor/重塑tensor，可以使用<code>torch.view</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)	<span class="comment"># the size of -1 is inferred from other dimensions</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br><span class="line">print(z)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</span></span><br><span class="line"><span class="string">tensor([[0.3415, 0.9245, 0.5641, 0.9463],</span></span><br><span class="line"><span class="string">        [0.5833, 0.5632, 0.3456, 0.6338],</span></span><br><span class="line"><span class="string">        [0.2562, 0.6854, 0.1495, 0.0351],</span></span><br><span class="line"><span class="string">        [0.6777, 0.8511, 0.9998, 0.7963]])</span></span><br><span class="line"><span class="string">tensor([0.3415, 0.9245, 0.5641, 0.9463, 0.5833, 0.5632, 0.3456, 0.6338, 0.2562,</span></span><br><span class="line"><span class="string">        0.6854, 0.1495, 0.0351, 0.6777, 0.8511, 0.9998, 0.7963])</span></span><br><span class="line"><span class="string">tensor([[0.3415, 0.9245, 0.5641, 0.9463, 0.5833, 0.5632, 0.3456, 0.6338],</span></span><br><span class="line"><span class="string">        [0.2562, 0.6854, 0.1495, 0.0351, 0.6777, 0.8511, 0.9998, 0.7963]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>假如你有一个元素张量，可以用item()获取值作为python数字</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([-0.2729])</span></span><br><span class="line"><span class="string">-0.2729296088218689</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>torch包含用于多维张量的数据结构，并且定义了浙西额数学运算。此外还提供了徐国使用程序，用于搞笑序列化Tensor和任意类型，以及其他有用的实用程序。</p>
<p>它有一个CUDA对应物，能够在计算能力 &gt;= 3.0的NVIDIA GPU上运行张量计算</p>
<p>关于张量的操作：</p>
<h3 id="NumPy-Bridge："><a href="#NumPy-Bridge：" class="headerlink" title="NumPy Bridge："></a>NumPy Bridge：</h3><p>将Torch Tensor转换为NumPy阵列（反之亦然）是一件轻而易举的事</p>
<p>Torch Tensor和NumPy阵列将共享其底层内存位置（如果Torch Tensor在CPU上），更改一个将改变另一个。</p>
<h4 id="将Torch-Tensor转换为NumPy数组"><a href="#将Torch-Tensor转换为NumPy数组" class="headerlink" title="将Torch Tensor转换为NumPy数组"></a>将Torch Tensor转换为NumPy数组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([1., 1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">[1. 1. 1. 1. 1.]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>了解numpy数组的值如何变化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([2., 2., 2., 2., 2.])</span></span><br><span class="line"><span class="string">[2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="将NumPy数组转换为Torch-Tensor"><a href="#将NumPy数组转换为Torch-Tensor" class="headerlink" title="将NumPy数组转换为Torch Tensor"></a>将NumPy数组转换为Torch Tensor</h4><p>了解更改np阵列如何自动更改Torch Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">torch.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">[2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong>除了CharTensor之外，CPU上的所有Tensor都支持转换为NumPy并返回</p>
<h4 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h4><p>可以使用<code>.to</code>方法将张量移动到任何设备上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)			<span class="comment"># a CUDA device object </span></span><br><span class="line">   	y = torch.ones_like(x, device=device)	<span class="comment"># directly create a tensor on GPU</span></span><br><span class="line">    x = x.to(device)						<span class="comment"># or just use strings ``.to("cuda")``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))		<span class="comment"># ``.to`` can also change dtype together!</span></span><br><span class="line"> <span class="string">"""</span></span><br><span class="line"><span class="string"> output:</span></span><br><span class="line"><span class="string"> tensor([[-0.6816,  0.2219, -0.4269],</span></span><br><span class="line"><span class="string">        [-1.1169, -1.7020,  0.6161],</span></span><br><span class="line"><span class="string">        [-0.2312, -0.8429,  0.9232],</span></span><br><span class="line"><span class="string">        [ 0.8789, -0.4840,  0.8420],</span></span><br><span class="line"><span class="string">        [-1.4957, -0.8483,  0.0130]])</span></span><br><span class="line"><span class="string">tensor([[ 0.3184,  1.2219,  0.5731],</span></span><br><span class="line"><span class="string">        [-0.1169, -0.7020,  1.6161],</span></span><br><span class="line"><span class="string">        [ 0.7688,  0.1571,  1.9232],</span></span><br><span class="line"><span class="string">        [ 1.8789,  0.5160,  1.8420],</span></span><br><span class="line"><span class="string">        [-0.4957,  0.1517,  1.0130]], device='cuda:0')</span></span><br><span class="line"><span class="string">tensor([[ 0.3184,  1.2219,  0.5731],</span></span><br><span class="line"><span class="string">        [-0.1169, -0.7020,  1.6161],</span></span><br><span class="line"><span class="string">        [ 0.7688,  0.1571,  1.9232],</span></span><br><span class="line"><span class="string">        [ 1.8789,  0.5160,  1.8420],</span></span><br><span class="line"><span class="string">        [-0.4957,  0.1517,  1.0130]], dtype=torch.float64)</span></span><br><span class="line"><span class="string"> """</span></span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/07/PyTorch-Study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/PyTorch-Study/" itemprop="url">PyTorch Study</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-07T14:57:07+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="PyTorch搭建神经网络"><a href="#PyTorch搭建神经网络" class="headerlink" title="PyTorch搭建神经网络"></a>PyTorch搭建神经网络</h3><h4 id="1-导入库"><a href="#1-导入库" class="headerlink" title="1. 导入库"></a>1. 导入库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br></pre></td></tr></table></figure>

<h4 id="2-搭建卷积神经网络"><a href="#2-搭建卷积神经网络" class="headerlink" title="2. 搭建卷积神经网络"></a>2. 搭建卷积神经网络</h4><p>网络定义一般由两部分组成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span>		<span class="comment"># 用来定义网络节点参数</span></span><br></pre></td></tr></table></figure>

<p>将节点连接成图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br></pre></td></tr></table></figure>

<p>卷积计算规则：</p>
<p>对我们输入形状1， 1， 64， 64.四个维度分别是<code>(batch, channel, height, width)</code></p>
<p><code>new_height = (height - kernel_size) + s * padding / (stride[0]) + 1</code></p>
<p>即在周围补一圈0， stride默认为1.因此</p>
<p><code>new_height = new_width = (64 - 3) / 1 + 1 = 62</code></p>
<p>由于输出通道是6，所以通过卷积层后维度为<code>(1, 6, 62, 62)</code></p>
<p>经过pooling后，<code>(1, 6, 31, 31)</code></p>
<p><code>x.view(1, -1)</code>把x伸缩为(1 : ?)的维度。这样整个网络其实输入<code>(1, 1, 64, 64)</code>，输出为(1, 10)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.linear = nn.Leaner(<span class="number">5766</span>, <span class="number">10</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpooling = nn.MaxPool2d((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="comment"># print (x.shape)</span></span><br><span class="line">        x = self.maxpooling(x)</span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># print (x.shape)</span></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h4 id="3-添加训练数据"><a href="#3-添加训练数据" class="headerlink" title="3. 添加训练数据"></a>3. 添加训练数据</h4><p>optimizer：是优化器，即所谓的反向传播算法。<code>criterion = nn.MSELoss()</code>定义损失函数。</p>
<p><code>input = torch.randn(1， 1， 64， 64).cuda()</code></p>
<p><code>output = torch.ones(1, 10).cuda()</code></p>
<p>定义训练样本，注意如果实在gpu中训练，在pytorch中需要.cuda()把数据从cpu中导入到gpu中</p>
<p>网络的功能是给定随机噪声向量，输出是逼近1的单位向量</p>
<h4 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(epoch):</span><br><span class="line">    prediction = net(input)</span><br><span class="line">    loss = creterion(prediction, output)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 消除优化器梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 指自动求导</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># 根据自动求导反向传播优化参数</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"EPOCH: &#123;&#125;, Loss:&#123;:4f&#125;"</span>).format(step, loss))</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/07/Inception/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/Inception/" itemprop="url">Inception</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-07T12:55:01+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Inception-v1"><a href="#Inception-v1" class="headerlink" title="Inception-v1"></a>Inception-v1</h3><p>在这篇论文之前，卷积神经网络的性能高都是依赖于提高网络的深度和宽度，而这篇论文是从网络结构上入手，改变了网络结构</p>
<h4 id="为什么提出Inception？"><a href="#为什么提出Inception？" class="headerlink" title="为什么提出Inception？"></a>为什么提出Inception？</h4><p>提高网络最简单粗暴的方法就是提高网络的深度和宽度，即增加隐层以及各层神经元数目。但这种简单粗暴的方法存在一些问题：</p>
<ul>
<li>会导致更大的参数空间，更容易过拟合</li>
<li>需要更多的计算资源</li>
<li>网络越深，梯度容易消失，优化困难（这时还没有提出BN，网络优化极其困难）</li>
</ul>
<p>基于此，我们的目标就是，提高网络计算资源的利用率，在计算率不变的情况下，提高网络的宽度和深度</p>
<p>作者认为，解决这种困难的方法就是，把全连接改成稀疏连接，卷积层也是稀疏连接，但是不对称稀疏数据数值计算效率低下，因为硬件全是针对密集矩阵优化的，所以，我们要找到卷积网络可以近似优化的最优局部稀疏结构，并且该结构下可以用现有的密度矩阵计算硬件实现，产生的结果就是Inception</p>
<h4 id="Inception结构"><a href="#Inception结构" class="headerlink" title="Inception结构"></a>Inception结构</h4><p><img src="https://img-blog.csdn.net/20180709151519421?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk1MzUwMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>首先看第一个结构，有四个通道，有1 * 1、3 * 3、5 * 5卷积核，该结构有几个特点：</p>
<ul>
<li>使用这些大小卷积核，没有什么特殊含义，主要方便对齐，只要padding = 0、1、2，就可以得到相同大小的特征图，可以顺利concat</li>
<li>采用大小不同的卷积核，以为着感受野的大小不同，就可以得到不同尺度的特征</li>
<li>采用比较大的卷积核，即5 * 5，意味着有些相关性可能隔得比较远，用大的卷积核才能学到此特征</li>
</ul>
<p>当时，这个结构有个缺点，5 * 5的卷积核的计算量太大。那么作者想到的第二个结构，用1 * 1的卷积核进行降维。</p>
<p>这个1 * 1的卷积核，它的作用就是：</p>
<ul>
<li>降低维度，减少计算瓶颈</li>
<li>增加网络层数，提高网络的表达能力。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/07/softmax/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/softmax/" itemprop="url">softmax</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-07T10:52:19+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="softmax交叉熵损失函数求导"><a href="#softmax交叉熵损失函数求导" class="headerlink" title="softmax交叉熵损失函数求导"></a>softmax交叉熵损失函数求导</h3><p>softmax经常被添加在分类任务的神经网络的输出层中，神经网络的反向传播中关键的步骤就是求导。</p>
<p>softmax函数：</p>
<p>一般在神经网络中，softmax可以作为分类任务的输出层。其实可以认为softmax输出的是几个类别选择的概率，比如我有一个分类任务，要分为三个类，softmax函数可以根据他们的相对大小，输出三个类别选取的概率，并且概率和为1。</p>
<p>公式：$S_{i}=\frac{e^{z_{i}}}{\sum_{k} e^{z_{k}}}$</p>
<p>$S_{}i$ 代表的是第i个神经元的输出，其实就是在输出后面套一个这个函数</p>
<p>首先是一个神经元的输出，一个神经元如下图：</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1565146619562.png" alt="1565146619562"></p>
<p>神经元的输出设为：$z_{i}=\sum_{j} w_{i j} x_{i j}+b$</p>
<p>其中 $w_{ij}$ 是第i个神经元的第j个权重，b是偏移值， $z_{i}$ 表示该网络的第i个输出</p>
<p>给这个输出加上一个softmax函数，就是$a_{i}=\frac{e^{z_{i}}}{\sum_{k} e^{z_{k}}}$</p>
<p>其中 $a_{i}$ 代表softmax的第i个输出值，右侧就是套用了softmax函数</p>
<h4 id="损失函数-loss-function"><a href="#损失函数-loss-function" class="headerlink" title="损失函数 loss function"></a>损失函数 loss function</h4><p>在神经网络反向传播中，要求一个损失函数，这个损失函数其实表示的是真实值与网络的估计值的误差，知道误差了，才能知道怎样去修改网络中的权重。</p>
<p>损失函数可以有很多形式，这里使用的是交叉熵函数，主要是由于这个求导结果比较简单，易于计算，并且交叉熵解决某些损失函数学习缓慢的问题。交叉熵的函数是这样的：$C=-\sum_{i} y_{i} \ln a_{i}$</p>
<p>其中 $y_{i}$ 表示真实的分类结果。</p>
<h4 id="推导过程："><a href="#推导过程：" class="headerlink" title="推导过程："></a>推导过程：</h4><p>首先，我们要明确一下我们要求什么，我们要求的是我们的loss对于神经元输出( $ z_{i}$ )的梯度，即：$\frac{\partial C}{\partial z_{i}}$</p>
<p>根据复合函数求导法则：$\frac{\partial C}{\partial z_{i}}=\sum_{j}\left(\frac{\partial C_{j}}{\partial a_{j}} \frac{\partial a_{j}}{\partial z_{i}}\right)$</p>
<p>这里为什么是 $a_{j}$ 而不是 $a_{i}$ ，这里要看一下softmax的公式了，因为softmax公式的特性，它的分母包含了所有神经元的输出，所以，对于不等于i的其他输出里面，也包含着 $z_{i}$，所有的a都要纳入到计算范围中，并且后面的计算可以看到需要分为 $i = j$ 和 $i \not= j$ 两种情况求导。</p>
<p>下面我们一个一个推：</p>
<p>$\frac{\partial C_{j}}{\partial a_{j}}=\frac{\partial\left(-y_{j} \ln a_{j}\right)}{\partial a_{j}}=-y_{j} \frac{1}{a_{j}}$</p>
<p>第二个稍微复杂一点，我们先把他分为两种情况：</p>
<ol>
<li><p>如果 $i = j$ :</p>
<p>$\frac{\partial a_{i}}{\partial z_{i}}=\frac{\partial\left(\frac{e^{z_{i}}}{\sum_{k} e^{z_{k}}}\right)}{\partial z_{i}}=\frac{\sum_{k} e^{z_{k}} e^{z_{i}}-\left(e^{z_{i}}\right)^{2}}{\left(\sum_{k} e^{z_{k}}\right)^{2}}=\left(\frac{e^{z_{i}}}{\sum_{k} e^{z_{k}}}\right)\left(1-\frac{e^{z_{i}}}{\sum_{k} e^{z_{k}}}\right)=a_{i}\left(1-a_{i}\right)$</p>
</li>
<li><p>如果 $i \not= j$ :</p>
<p>$\frac{\partial a_{j}}{\partial z_{i}}=\frac{\partial\left(\frac{e^{z_{j}}}{\sum_{k} e^{z_{k}}}\right)}{\partial z_{i}}=-e^{z_{j}}\left(\frac{1}{\sum_{k} e^{z_{k}}}\right)^{2} e^{z_{i}}=-a_{i} a_{j}$</p>
</li>
</ol>
<p>接下来，只需要组合两个式子：</p>
<p>$\frac{\partial C}{\partial z_{i}}=\sum_{j}\left(\frac{\partial C_{j}}{\partial a_{j}} \frac{\partial a_{j}}{\partial z_{i}}\right)=\sum_{j=\dot{\psi}}\left(\frac{\partial C_{j}}{\partial a_{j}} \frac{\partial a_{j}}{\partial z_{i}}\right)+\sum_{i=j}\left(\frac{\partial C_{j}}{\partial a_{j}} \frac{\partial a_{j}}{\partial z_{i}}\right)$</p>
<p>$=\sum_{j=\dot{y}}-y_{j} \frac{1}{a_{j}}\left(-a_{i} a_{j}\right)+\left(-y_{i} \frac{1}{a_{i}}\right)\left(a_{i}\left(1-a_{i}\right)\right)$</p>
<p>$=\sum_{j=i} a_{i} y_{j}+\left(-y_{i}\left(1-a_{i}\right)\right)$</p>
<p>$=\sum_{j=\dot{\psi}} a_{i} y_{j}+a_{i} y_{i}-y_{i}$</p>
<p>$=a_{i} \sum_{j} y_{j}-y_{i}$</p>
<p>最后的结果看起来简单了很多，最后，针对分类问题，我们给定的结果 $y_{i}$ 最终只会哟一个类别是1，其他类别都是0，因此对于分类问题，这个梯度等于：</p>
<p>$\frac{\partial C}{\partial z_{i}}=a_{i}-y_{i}$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/07/29/OverTheWall/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/29/OverTheWall/" itemprop="url">OverTheWall</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-29T19:38:03+08:00">
                2019-07-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="匿名术"><a href="#匿名术" class="headerlink" title="匿名术"></a>匿名术</h3><ul>
<li><p>网络层面：隐藏公网IP</p>
</li>
<li><p>操作系统层面</p>
</li>
<li><p>个人软件层面</p>
</li>
<li><p>通讯工具层面</p>
</li>
<li><p>通讯工具层面</p>
</li>
</ul>
<h3 id="DNS欺骗（DNS污染）"><a href="#DNS欺骗（DNS污染）" class="headerlink" title="DNS欺骗（DNS污染）"></a>DNS欺骗（DNS污染）</h3><p>DNS欺骗又称DNS Spoofing，DNS污染又称为“域名缓存投毒”。如果你使用的是翻墙外的DNS服务器，那么，你每次进行DNS查询，都会要经过国际出口。这时候GFW会通过技术手段伪造DNS的查询结果——使得你查询到的网站IP是错误的。如此一来，你自然就无法访问该网站。</p>
<p>DNS劫持不同于DNS污染。DNS劫持是指DNS服务器上的记录被认为修改成错误的，把某些敏感网站的记录修改成错误的。</p>
<h3 id="代理软件"><a href="#代理软件" class="headerlink" title="代理软件"></a>代理软件</h3><h4 id="代理的工作原理"><a href="#代理的工作原理" class="headerlink" title="代理的工作原理"></a>代理的工作原理</h4><p>假设你想通过翻墙代理访问某个被墙的网站，这时候会经历如下几个步骤：</p>
<ul>
<li>你的上网软件（通常是浏览器）会把数据发送给你电脑中的代理工具</li>
<li>该工具把数据进行<strong>加密</strong>，然后发送给<strong>国外</strong>的某个代理服务器</li>
<li>该代理服务器吧数据解密，然后发送给你要访问的网站</li>
<li>从该网站回传的数据，也是经过上述过程，最终回到你的浏览器</li>
</ul>
<h4 id="代理的分类"><a href="#代理的分类" class="headerlink" title="代理的分类"></a>代理的分类</h4><p>按照是否加密，代理软件可分为加密代理和不加密代理两种</p>
<p>按照协议类型，常见的有HTTP和SOCKS代理。如果你纯粹用浏览器翻墙，HTTP代理就够用了。如果还需要使用其他软件翻墙，那就得使用SOCKS代理</p>
<h4 id="代理工具的获取"><a href="#代理工具的获取" class="headerlink" title="代理工具的获取"></a>代理工具的获取</h4>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/07/29/论文中的Python知识点/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/29/论文中的Python知识点/" itemprop="url">论文中的Python知识点</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-29T09:51:31+08:00">
                2019-07-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1-元类"><a href="#1-元类" class="headerlink" title="1. 元类"></a>1. 元类</h3><p>首先理解python中的类，用class修饰的都可以叫做类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Class</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">c = Class()</span><br><span class="line">Class.b = <span class="number">2</span></span><br><span class="line">print(c.b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># out 2</span></span><br></pre></td></tr></table></figure>

<p>我们平时用的类都是实例化以后的类，可以在任何时候动态的创建类，通常情况我们都是这样c=Class(),python解释器会将它认为是创建类，可是解释器本身是如何创建类的，答案是利用type</p>
<p>type平时我们可能认为是查看对象的类型，例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(type(c))</span><br><span class="line"><span class="comment"># out:</span></span><br><span class="line"><span class="comment"># &lt;class '__main__.Class'&gt;</span></span><br><span class="line">print(type(Class))</span><br><span class="line"><span class="comment"># out:</span></span><br><span class="line"><span class="comment"># &lt;class 'type'&gt;</span></span><br></pre></td></tr></table></figure>

<p>所以，Class的类型是type，我们可以用type直接生成一个类</p>
<p>type(类名, 父类的元组（针对继承的情况，可以为空），包含属性的字典（名称和值）)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Class_type = type(<span class="string">'Class_type'</span>, (), &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;)</span><br><span class="line">c_t = Class_type()</span><br><span class="line">print(c_t.a)</span><br><span class="line"><span class="comment"># out：1</span></span><br></pre></td></tr></table></figure>

<p><strong>元类</strong>：就是用来创建这些类（对象）的类，元类就是类的类</p>
<p>type就是所有类的元类，可以理解为所有的类都是由type创建的，我们也可以创建自己的元类，这个类需要继承在type</p>
<p>__metaclass__属性</p>
<p>Class中有<em>\</em>metaclass__这个属性吗？如果是，Python会在内存中通过<em>\</em>metaclass__创建一个名字为Test的类对象<br> 如果Python没有找到<em>\</em>metaclass__，它会继续在Base（父类）中寻找<em>\</em>metaclass__属性，并尝试做和前面同样的操作。<br> 如果Python在任何父类中都找不到<em>\</em>metaclass__，它就会在模块层次中去寻找<em>\</em>metaclass__，并尝试做同样的操作。<br> 如果还是找不到<em>\</em>metaclass__,Python就会用内置的type来创建这个类对象</p>
<p>那么<em>\</em>metaclass__是什么？</p>
<p>答：就是可以创建类的东西，类是由type创建的，所以<em>\</em>metaclass__内部一定要返回一个类，它可以是一个函数，也可以是一个类，而这个类就是我们自定义的元类，这个类必须继承自type</p>
<p>通常元类用来创建API是非常好的选择，使用元类的编写很复杂，但使用者可以非常简洁的调用API</p>
<h4 id="abc-ABCMeta："><a href="#abc-ABCMeta：" class="headerlink" title="abc.ABCMeta："></a>abc.ABCMeta：</h4><p>简单的说ABCMeta就是让你的类变成一个纯虚类，子类必须实现某个方法，这个方法在父类中用@abc.abstractmethod修饰</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> abc</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span><span class="params">(object,metaclass=abc.ABCMeta)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_a</span><span class="params">(self,data)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param data:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"><span class="meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_b</span><span class="params">(self,data,out)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param data:</span></span><br><span class="line"><span class="string">        :param out:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_d</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'func_d in base'</span>)</span><br></pre></td></tr></table></figure>

<p>你可以实现这两个虚方法，也可以不实现<br>这样在Base的子类中就必须实现func_a，func_b2个函数，否则就会报错</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sub</span><span class="params">(Base)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_a</span><span class="params">(self,data)</span>:</span></span><br><span class="line">        print(<span class="string">'over write func_a'</span>,data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_b</span><span class="params">(self,data,out)</span>:</span></span><br><span class="line">        print(<span class="string">'over write func_b'</span>)</span><br></pre></td></tr></table></figure>

<p>如果还想调用虚类的方法用super</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func_b</span><span class="params">(self,data,out)</span>:</span></span><br><span class="line">        super(Sub,self).func_b(data,out)</span><br><span class="line">        print(<span class="string">'over write func_b'</span>)</span><br></pre></td></tr></table></figure>

<p>还有一种方法是，注册虚子类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Register</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_c</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'func_c in third class'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func_a</span><span class="params">(self,data)</span>:</span></span><br><span class="line">        print(<span class="string">'func_a in third class'</span>,data)</span><br><span class="line">Base.register(Register)</span><br></pre></td></tr></table></figure>

<p>这样调用issubclass(), issubinstance()进行判断时仍然返回真值</p>
<h3 id="2-tensorflow中的“tf-name-scope-”有什么用？"><a href="#2-tensorflow中的“tf-name-scope-”有什么用？" class="headerlink" title="2. tensorflow中的“tf.name_scope()”有什么用？"></a>2. tensorflow中的“tf.name_scope()”有什么用？</h3><h4 id="2-1-tf-name-scope-命名空间的实际作用"><a href="#2-1-tf-name-scope-命名空间的实际作用" class="headerlink" title="2.1. tf.name_scope()命名空间的实际作用"></a><strong>2.1. tf.name_scope()命名空间的实际作用</strong></h4><p>（1）在某个tf.name_scope()指定的区域中定义的所有对象及各种操作，他们的“name”属性上会增加该命名区的区域名，用以区别对象属于哪个区域； </p>
<p>（2）将不同的对象及操作放在由tf.name_scope()指定的区域中，便于在tensorboard中展示清晰的逻辑关系图，这点在复杂关系图中特别重要。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf;  </span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 无tf.name_scope()</span></span><br><span class="line">a = tf.constant(<span class="number">1</span>,name=<span class="string">'my_a'</span>) <span class="comment">#定义常量</span></span><br><span class="line">b = tf.Variable(<span class="number">2</span>,name=<span class="string">'my_b'</span>) <span class="comment">#定义变量</span></span><br><span class="line">c = tf.add(a,b,name=<span class="string">'my_add'</span>) <span class="comment">#二者相加（操作）</span></span><br><span class="line">print(<span class="string">"a.name = "</span>+a.name)</span><br><span class="line">print(<span class="string">"b.name = "</span>+b.name)</span><br><span class="line">print(<span class="string">"c.name = "</span>+c.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 有tf.name_scope()</span></span><br><span class="line"><span class="comment"># with tf.name_scope('cgx_name_scope'): #定义一块名为cgx_name_scope的区域，并在其中工作</span></span><br><span class="line"><span class="comment">#     a = tf.constant(1,name='my_a')</span></span><br><span class="line"><span class="comment">#     b = tf.Variable(2,name='my_b')</span></span><br><span class="line"><span class="comment">#     c = tf.add(a,b,name='my_add')</span></span><br><span class="line"><span class="comment"># print("a.name = "+a.name)</span></span><br><span class="line"><span class="comment"># print("b.name = "+b.name)</span></span><br><span class="line"><span class="comment"># print("c.name = "+c.name)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存graph用于tensorboard绘图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"./test"</span>,sess.graph)</span><br><span class="line">    print(sess.run(c))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># 无tf.name_scope()</span></span><br><span class="line">a.name = my_a:<span class="number">0</span></span><br><span class="line">b.name = my_b:<span class="number">0</span></span><br><span class="line">c.name = my_add:<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有tf.name_scope()</span></span><br><span class="line">a.name = cgx_name_scope/my_a:<span class="number">0</span></span><br><span class="line">b.name = cgx_name_scope/my_b:<span class="number">0</span></span><br><span class="line">c.name = cgx_name_scope/my_add:<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>从输出结果可以看出，在tf.name_scope()下的所有对象和操作，其name属性前都加了cgx_name_scope，用以表示这些内容全在其范围下。<br>下图展示了两种情况的tensorboard差异，差别一目了然。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/14029140-b8d46d738bf1230c.jpg?imageMogr2/auto-orient/" alt="img"></p>
<h4 id="2-2-name-scope-只决定“对象”属于哪个范围，并不会对“对象”的“作用域”产生任何影响。"><a href="#2-2-name-scope-只决定“对象”属于哪个范围，并不会对“对象”的“作用域”产生任何影响。" class="headerlink" title="2.2. name_scope()只决定“对象”属于哪个范围，并不会对“对象”的“作用域”产生任何影响。"></a><strong>2.2. name_scope()只决定“对象”属于哪个范围，并不会对“对象”的“作用域”产生任何影响。</strong></h4><p>tf.name_scope()只是规定了对象和操作属于哪个区域，但这并不意味着他们的作用域也只限于该区域（with的这种写法很容易让人产生这种误会），不要将其和“全局变量、局部变量”的概念搞混淆，两者完全不是一回事。在name_scope中定义的对象，从被定义的位置开始，直到后来某个地方对该对象重新定义，中间任何地方都可以使用该对象。本质上name_scope只对对象的name属性进行圈定，并不会对其作用域产生任何影响。这就好比甲、乙、丙、丁属于陈家，这里“陈家”就是一个name_scope划定的区域，虽然他们只属于陈家，但他们依然可以去全世界的任何地方，并不会只将他们限制在陈家范围。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'cgx_1'</span>):</span><br><span class="line">    a = tf.Variable(tf.constant(<span class="number">4</span>), name=<span class="string">'my_a'</span>)</span><br><span class="line">    print(<span class="string">'case1: a.name = '</span> + a.name)</span><br><span class="line">print(<span class="string">"case2: a.name = "</span> + a.name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'cgx_2'</span>):</span><br><span class="line">    print(<span class="string">"case3: a.name = "</span> + a.name)</span><br><span class="line">    a = tf.Variable(tf.constant(<span class="number">4</span>), name=<span class="string">'my_a'</span>)</span><br><span class="line">    print(<span class="string">"case4: a.name = "</span> + a.name)</span><br><span class="line">print(<span class="string">"case5: a.name = "</span> + a.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># out：</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">case1: a.name = cgx_1/my_a:0</span></span><br><span class="line"><span class="string">case2: a.name = cgx_1/my_a:0</span></span><br><span class="line"><span class="string">case3: a.name = cgx_1/my_a:0</span></span><br><span class="line"><span class="string">case4: a.name = cgx_2/my_a:0</span></span><br><span class="line"><span class="string">case5: a.name = cgx_2/my_a:0</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p>（1）程序首先指定了命名区域cgx_1，并在其中定义了变量a，紧接着case1直接在cgx_1中输出a.name = cgx_1/my_a:0，这很好理解，跟想象的一样；<br> （2）case2在cgx_1之外的公共区域也输出了相同的a.name，<strong>这就说明a的作用范围并没有被限制在cgx_1中</strong>；<br> （3）接着程序又新指定了命名区域cgx_2，并在其中执行case3，输出a.name，结果还是和case1和case2完全相同，实际上还是最前面定义的那个a，这更进一步说明<strong>name_scope不会对对象的作用域产生影响</strong>；<br> （4）★★接着在cgx_2中<strong>重新定义了变量“a”</strong>，紧接着就执行case4，输出a.name = cgx_2/my_a:0，可见此时的结果与前面三个case就不同了，说明这里<strong>新定义的a覆盖了前面的a，即使他们在两个完全独立的name_scope中</strong>；<br> （5）case5输出的结果与case4结果相同，这已经无须解释了。</p>
<h4 id="2-3-tf-name-scope-‘cgx-scope’-语句重复执行几次，就会生成几个独立的命名空间，尽管表面上看起来都是“cgx-scope”，实际上tensorflow在每一次执行相同语句都会在后面加上“-序数”，加以区别。"><a href="#2-3-tf-name-scope-‘cgx-scope’-语句重复执行几次，就会生成几个独立的命名空间，尽管表面上看起来都是“cgx-scope”，实际上tensorflow在每一次执行相同语句都会在后面加上“-序数”，加以区别。" class="headerlink" title="2.3 tf.name_scope(‘cgx_scope’)语句重复执行几次，就会生成几个独立的命名空间，尽管表面上看起来都是“cgx_scope”，实际上tensorflow在每一次执行相同语句都会在后面加上“_序数”，加以区别。"></a><strong>2.3 tf.name_scope(‘cgx_scope’)语句重复执行几次，就会生成几个独立的命名空间，尽管表面上看起来都是“cgx_scope”，实际上tensorflow在每一次执行相同语句都会在后面加上“_序数”，加以区别。</strong></h4><p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'cgx_scope'</span>):</span><br><span class="line">    a = tf.Variable(<span class="number">1</span>, name=<span class="string">'my_a'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'cgx_scope'</span>):</span><br><span class="line">    b = tf.Variable(<span class="number">2</span>, name=<span class="string">'my_b'</span>)</span><br><span class="line"></span><br><span class="line">c = tf.add(a, b, name=<span class="string">'my_add'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'a.name = '</span> + a.name)</span><br><span class="line">print(<span class="string">'b.name = '</span> + b.name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># out:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">a.name = cgx_scope/my_a:0</span></span><br><span class="line"><span class="string">b.name = cgx_scope_1/my_b:0</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p>（1）指定了“<strong>cgx_scope</strong>”命名区域，并在其中定义变量a；<br> （2）又指定了相同名称的“<strong>cgx_scope</strong>”命名区域，并在其中定义变量b；<br> （3）输出a.name = cgx_scope/my_a:0和b.name = cgx_scope_1/my_b:0，<strong>可见b.name已经自动加了“_1”，这是tensorflow的特点，自动检测是否重复，有重复就自动增加数字作为标记</strong>。</p>
<h3 id="3-tf-shape"><a href="#3-tf-shape" class="headerlink" title="3. tf.shape()"></a>3. tf.shape()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.shape(</span><br><span class="line">    input,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    out_type=tf.int32</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ul>
<li>将矩阵的维度输出为一个维度矩阵</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line">A = np.array([[[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]],</span><br><span class="line">              [[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]],</span><br><span class="line">              [[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]]])</span><br><span class="line"></span><br><span class="line">t = tf.shape(A)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(t))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># out: [3 2 3]</span></span><br></pre></td></tr></table></figure>

<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><ul>
<li>input：张量或稀疏张量</li>
<li>name：op 的名字，用于tensorboard中</li>
<li>out_type：默认为tf.int32</li>
</ul>
<h4 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h4><ul>
<li>返回out_type类型的张量</li>
</ul>
<h3 id="4-tf-reshape"><a href="#4-tf-reshape" class="headerlink" title="4. tf.reshape()"></a>4. tf.reshape()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.reshape(tensor,shape,name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><ul>
<li><p>tensor：输入张量</p>
</li>
<li><p>shape：列表形式，可以存在-1</p>
<p>-1 代表的含义是不用我们自己指定这一维的大小，函数会自动计算，但列表中只能存在一个-1</p>
</li>
<li><p>name：命名</p>
</li>
</ul>
<h4 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h4><p>将tensor变换为参数shape的形式</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line">print(<span class="string">'a = '</span>, a)</span><br><span class="line"></span><br><span class="line">b = a.reshape((<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">print(<span class="string">'b = '</span>, b)</span><br><span class="line"></span><br><span class="line">c = a.reshape((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">print(<span class="string">'c = '</span>, c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># out：</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">a =  [0 1 2 3 4 5 6 7]</span></span><br><span class="line"><span class="string">b =  [[0 1 2 3]</span></span><br><span class="line"><span class="string"> [4 5 6 7]]</span></span><br><span class="line"><span class="string">c =  [[[0 1]</span></span><br><span class="line"><span class="string">  [2 3]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> [[4 5]</span></span><br><span class="line"><span class="string">  [6 7]]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<h3 id="5-tf-control-dependencies"><a href="#5-tf-control-dependencies" class="headerlink" title="5. tf.control_dependencies()"></a>5. tf.control_dependencies()</h3><p>在有些机器学习程序中我们想要指定某些操作执行的依赖关系，这时我们可以使用tf.control_dependencies()来实现。 </p>
<p>control_dependencies(control_inputs)返回一个控制依赖的上下文管理器，使用with关键字可以让在这个上下文环境中的操作都在control_inputs 执行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b, c]):</span><br><span class="line">  <span class="comment"># `d` and `e` will only run after `a`, `b`, and `c` have executed.</span></span><br><span class="line">  d = ...</span><br><span class="line">  e = ...</span><br></pre></td></tr></table></figure>

<p>可以嵌套<code>control_dependencies</code> 使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</span><br><span class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></span><br><span class="line">  <span class="keyword">with</span> g.control_dependencies([c, d]):</span><br><span class="line">    <span class="comment"># Ops constructed here run after `a`, `b`, `c`, and `d`.</span></span><br></pre></td></tr></table></figure>

<p>可以传入<code>None</code> 来消除依赖：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b]):</span><br><span class="line">  <span class="comment"># Ops constructed here run after `a` and `b`.</span></span><br><span class="line">  <span class="keyword">with</span> g.control_dependencies(<span class="literal">None</span>):</span><br><span class="line">    <span class="comment"># Ops constructed here run normally, not waiting for either `a` or `b`.</span></span><br><span class="line">    <span class="keyword">with</span> g.control_dependencies([c, d]):</span><br><span class="line">      <span class="comment"># Ops constructed here run after `c` and `d`, also not waiting</span></span><br><span class="line">      <span class="comment"># for either `a` or `b`.</span></span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：<br>控制依赖只对那些在上下文环境中<strong>建立</strong>的操作有效，仅仅在context中<strong>使用</strong>一个操作或张量是没用的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># WRONG</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></span><br><span class="line">  t = tf.matmul(tensor, tensor)</span><br><span class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">    <span class="comment"># The matmul op is created outside the context, so no control</span></span><br><span class="line">    <span class="comment"># dependency will be added.</span></span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># RIGHT</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(pred, tensor)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.control_dependencies([pred]):</span><br><span class="line">    <span class="comment"># The matmul op is created in the context, so a control dependency</span></span><br><span class="line">    <span class="comment"># will be added.</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(tensor, tensor)</span><br></pre></td></tr></table></figure>

<p>例子：<br>在训练模型时我们每步训练可能要执行两种操作，<code>op a, b</code> 这时我们就可以使用如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.control_dependencies([a, b]):</span><br><span class="line">    c= tf.no_op(name=<span class="string">'train'</span>)<span class="comment">#tf.no_op；什么也不做</span></span><br><span class="line">sess.run(c)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">c= tf.no_op提供一个什么都不做的节点，该节点属于整个执行的流程图，但是操作a和操作b不是流程图中的一部分，但是为了确保操作a和操作b在某一个环节（此时可能是个未知环节）之前执行，所以提供一个什么都不做的环节c(前面称为节点)，确保操作a和操作b在c之前能够完成。而环节c可以插入流程图中。在整个流程图运行起来时，当运行到c时，就确保a,b操作先执行。 我只是根据官方文档以及常用用法猜测，不一定对。</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p>在这样简单的要求下，可以将上面代码替换为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c= tf.group([a, b])</span><br><span class="line">sess.run(c)</span><br></pre></td></tr></table></figure>

<h3 id="set-shape-与reshape"><a href="#set-shape-与reshape" class="headerlink" title="set_shape()与reshape()"></a>set_shape()与reshape()</h3><p>set_shape() 方法更新张量对象的静态形状，通常用于在无法直接推断时提供其他形状信息。它不会改变张量的动态形状</p>
<p>reshape()操作创建一个具有不同动态形状的新张量</p>
<h3 id="tf-image-resize-images"><a href="#tf-image-resize-images" class="headerlink" title="tf.image.resize_images()"></a>tf.image.resize_images()</h3><p>改变图片尺寸的大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> img_resized = tf.image.resize_images(image_data, [<span class="number">300</span>, <span class="number">300</span>], method=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 第一个参数为袁术图像的大小</span></span><br><span class="line"><span class="comment"># 第二三个分别为调整后图像的大小</span></span><br><span class="line"><span class="comment"># method参数给出了调整图像大小的方向</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">method = 0, 双线性插值法</span></span><br><span class="line"><span class="string">method = 1, 最近邻居法</span></span><br><span class="line"><span class="string">method = 2, 双三次插值法</span></span><br><span class="line"><span class="string">method = 3, 面积插值法</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<h3 id="xreadline-与-readlines"><a href="#xreadline-与-readlines" class="headerlink" title="xreadline() 与 readlines()"></a>xreadline() 与 readlines()</h3><p>xreadlines返回的是一个生成器类型</p>
<p>readlines()返回的是一个列表</p>
<p>但是使用时是相同的</p>
<h3 id="os-path-join"><a href="#os-path-join" class="headerlink" title="os.path.join()"></a>os.path.join()</h3><p>连接两个或更多的路径名组件</p>
<ul>
<li>如果各组件名首字母不包含‘/’，则函数会自动加上</li>
<li>如果有一个组件是一个绝对路径，则在它之前的所有组件均会被舍弃</li>
<li>如果最后䘝组件为空，则生成的路径以一个‘/’分隔符结尾</li>
</ul>
<h3 id="tf-train-slice-input-producer"><a href="#tf-train-slice-input-producer" class="headerlink" title="tf.train.slice_input_producer()"></a>tf.train.slice_input_producer()</h3><p>是一个tensor生成器，作用是按照设定，每次从一个tensor列表中按顺序或者随机抽取出一个tensor放入文件名队列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slice_input_producer(tensor_list, num_epochs=<span class="literal">None</span>, shuffle=<span class="literal">True</span>, seed=<span class="literal">None</span>, capacity=<span class="number">32</span>, shared_name=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<h3 id="tf-read-file-amp-tf-image-decode-jpeg-处理图片"><a href="#tf-read-file-amp-tf-image-decode-jpeg-处理图片" class="headerlink" title="tf.read_file() &amp; tf.image.decode_jpeg()处理图片"></a>tf.read_file() &amp; tf.image.decode_jpeg()处理图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file_contents = tf.read_file(filename)</span><br><span class="line">image = tf.image.decode_png(file_contents)	<span class="comment"># 解码png格式</span></span><br></pre></td></tr></table></figure>

<h3 id="os-path-splitext"><a href="#os-path-splitext" class="headerlink" title="os.path.splitext()"></a>os.path.splitext()</h3><p><code>os.path.splitext(“文件路径”)</code>分离文件名与扩展名；默认返回<code>(frame,fextension)</code>元组，可做分片操作</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">path_01=<span class="string">'D:/User/wgy/workplace/data/notMNIST_large.tar.gar'</span></span><br><span class="line">path_02=<span class="string">'D:/User/wgy/workplace/data/notMNIST_large'</span></span><br><span class="line">root_01=os.path.splitext(path_01)</span><br><span class="line">root_02=os.path.splitext(path_02)</span><br><span class="line">print(root_01)</span><br><span class="line">print(root_02)</span><br><span class="line"></span><br><span class="line"><span class="comment"># out：</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">('D:/User/wgy/workplace/data/notMNIST_large.tar', '.gar')</span></span><br><span class="line"><span class="string">('D:/User/wgy/workplace/data/notMNIST_large', '')</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<h3 id="tf-concat"><a href="#tf-concat" class="headerlink" title="tf.concat()"></a>tf.concat()</h3><p><code>tf.concat([tensor1, tensor2, tensor3, ...], axis)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t1 = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]  </span><br><span class="line">t2 = [[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]  </span><br><span class="line">tf.concat([t1, t2], <span class="number">0</span>)  <span class="comment"># [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]  </span></span><br><span class="line">tf.concat([t1, t2], <span class="number">1</span>)  <span class="comment"># [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]</span></span><br><span class="line"><span class="comment"># axis = 0, 代表在第0个维度拼接</span></span><br><span class="line"><span class="comment"># axis = 1, 代表在第1个维度拼接</span></span><br></pre></td></tr></table></figure>

<p>对于一个二维矩阵，第0个维度代表最外层方括号所框下的子集，第一个维度代表内部方括号所框下的子集。<strong>维度越高，括号越小</strong></p>
<p>对于[ [ ], [ ]]和[[ ], [ ]]，低维拼接等于拿掉最外面括号，高维拼接是拿掉里面的括号(保证其他维度不变)。</p>
<p><strong>注意：tf.concat()拼接的张量只会改变一个维度，其他维度是保存不变的。</strong></p>
<p>比如两个shape为[2,3]的矩阵拼接，要么通过axis=0变成[4,3]，要么通过axis=1变成[2,6]。<strong>改变的维度索引对应axis的值。</strong></p>
<h3 id="tf-contrib-layers-batch-norm"><a href="#tf-contrib-layers-batch-norm" class="headerlink" title="tf.contrib.layers.batch_norm()"></a>tf.contrib.layers.batch_norm()</h3><h3 id="tf-contrib-layers-conv2d"><a href="#tf-contrib-layers-conv2d" class="headerlink" title="tf.contrib.layers.conv2d()"></a>tf.contrib.layers.conv2d()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tf.contrib.layers.conv2d(</span><br><span class="line">    inputs,			<span class="comment"># [batch_size] + input_spatial_shape + [in_channels]如果data_format不以“NC”（默认值）[batch_size, in_channels] + input_spatial_shape开头，或者 data_format以“NC”开头，则为形状等级N + 2的张量。</span></span><br><span class="line">    num_outputs,	<span class="comment"># 整数，输出过滤器的数量。</span></span><br><span class="line">    kernel_size,	<span class="comment"># N个正整数的序列，指定过滤器的空间维度。可以是单个整数，以指定所有空间维度的相同值。</span></span><br><span class="line">    stride=<span class="number">1</span>,	</span><br><span class="line">    padding=<span class="string">'SAME'</span>,</span><br><span class="line">    data_format=<span class="literal">None</span>,</span><br><span class="line">    rate=<span class="number">1</span>,</span><br><span class="line">    activation_fn=tf.nn.relu,</span><br><span class="line">    normalizer_fn=<span class="literal">None</span>,	<span class="comment"># 使用标准化功能代替biases。如果 normalizer_fn提供biases_initializer， biases_regularizer则忽略并且biases不创建也不添加。没有规范化器功能，默认设置为“无”</span></span><br><span class="line">    normalizer_params=<span class="literal">None</span>,		<span class="comment"># 规范化函数参数。</span></span><br><span class="line">    weights_initializer=initializers.xavier_initializer(),	<span class="comment"># 权重的初始化程序。</span></span><br><span class="line">    weights_regularizer=<span class="literal">None</span>,	<span class="comment"># 可选的权重正则化器。</span></span><br><span class="line">    biases_initializer=tf.zeros_initializer(),	<span class="comment"># 偏移量的初始化程序。如果没有跳过偏移量。</span></span><br><span class="line">    biases_regularizer=<span class="literal">None</span>,	<span class="comment"># 偏移量的可选正则化器。</span></span><br><span class="line">    reuse=<span class="literal">None</span>,					<span class="comment"># 是否应重用图层及其变量。必须给出能够重用层范围的能力</span></span><br><span class="line">    variables_collections=<span class="literal">None</span>,	<span class="comment"># 所有变量的集合的可选列表或包含每个变量的不同集合列表的字典。</span></span><br><span class="line">    outputs_collections=<span class="literal">None</span>,	<span class="comment"># 用于添加输出的集合。</span></span><br><span class="line">    trainable=<span class="literal">True</span>,				<span class="comment"># 如果True还将变量添加到图表集合中 GraphKeys.TRAINABLE_VARIABLES（请参阅tf.Variable）。</span></span><br><span class="line">    scope=<span class="literal">None</span>					<span class="comment"># 可选范围variable_scope。</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="tf-contrib-layers-variance-scaling-initializer"><a href="#tf-contrib-layers-variance-scaling-initializer" class="headerlink" title="tf.contrib.layers.variance_scaling_initializer()"></a>tf.contrib.layers.variance_scaling_initializer()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">variance_scaling_initializer(</span><br><span class="line">    factor=<span class="number">2.0</span>,</span><br><span class="line">    mode=<span class="string">'FAN_IN'</span>,</span><br><span class="line">    uniform=<span class="literal">False</span>,</span><br><span class="line">    seed=<span class="literal">None</span>,</span><br><span class="line">    dtype=tf.float32</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>方差缩放初始化</p>
<p>这种初始化方法比常规高斯分布初始化、阶段高斯分布初始化及Xavier初始化的泛华/缩放性能更好。粗略地说，方差缩放初始化根据每一层输入或输出的数量(在 TensorFlow 中默认为输入的数量)来调整初始随机权重的方差，从而帮助信号在不需要其他技巧(如梯度裁剪或批归一化)的情况下在网络中更深入地传播。</p>
<h3 id="tf-constant-initializer"><a href="#tf-constant-initializer" class="headerlink" title="tf.constant_initializer()"></a>tf.constant_initializer()</h3><p>初始化为常数，这个非常有用，通常偏置项就是用它初始化的。</p>
<p>由它衍生出的两个初始化方法：</p>
<ul>
<li>tf.zeros_initializer()， 也可以简写为tf.Zeros()</li>
<li>tf.ones_initializer(), 也可以简写为tf.Ones()</li>
</ul>
<h3 id="tf-contrib-layers-convolution"><a href="#tf-contrib-layers-convolution" class="headerlink" title="tf.contrib.layers.convolution()"></a>tf.contrib.layers.convolution()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def convolution(inputs,</span><br><span class="line">                num_outputs,</span><br><span class="line">                kernel_size,</span><br><span class="line">                stride=1,</span><br><span class="line">                padding=&apos;SAME&apos;,</span><br><span class="line">                data_format=None,</span><br><span class="line">                rate=1,</span><br><span class="line">                activation_fn=nn.relu,</span><br><span class="line">                normalizer_fn=None,</span><br><span class="line">                normalizer_params=None,</span><br><span class="line">                weights_initializer=initializers.xavier_initializer(),</span><br><span class="line">                weights_regularizer=None,</span><br><span class="line">                biases_initializer=init_ops.zeros_initializer(),</span><br><span class="line">                biases_regularizer=None,</span><br><span class="line">                reuse=None,</span><br><span class="line">                variables_collections=None,</span><br><span class="line">                outputs_collections=None,</span><br><span class="line">                trainable=True,</span><br><span class="line">                scope=None):</span><br></pre></td></tr></table></figure>

<h3 id="x-get-shape-as-list"><a href="#x-get-shape-as-list" class="headerlink" title="x.get_shape().as_list()"></a>x.get_shape().as_list()</h3><p>x.get_shape()，只有tensor才可以使用这种方法，返回的是一个元组</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">a_array = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">b_list = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]]</span><br><span class="line">c_tensor = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">print(c_tensor.get_shape())</span><br><span class="line">print(c_tensor.get_shape().as_list())</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(tf.shape(a_array)))</span><br><span class="line">    print(sess.run(tf.shape(b_list)))</span><br><span class="line">    print(sess.run(tf.shape(c_tensor)))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># out:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">(2, 3)</span></span><br><span class="line"><span class="string">[2, 3]</span></span><br><span class="line"><span class="string">[2 3]</span></span><br><span class="line"><span class="string">[2 3]</span></span><br><span class="line"><span class="string">[2 3]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>

<p>只能用于tensor来返回shape，但是是一个元组，需要通过as_list()的操作转换成list.</p>
<h3 id="tf-image-rgb-to-grayscale"><a href="#tf-image-rgb-to-grayscale" class="headerlink" title="tf.image.rgb_to_grayscale()"></a>tf.image.rgb_to_grayscale()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.image.rgb_to_grayscale(</span><br><span class="line">    images,		<span class="comment"># 要转换的RGB张量，最后一个维度的大小必须为3，并且应该包含RGB值</span></span><br><span class="line">    name=<span class="literal">None</span>	<span class="comment"># 操作的名称（可选）</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 返回：该函数返回转换后的灰度图像</span></span><br></pre></td></tr></table></figure>

<p>将一个或多个图像从RGB转化为灰度</p>
<p>输出与images具有相同DType和等级的张量，最后一个维度大小为1，包含像素的灰度值。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/07/29/双边网格/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/29/双边网格/" itemprop="url">双边网格</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-29T08:42:33+08:00">
                2019-07-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1-双边滤波器的快速近似"><a href="#1-双边滤波器的快速近似" class="headerlink" title="1. 双边滤波器的快速近似"></a>1. 双边滤波器的快速近似</h3><p>在对双边网格做总结之前，先介绍一下双边滤波器的快速近似方法（<a href="http://www.cis.rit.edu/~cnspci/references/dip/filtering/paris2006.pdf），它是双边网格的雏形，是一个将双边滤波器扩展大高维空间进行线性卷积的方法。而双边网格的坐着在这篇文章的基础上，将高维空间数据映射成3D" target="_blank" rel="noopener">http://www.cis.rit.edu/~cnspci/references/dip/filtering/paris2006.pdf），它是双边网格的雏形，是一个将双边滤波器扩展大高维空间进行线性卷积的方法。而双边网格的坐着在这篇文章的基础上，将高维空间数据映射成3D</a> array，进而提出了双边网格</p>
<h4 id="1-1-双边滤波器"><a href="#1-1-双边滤波器" class="headerlink" title="1.1 双边滤波器"></a>1.1 双边滤波器</h4><p>简单的说，双边滤波器是一种高斯滤波器的扩展。传统的高斯滤波器有一个高斯核，通过对空间相邻像素点以高斯函数位权值取均值，实现对图像的平滑。由于传统高斯滤波器只考虑了空域的信息，尽管它能实现图像的有效平滑，但同时也模糊了边缘信息。双边滤波器的原理即在传统高斯滤波器的基础上添加了一个表征亮度差异的高斯核，即既考虑了空间的信息，又考虑了值域的信息。在灰度差别不大的范围的范围，表征亮度高斯核的权值较大，接近于1，因此双边滤波器退化为传统的高斯滤波器，实现对图像的平滑；在边缘部分，尽管相邻像素点的空间接近，空间距离小，但由于边缘部分，灰度差别比较大，因此在值域上距离较大，所以新加入的高斯核使得在该部分不执行平滑和粗粒，保留图像的边缘。所以说，双边滤波是一种非线性（两个高斯核的乘积）的铝箔方法，是结合图像的空间邻近度和像素值相似度的一种折中处理，同时考虑空域信息和灰度相似性，达到保边去噪的目的</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Mariana</p>
              <p class="site-description motion-element" itemprop="description">a study blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mariana</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
