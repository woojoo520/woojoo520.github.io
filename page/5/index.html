<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="a study blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Celery Fairy">
<meta property="og:url" content="https://woojoo520.github.io/page/5/index.html">
<meta property="og:site_name" content="Celery Fairy">
<meta property="og:description" content="a study blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Celery Fairy">
<meta name="twitter:description" content="a study blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://woojoo520.github.io/page/5/">





  <title>Celery Fairy</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <a href="https://github.com/woojoo520" class="github-corner" aria-label="View source on GitHub">
      <svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/>
      </svg>
    </a>
    <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Celery Fairy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Celery's Blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/19/LeetCode-Day4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/19/LeetCode-Day4/" itemprop="url">LeetCode-Day4</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-19T09:10:55+08:00">
                2019-08-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="14-最长公共前缀"><a href="#14-最长公共前缀" class="headerlink" title="14. 最长公共前缀"></a>14. 最长公共前缀</h3><p>编写一个函数来查找字符串数组中的最长公共前缀。</p>
<p>如果不存在公共前缀，返回空字符串 “”。</p>
<p>示例 1:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]</span><br><span class="line">输出: &quot;fl&quot;</span><br></pre></td></tr></table></figure>

<p>示例 2:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入: [&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;]</span><br><span class="line">输出: &quot;&quot;</span><br><span class="line">解释: 输入不存在公共前缀。</span><br></pre></td></tr></table></figure>

<p><strong>说明:</strong></p>
<p>所有输入只包含小写字母 a-z 。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">longestCommonPrefix</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; strs)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(strs.empty()) <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j&lt;strs[<span class="number">0</span>].size(); j++)</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;strs.size(); i++)</span><br><span class="line">                <span class="keyword">if</span>(j&gt;strs[i].size() || strs[<span class="number">0</span>][j] != strs[i][j])</span><br><span class="line">                    <span class="keyword">return</span> strs[i].substr(<span class="number">0</span>,j);</span><br><span class="line">        <span class="keyword">return</span> strs[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>解释：</p>
<p>如果strs的长度为0，直接返回“”</p>
<p>然后同时遍历每个字母的第一个字符，第二个字符…直到遇到不相等的就直接返回。</p>
<h3 id="15-三数之和"><a href="#15-三数之和" class="headerlink" title="15. 三数之和"></a>15. 三数之和</h3><p>给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。</p>
<p>注意：答案中不可以包含重复的三元组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">例如, 给定数组 nums = [-1, 0, 1, 2, -1, -4]，</span><br><span class="line"></span><br><span class="line">满足要求的三元组集合为：</span><br><span class="line">[</span><br><span class="line">  [-1, 0, 1],</span><br><span class="line">  [-1, -1, 2]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p><strong>解题方案：</strong></p>
<ul>
<li>首先对数组进行排序， 排序后固定一个数nums[i]，再使用左右指针指向nums[i]后面的两端，数字分别为nums[L]和nums[R]， 计算三个数的和sum，判断是否满足为0，满足则添加进结果集</li>
<li>如果nums[i] 大于0，则三数之和必然无法等于0，结束循环</li>
<li>如果nums[i] == nums[i - 1]，则说明该数字重复，会导致结果重复，所以应该跳过</li>
<li>当sum == 0时，nums[L] == nums[L + 1]则会导致结果重复，应该跳过,L++</li>
<li>当sum == 0时，nums[R] == nums[R - 1]时会导致结果重复，应该跳过，R–</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; threeSum(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        sort(nums.begin(), nums.end());</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; ans;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; nums.size() &amp;&amp; nums[i] &lt;= <span class="number">0</span>;i++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(i == <span class="number">0</span> || nums[i] &gt; nums[i - <span class="number">1</span>]) &#123;</span><br><span class="line">                <span class="keyword">int</span> l = i + <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">int</span> r = nums.size() - <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">while</span>(l &lt; r) &#123;</span><br><span class="line">                    <span class="keyword">int</span> sum = nums[i] + nums[l] + nums[r];</span><br><span class="line">                    <span class="keyword">if</span>(sum == <span class="number">0</span>) &#123;</span><br><span class="line">                        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; temp;</span><br><span class="line">                        temp.push_back(nums[i]);</span><br><span class="line">                        temp.push_back(nums[l]);</span><br><span class="line">                        temp.push_back(nums[r]);</span><br><span class="line">                        ans.push_back(temp);</span><br><span class="line">                        l += <span class="number">1</span>;</span><br><span class="line">                        r -= <span class="number">1</span>;</span><br><span class="line">                        <span class="keyword">while</span>(l &lt; r &amp;&amp; nums[l] == nums[l - <span class="number">1</span>]) &#123;</span><br><span class="line">                            l++;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">while</span>(l &lt; r &amp;&amp; nums[r] == nums[r + <span class="number">1</span>]) &#123;</span><br><span class="line">                            r--;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">else</span> <span class="keyword">if</span>(sum &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                        l++;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        r--;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/17/LeetCode-Day3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/17/LeetCode-Day3/" itemprop="url">LeetCode-Day3</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-17T21:44:01+08:00">
                2019-08-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="11-盛最多水的容器"><a href="#11-盛最多水的容器" class="headerlink" title="11. 盛最多水的容器"></a>11. 盛最多水的容器</h3><p>给定 n 个非负整数 a1，a2，…，an，每个数代表坐标中的一个点 (i, ai) 。在坐标内画 n 条垂直线，垂直线 i 的两个端点分别为 (i, ai) 和 (i, 0)。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。</p>
<p>说明：你不能倾斜容器，且 n 的值至少为 2</p>
<p><img src="https://aliyun-lc-upload.oss-cn-hangzhou.aliyuncs.com/aliyun-lc-upload/uploads/2018/07/25/question_11.jpg" alt="img"></p>
<p><strong>示例:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [1,8,6,2,5,4,8,3,7]</span><br><span class="line">输出: 49</span><br></pre></td></tr></table></figure>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxArea</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; height)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len = height.size();</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">0</span>, right = len - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">0</span>, temp = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> minHeight;</span><br><span class="line">        <span class="keyword">while</span>(left != right) &#123;</span><br><span class="line">            minHeight = height[left] &lt; height[right] ? height[left++] : height[right--];</span><br><span class="line">            temp = minHeight * (right - left + <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span>(temp &gt; max) &#123;</span><br><span class="line">                max = temp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;  </span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>解释：</p>
<p>这里采用的是<strong>双指针法</strong></p>
<p>这种方法背后的思路在于，两线段之间形成的区域总是会受到较短的那条长度的限制。此外，两条线段距离越远，得到的面积就会越大。</p>
<p>所以，我们在由线段构成的数组中使用两个指针，一个放在开始，一个放在末尾。此外，我们会使用变量 max 来持续存储到目前为止所获得的最大面积。 在每一步中，我们会找出指针所指向的两条线段形成的区域，更新 max，并将指向较短线段的指针向较长线段那端移动一步。</p>
<p><img src="https://pic.leetcode-cn.com/Figures/11_Container_WaterSlide1.PNG" alt="img"></p>
<p>这种方法如何工作？</p>
<p>最初我们考虑由最外围两条线段构成的区域。现在，为了使面积最大化，我们需要考虑更长的两条线段之间的区域。如果我们试图将指向较长线段的指针向内侧移动，矩形区域的面积将受限于较短的线段而不会获得任何增加。但是，在同样的条件下，移动指向较短线段的指针尽管造成了矩形宽度的减小，但却可能会有助于面积的增大。因为移动较短线段的指针会得到一条相对较长的线段，这可以克服由宽度减小而引起的面积减小。</p>
<h3 id="12-整数转罗马数字"><a href="#12-整数转罗马数字" class="headerlink" title="12.整数转罗马数字"></a>12.整数转罗马数字</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">intToRoman</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> values[] = &#123;<span class="number">1000</span>, <span class="number">900</span>, <span class="number">500</span>, <span class="number">400</span>, <span class="number">100</span>, <span class="number">90</span>, <span class="number">50</span>, <span class="number">40</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">1</span>&#125;;</span><br><span class="line">        <span class="built_in">string</span> reps[] = &#123;<span class="string">"M"</span>, <span class="string">"CM"</span>, <span class="string">"D"</span>, <span class="string">"CD"</span>, <span class="string">"C"</span>, <span class="string">"XC"</span>, <span class="string">"L"</span>, <span class="string">"XL"</span>, <span class="string">"X"</span>, <span class="string">"IX"</span>, <span class="string">"V"</span>, <span class="string">"IV"</span>, <span class="string">"I"</span>&#125;;</span><br><span class="line">        <span class="built_in">string</span> res;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">13</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">while</span>(num &gt;= values[i]) &#123;</span><br><span class="line">                num -= values[i];</span><br><span class="line">                res += reps[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;        </span><br><span class="line">        <span class="keyword">return</span> res; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/17/LeetCode-Day2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/17/LeetCode-Day2/" itemprop="url">LeetCode-Day2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-17T21:43:47+08:00">
                2019-08-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>今天主要学习了int的最大范围的表示</p>
<p>$ -2^{31} &lt; int &lt; 2^{31} - 1$</p>
<p>min表示为<code>-(1 &lt;&lt; 31)</code></p>
<p>max表示为<code>(1 &lt;&lt; 31) - 1</code></p>
<p>这里需要注意的是<code>&lt;&lt;</code>移位运算符的优先级比较低，不可以写成<code>1 &lt;&lt; 31 - 1</code>，会被理解为<code>1 &lt;&lt; (31 - 1)</code>，即<code>1 &lt;&lt;30</code></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/17/HDR的简单介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/17/HDR的简单介绍/" itemprop="url">HDR的简单介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-17T08:57:27+08:00">
                2019-08-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="HDR"><a href="#HDR" class="headerlink" title="HDR"></a>HDR</h2><h3 id="HDR的简介"><a href="#HDR的简介" class="headerlink" title="HDR的简介"></a>HDR的简介</h3><p><strong>HDR——High-Dynamic Range（高动态光照渲染）</strong>，相比普通的图像，可以提供更多的动态范围和图像细节，根据不同的曝光时间的LDR(Low-Dynamic Range)图像，利用曝光时间相对应最佳细节的LDR图像来合成最终的HDR图像，能够更好的反映出真实环境中的视觉效果。</p>
<p>说白了，就是一张图片尽可能得同时显示最亮和最暗的地方，或者说是让亮的地方更亮，让暗的地方更暗（或者理解为<strong>能够大幅提高画面细节的明暗对比度</strong>）。HDR显示下的色彩更接近人眼真实的感知，所以HDR提供更逼真，更身临其境的电影、游戏和内容创作体验。</p>
<p>人眼的视觉是目前最强大的摄影器材跟显像设备，我们不断升级的摄影跟显示器设备都是为了还原人眼所见，以求达到所见即所得的效果。</p>
<h3 id="HDR的组成"><a href="#HDR的组成" class="headerlink" title="HDR的组成"></a>HDR的组成</h3><p>HDR由两部分组成：<strong>动态曝光控制</strong>和<strong>光晕效果</strong>。</p>
<h4 id="动态曝光控制"><a href="#动态曝光控制" class="headerlink" title="动态曝光控制"></a>动态曝光控制</h4><p>通常，显示器能够显示R、G、B分量在[0, 255]之间的像素值。而256个不同的亮度级别显然不能表示自然界中光线的亮度情况。举个例子，太阳的亮度是白炽灯亮度的几千倍或者被漫反射照亮的室内亮度的几万倍，这远远超出了显示器的亮度表示能力。HDR技术所要解决的问题就是在有限的亮度范围内表示出宽广的亮度范围。原理类似于照相机的曝光功能，通过算法调整光线亮度，将光线从高动态范围映射到低动态范围，从而得到令人信服的光照效果</p>
<h4 id="光晕效果"><a href="#光晕效果" class="headerlink" title="光晕效果"></a>光晕效果</h4><p>人从暗处走到光亮的地方，瞳孔由于来不及收缩，眼睛会自己眯起来，以保护视网膜上的感光细胞。HDR通过对原始图像进行调整，可以模拟这种人眼自动适应光线变化的胜利反应，产生类似于光晕的效果</p>
<p>图像经DHR处理后的理想效果是亮处足够耀眼，暗处能够分辨物体的轮廓与深度，而非原图的一团漆黑。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/16/DCNN-trick/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/16/DCNN-trick/" itemprop="url">DCNN-trick</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-16T07:36:47+08:00">
                2019-08-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="深度神经网络中的实现细节（技巧Or提示）"><a href="#深度神经网络中的实现细节（技巧Or提示）" class="headerlink" title="深度神经网络中的实现细节（技巧Or提示）"></a>深度神经网络中的实现细节（技巧Or提示）</h2><h3 id="1-数据增强"><a href="#1-数据增强" class="headerlink" title="1. 数据增强"></a>1. 数据增强</h3><p>由于深度网络需要在大量训练图像上进行训练以获得令人满意的性能，如果原始图像数据集包含有限的训练图像，则最好进行数据增强以提高性能。此外，数据扩充称为培训深层网络时必须要做的事情。</p>
<ul>
<li>有许多方法可以进行数据增强，例如流行的<strong>水平翻转</strong>，<strong>随机裁剪</strong>和<strong>颜色抖动</strong>。此外，还可以尝试许多不同处理的组合，例如同时进行旋转和放缩。此外，还可以尝试将所有色素的饱和度 和 值（HSV颜色空间中的S和V的分量）提高到0.25到4之间的power（对于补丁中的所有像素都相同），将这些值乘以0.7~1.4之间的系数，并且加上一个介于-0.1到0.1之间的值。此外，还可以在[-0.1, 0.1]之间添加一个值到图像/补丁中所有像素的H值（HSV的H分量）</li>
<li>Krizhevsky等人在训练著名的Alex-Net时提出了$Fancy PCA$。$Fancy PCA$改变了训练图像中的RGB通道的强度。实际上，可以在整个训练图像中首先对RGB像素值集执行PCA。然后，对于每一个训练图像，只添加以下量的每个RGB图像的像素（例如： $I_{xy}=[I_{xy}^R, I_{xy}^G, I_{xy}^B]^{T}] : [P_{1}, P_{2}, P_{3}] [\alpha_{1} \lambda_{1}, \alpha_{2} \lambda_{2}, \alpha_{3} \lambda_{3}]^{T}$），其中$P_{i}$ 和$\lambda_{i}$分别是RGB像素值的3 * 3协方差矩阵的第 i 个本征向量和特征值，$\alpha_{i}$是具有平均值0和标准差0.1的高斯绘制的随机变量。请注意，每个$\alpha_{i}$仅针对特定训练图像的所有像素绘制一次，直到该图像再次用于训练。也就是说，当模型再次遇到相同的训练图像时，它会随机产生另一个$\alpha_{i}$用于数据增强。在<code>ImageNet Classification with Deep Convolutional
Neural Networks</code>中，他们声称<code>Fancy PCA</code>可以近似捕获自然图像的重要特性，即物体特性对于照明强度和颜色的变化是不变的。对于分类性能，该方案在ImageNet 2012的竞争中将前1个错误率降低了1%以上。</li>
</ul>
<h3 id="2-对图像进行预处理"><a href="#2-对图像进行预处理" class="headerlink" title="2. 对图像进行预处理"></a>2. 对图像进行预处理</h3><p>现在我们已经获得了大量的训练样本（图像/作物），但是不要着急。实际上，有必要对这些图像进行预处理。介绍下列几种预处理方法：</p>
<ul>
<li><p><code>zero-center the data, and then normalize them</code>，在Python中就是以下两行代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X -= np.mean(X, axis=<span class="number">0</span>)	<span class="comment"># zero-center</span></span><br><span class="line">X /= np.std(X, axis=<span class="number">0</span>)	<span class="comment"># normalize</span></span><br></pre></td></tr></table></figure>

<p>其中，X是输入数据。该预处理的另一种形式是对每个维度进行归一化，使得沿维度的最小值和最大值分别为-1 和 1。 如果您有理由相信不同的输入要素具有不同的比例（或单位），则应用此预处理才是有意义的，但他们应该与学习算法具有大致相同的重要性。在图像的情况下，像素的相对比例已经近似相等（并且在0-255范围之内），因此不必严格地执行该附加的预处理步骤</p>
</li>
<li><p>另一种类似于第一种的预处理方法是$PCA \space\space Whitening$ 。在此过程中，数据如上述所示先居中。然后，可以计算协方差矩阵，该矩阵高速我们数据中的相关结构：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X -=np.mean(X, axis=<span class="number">0</span>)	<span class="comment"># zero-center</span></span><br><span class="line">cov = np.dot(X.T, X) / X.shape[<span class="number">0</span>]	<span class="comment"># compute the covariance matrix</span></span><br></pre></td></tr></table></figure>

<p>之后，通过将原始（但以零为中心）的数据投影到特征基础上来求解相关数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">U, S, V = np.linalg.svd(cov)	<span class="comment"># 计算数据协方差矩阵的SVD分解</span></span><br><span class="line">Xrot = np.dot(X, U)		<span class="comment"># 将数据分解</span></span><br></pre></td></tr></table></figure>

<p>最后一个转换是白化，它将特征基础中的数据取出来并将每个维度除以特征值以标准化比例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Xwhite = Xrot / np.sqrt(S + le<span class="number">-5</span>)	<span class="comment"># 除以特征值（他们是奇异值的平方根）</span></span><br></pre></td></tr></table></figure>

<p>注意，这里它增加一个<code>1e-5</code>（或一个小常数）以防止除零。这种转换的一个弱点是它可以极大地夸大数据中的噪声，因为它将所有维度（包括主要是噪声的微小方差的无关维度）拉伸到输入中的相同大小。实际上，这可以通过更强的平滑来减轻（即，将<code>1e-5</code>增加为更大的数量）</p>
<p>请注意，我们在此描述这些预处理只是为了完整性，实际上，这些变换不用于卷积神经网络。但是，将数据置零中心也非常重要，并且通常也会看到每个像素的归一化。</p>
<h3 id="3-网络的初始化"><a href="#3-网络的初始化" class="headerlink" title="3. 网络的初始化"></a>3. 网络的初始化</h3><p>现在，数据准备好了。但是，在开始训练网络之前，必须初始化其参数。</p>
<ul>
<li><p>全零初始化</p>
<p>在理想情况下，通过适当的数据归一化，我们可以合理的假设大约一半的权重是正的，而其中一半是负的。一个听起来合理的想法可能是将所有初始权重设置为零，但是，这证明是一个错误的想法。因为如果网络中的每个神经元计算相同的输出，那么他们也将在反向传播期间计算相同的梯度并经历完全相同的参数更新。换句话说，如果神经元的权重被初始化为相同，则神经元之间不存在不对称的来源</p>
</li>
<li><p>用小随机数初始化</p>
<p>因此，我们仍然希望权重非常接近零，但不是相同的零。通过这种方式，您可以将这些神经元随机变为非常接近零的小数，并将其视为对称性破坏。我们的想法是，神经元在开始时都是随机且独特的，因此他们将计算不同的更新并将自身整合为整个网络的不同部分。权重的实现可能看起来像$weight$ ~ $ 0.001 \times N(0, 1)$，$N(0, 1)$：零均值，单位标准差高斯。也可以使用从均匀分布中抽取的小叔子，但这似乎对实际中的最终性能的影响相对较小。</p>
</li>
<li><p>标准差异</p>
<p>上述建议的一个问题是来自随机初始化神经元的输出的分布具有随输入数量增长的方差。事实证明，您可以通过将其权重向量按其<em>扇入的</em>平方根（即其输入数）进行缩放，<strong>将每个神经元输出的方差归一化为1</strong>，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(n) / sqrt(n) <span class="comment"># calibrating the variances with 1/sqrt(n)</span></span><br></pre></td></tr></table></figure>

<p>其中“randn”是前面提到的Gaussian，“n”是其输入的数量。这确保了网络中的所有神经元最初具有大致相同的输出分布并且凭经验提高了收敛速度。详细的推导可以从Page。幻灯片的18到23个。请注意，在推导中，它不考虑ReLU神经元的影响</p>
<p><strong>目前的建议：</strong></p>
<p>如前所述，通过校准神经元的方差进行的先前初始化不考虑ReLU。He <em>等人</em>最近关于这一主题的论文。<a href="http://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">[4]</a>推导出一个专门针对ReLU的初始化，得出网络中神经元的方差应该如下的结论 $2.0 / n$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn（n）* sqrt（<span class="number">2.0</span> / n）<span class="comment">#current recommendation</span></span><br></pre></td></tr></table></figure>

<p>这是目前在实践中使用的建议.</p>
</li>
</ul>
<h3 id="4-培训期间的一些提示"><a href="#4-培训期间的一些提示" class="headerlink" title="4. 培训期间的一些提示"></a>4. 培训期间的一些提示</h3><p>现在，一切都准备好了。让我们开始训练深层网络！</p>
<ul>
<li><p><strong>过滤器和池大小</strong>。在训练期间，输入图像的大小优选为2的幂，例如32（例如，<em>CIFAR-10</em>），<em>64,224</em>（例如，常用的<em>ImageNet</em>），384或512等。此外，它是重要的。使用具有零填充的小滤波器（例如$3 \times 3$）和小步幅（例如1），这不仅减少了参数的数量，而且提高了整个深度网络的准确率。同时，上面提到的特殊情况，即具有$3 \times 3$步幅1的滤波器，可以保持图像/特征图的空间大小。对于池化层，常用的池化大小为$2 \times 2$。</p>
</li>
<li><p><strong>学习率</strong>。此外，正如Ilya Sutskever <a href="http://yyue.blogspot.sg/2015/01/a-brief-overview-of-deep-learning.html/" target="_blank" rel="noopener">[2]</a>在博客中所描述的，他建议按小批量大小划分渐变。因此，如果更改迷你批量大小，则不应始终更改学习率（LR）。为了获得适当的LR，使用验证集是一种有效的方法。通常，训练开始时LR的典型值为0.1。在实践中，如果您发现您在验证集上停止了进展，则将LR除以2（或5），并继续前进，这可能会让您感到惊讶。</p>
</li>
<li><p><strong>在预先训练的模型上进行微调</strong>。如今，许多最先进的深层网络由着名研究团体发布，即<em>Caffe Model Zoo</em>和<em>VGG Group</em>。由于预训练深度模型具有出色的泛化能力，您可以直接将这些预先训练过的模型应用于您自己的应用程序。为了进一步提高数据集的分类性能，一种非常简单而有效的方法是根据您自己的数据微调预先训练的模型。如下表所示，两个最重要的因素是新数据集的大小（小或大），以及它与原始数据集的相似性。可以在不同情况下使用不同的微调策略。例如，一个很好的例子是您的新数据集与用于训练预训练模型的数据非常相似。在</p>
<table>
<thead>
<tr>
<th></th>
<th>Very similar dataset</th>
<th>very different dateset</th>
</tr>
</thead>
<tbody><tr>
<td><strong>very little data</strong></td>
<td>Use linear classifier on top layer</td>
<td>You’re in trouble……Try linear classifier from different stages</td>
</tr>
<tr>
<td><strong>quite a lot of data</strong></td>
<td>Finetune a few layers</td>
<td>Finetune a large number o flayers</td>
</tr>
</tbody></table>
<p>这种情况下，如果您的数据非常少，则可以在从预训练模型的顶层提取的特征上训练线性分类器。如果您手边有大量数据，请以较小的学习率微调一些预先训练过的模型。但是，如果您自己的数据集与预训练模型中使用的数据完全不同，但具有足够的训练图像，则应对您的数据进行微调，同时以较小的学习速率来提高性能。但是，如果您的数据集不仅包含很少的数据，而且与预先训练的模型中使用的数据非常不同，那么您将遇到麻烦。由于数据有限，仅训练线性分类器似乎更好。由于数据集非常不同，因此从网络顶部训练分类器可能不是最好的，因为网络包含更多数据集特定的功能。相反，在网络中较早的某个地方训练SVM分类器对激活/特征可能会更好。</p>
</li>
</ul>
<h3 id="5-激活功能的选择"><a href="#5-激活功能的选择" class="headerlink" title="5. 激活功能的选择"></a>5. 激活功能的选择</h3><p>深度网络的关键因素之一是<em>激活功能</em>，它将<strong>非线性</strong>带入网络。这里我们将介绍一些流行的激活函数的细节和特征，并在本节后面给出建议。</p>
<p><img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/neuron.png" alt="ç¥ç»å"></p>
<p>Sigmoid</p>
<p><img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/sigmod.png" alt="SIGMOD"></p>
<p>S形非线性具有数学形式<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/4713673387403833492-130.png" alt="西格玛（X）= 1 /（1 + E ^ { -  X}）">。它需要一个实数值，并将其“压缩”到0到1之间的范围内。特别是，大的负数变为0，大的正数变为1. S形函数在历史上经常被使用，因为它具有很好的解释作为神经元的射击速率：从完全不射击（0）到假定最大频率（1）的完全饱和射击。</p>
<p>在实践中，S形非线性最近已失宠，很少使用。它有两个主要缺点：</p>
<ul>
<li><p>Sigmoids饱和并杀死渐变。乙状结肠神经元的一个非常不希望的特性是，当神经元的激活在0或1的尾部饱和时，这些区域的梯度几乎为零。回想一下，在反向传播期间，这个（局部）梯度将乘以该门的输出的梯度以用于整个目标。因此，如果局部梯度非常小，它将有效地“杀死”梯度，并且几乎没有信号将通过神经元流到其权重并递归地流向其数据。此外，在初始化乙状结肠神经元的重量以防止饱和时，必须格外小心。例如，如果初始权重太大，那么大多数神经元将变得饱和，网络几乎不会学习。</p>
</li>
<li><p>Sigmoid输出不是以零为中心的*。这是不合需要的，因为神经网络中后续处理层中的神经元（很快就会更多）将接收非零中心的数据。这对梯度下降期间的动态有影响，因为如果进入神经元的数据总是正的（例如，<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/5999452984634080335-130.png" alt="X&gt; 0">元素方式<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/2668512978356043857-130.png" alt="F =瓦特^的Tx + B">），那么<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/15232045814-130.png" alt="w ^">在反向传播期间权重的梯度将变为全部为正，或者全部为负（取决于整个表达的梯度<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/13056039271-130.png" alt="F">）。这可能会在权重的梯度更新中引入不希望的锯齿形动态。但是，请注意，一旦这些渐变在一批数据中相加，权重的最终更新可能会有可变符号，从某种程度上缓解了这个问题。因此，这是不方便的，但与上述饱和激活问题相比，其具有较不严重的后果。</p>
</li>
</ul>
<h4 id="tanh（x）"><a href="#tanh（x）" class="headerlink" title="tanh（x）"></a>tanh（x）</h4><p><img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/tanh.png" alt="æ­£å"></p>
<p>tanh非线性将实数值压缩到范围[-1,1]。像S形神经元一样，它的激活饱和，但与S形神经元不同，它的输出是零中心的。因此，在实践中，tanh非线性总是优于S形非线性。</p>
<h4 id="Rectified-Linear-Unit（整流线性单元）"><a href="#Rectified-Linear-Unit（整流线性单元）" class="headerlink" title="Rectified Linear Unit（整流线性单元）"></a>Rectified Linear Unit（整流线性单元）</h4><p>整流线性单元（ReLU）在过去几年中变得非常流行。它计算函数<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/3442307160410329206-130.png" alt="F（X）= MAX（0，x）的">，它简单地设置为零阈值</p>
<ul>
<li><p>（<em>优点</em>）与涉及昂贵操作（指数等）的sigmoid / tanh神经元相比，可以通过简单地将激活矩阵设置为零来实现ReLU。同时，ReLUs不会饱和。</p>
</li>
<li><p>（<em>优点</em>）发现与sigmoid / tanh函数相比，随机梯度下降的收敛大大加速（例如，<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">[1]中</a>的因子为6 ）。有人认为这是由于它的线性，非饱和形式。</p>
</li>
<li><p>（<em>缺点</em>）不幸的是，ReLU单位在训练期间可能很脆弱并且可能“死亡”。例如，流过ReLU神经元的大梯度可能导致权重更新，使得神经元永远不会再次激活任何数据点。如果发生这种情况，那么流经该单元的梯度将从该点开始永远为零。也就是说，ReLU单元可以在训练期间不可逆转地死亡，因为它们可以从数据流形中被淘汰。例如，如果学习率设置得太高，您可能会发现多达40％的网络可能“死”（即，永远不会在整个训练数据集中激活的神经元）。通过适当设置学习率，这不是一个问题。</p>
</li>
</ul>
<h4 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h4><p><img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/leaky.png" alt="lrelu"></p>
<p>Leaky ReLUs是解决“垂死的ReLU”问题的一次尝试。当<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/5999452984636080433-130.png" alt="x &lt;0的">泄漏的ReLU反而具有小的负斜率（0.01或左右）时，而不是函数为零。也就是说，该函数计算<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/4196616167090173439-130.png" alt="f（x）= alpha x">if <img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/5999452984636080433-130.png" alt="x &lt;0的">和<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/5484684767813338902-130.png" alt="F（X）= X">if <img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/4437941819613442466-130.png" alt="xgeq 0">，where <img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/7227645438722938594-130.png" alt="α">是一个小常量。有些人用这种形式的激活函数报告成功，但结果并不总是一致的</p>
<h4 id="Parametric-ReLU-参数化ReLU"><a href="#Parametric-ReLU-参数化ReLU" class="headerlink" title="Parametric ReLU(参数化ReLU)"></a>Parametric ReLU(参数化ReLU)</h4><p>如今，提出了更广泛的激活功能，即<strong>整流单元族</strong>。在下文中，我们将讨论ReLU的变体。</p>
<p><img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/relufamily.png" alt="relufamily"></p>
<p>ReLU，Leaky ReLU，PReLU和RReLU。在这些图中，对于PReLU，<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/2031553203773749404-130.png" alt="alpha_i">是学习而Leaky ReLU <img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/2031553203773749404-130.png" alt="alpha_i">是固定的。对于RReLU，<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/8090385205122200253-130.png" alt="alpha_ {}纪">随机变量是在给定范围内保持采样，并在测试中保持固定。</p>
<p>第一种变体称为<em>参数整流线性单元</em>（<em>PReLU</em>）<a href="http://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">[4]</a>。在PReLU中，负面部分的斜率是从数据而不是预定义中学习的。他<em>等人</em>。<a href="http://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">[4]</a>声称PReLU是超越<a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a>分类任务的人类表现的关键因素。PReLU的反向传播和更新过程非常简单，类似于传统的ReLU</p>
<h4 id="随机ReLU"><a href="#随机ReLU" class="headerlink" title="随机ReLU"></a>随机ReLU</h4><p>第二种变体称为<em>随机整流线性单元</em>（<em>RReLU</em>）。在RReLU中，负部分的斜率在训练中的给定范围内随机化，然后在测试中固定。如<a href="http://arxiv.org/abs/1505.00853" target="_blank" rel="noopener">[5]中所述</a>，在最近的Kaggle <a href="https://www.kaggle.com/c/datasciencebowl" target="_blank" rel="noopener">国家数据科学碗（NDSB）</a>竞赛中，据报道RReLU由于其随机性质可以减少过度拟合。此外，由NDSB比赛获胜者建议<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/2031553203773749404-130.png" alt="alpha_i">，训练中的随机抽样从<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/348464217043394429-130.png" alt="1 / U（3,8）">测试时间开始，并且在测试时间内被固定为其期望，即<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/6504549066650438132-130.png" alt="2 /（1 + U）= 2/11">。</p>
<p>在<a href="http://arxiv.org/abs/1505.00853" target="_blank" rel="noopener">[5]中</a>，作者评估了<em>CIFAR-10</em>，<em>CIFAR-100</em>和<em>NDSB</em>数据集上具有不同激活函数的两种最先进CNN架构的分类性能，如下表所示。<em>请注意，对于这两个网络，激活功能后面是每个卷积层。而<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/12416037344-130.png" alt="一个">这些表中的实际表明<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/6763807723309486172-130.png" alt="1 /α-">，<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/7227645438722938594-130.png" alt="α">上述斜率在哪里</em>。</p>
<p><img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/relures.png" alt="relures"></p>
<p>从这些表中，我们可以发现ReLU的性能并不是所有三个数据集的最佳性能。对于Leaky ReLU，更大的斜率<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/7227645438722938594-130.png" alt="α">将获得更好的准确率。PReLU很容易在小数据集上过度拟合（其训练误差最小，而测试误差不尽如人意），但仍然优于ReLU。此外，RReLU明显优于NDSB上的其他激活功能，这表明RReLU可以克服过度拟合，因为该数据集的训练数据少于CIFAR-10 / CIFAR-100。<strong>总之，在这三个数据集中，三种类型的ReLU变体都始终优于原始ReLU。而PReLU和RReLU似乎是更好的选择。此外，何等人。[4]**</strong>也报道了类似的结论**。</p>
<h3 id="6-多样化的正规化"><a href="#6-多样化的正规化" class="headerlink" title="6. 多样化的正规化"></a>6. 多样化的正规化</h3><p>有几种方法可以控制神经网络的容量以防止过度拟合：</p>
<ul>
<li><strong>L2 regularization（L2正则化）</strong>可能是最常见的正则化形式。它可以通过直接在目标中惩罚所有参数的平方幅度来实现。也就是说，对于<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/15232045814-130.png" alt="w ^">网络中的每个权重，我们将该项添加<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/4439456383146610173-130.png" alt="frac {1} {2} lambda w ^ 2">到目标中，其中<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/9213219682577124982-130.png" alt="拉姆达">是正则化强度。通常会看到<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/7924204476109182584-130.png" alt="压裂{1} {2}">前面的因素，因为这个术语相对于参数的梯度<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/15232045814-130.png" alt="w ^">只是简单<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/2768874965512882837-130.png" alt="拉姆达威">而不是<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/2017924880831122006-130.png" alt="2 lambda w">。L2正则化具有严重惩罚峰值权重向量并且优选漫反射权向量的直观解释。</li>
<li><strong>L1 regularizationL1（正则化）</strong>是另一种相对常见的正则化形式，其中对于每个权重，<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/15232045814-130.png" alt="w ^">我们将该术语添加<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/1540651743016124243-130.png" alt="lambda | w |">到目标中。可以将L1正则化与L2正则化相结合:( <img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/5909288400939027289-130.png" alt="lambda_1 | w | + lambda_2 w ^ 2">这称为[弹性网正则化](<a href="http://web.stanford.edu/~hastie/Papers/B67.2" target="_blank" rel="noopener">http://web.stanford.edu/~hastie/Papers/B67.2</a> (2005) 301-320 Zou &amp; Hastie.pdf)）。L1正则化具有引人注目的特性，即它导致权重向量在优化期间变得稀疏（即非常接近于零）。换句话说，具有L1正则化的神经元最终仅使用其最重要输入的稀疏子集并且变得几乎不变于“噪声”输入。相比之下，来自L2正则化的最终权重向量通常是漫反射的，小数目。实际上，如果您不关心明确的特征选择，可以期望L2正则化比L1具有更好的性能。</li>
<li><strong>Max norm constraints（最大范数约束）</strong>。另一种形式的正则化是对每个神经元的权重向量的大小强制执行绝对上限，并使用预计的梯度下降来强制执行约束。实际上，这对应于正常执行参数更新，然后通过钳制<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/6235765355760146286-130.png" alt="VEC【W】">每个神经元的权重向量来实现约束以满足<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/5858718376305099-130.png" alt="并行vec {w} parallel_2 &lt;c">。典型值为<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/12672038114-130.png" alt="C">3或4的订单。有些人报告使用这种正规化形式时的改进。其吸引人的特性之一是，即使学习率设置得太高，网络也无法“爆炸”，因为更新总是有限的。</li>
<li><strong>Dropout</strong>是Srivastava <em>等人</em>非常有效，简单且最近引入的正则化技术。在<a href="http://jmlr.org/papers/v15/srivastava14a.html" target="_blank" rel="noopener">[6]</a>中补充了其他方法（L1，L2，maxnorm）。在训练期间，丢失可以解释为在完整神经网络内对神经网络进行采样，并且仅基于输入数据更新采样网络的参数。（然而，可能的采样网络的指数数量并不是独立的，因为它们共享参数。）在测试期间，没有应用丢失，并且解释了评估所有子网络的指数级整体的平均预测（更多关于合奏在下一节）。在实践中，辍学率的价值<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/7993929496961263-130.png" alt="P = 0.5"> 是一个合理的默认值，但可以根据验证数据进行调整。</li>
</ul>
<p><img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/dropout.png" alt="éåº"></p>
<p>最常用的正则化技术<em>dropout</em> <a href="http://jmlr.org/papers/v15/srivastava14a.html" target="_blank" rel="noopener">[6]</a>。在训练时，通过仅以一定概率保持神经元活动<img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/eqs/14336043121-130.png" alt="p">（超参数）或者将其设置为零来实现丢失。</p>
<h3 id="7-从数字中找到的一些见解"><a href="#7-从数字中找到的一些见解" class="headerlink" title="7. 从数字中找到的一些见解"></a>7. 从数字中找到的一些见解</h3><p>最后，根据上面的提示，您可以为自己的深层网络获得满意的设置（例如，数据处理，架构选择和详细信息等）。在培训期间，您可以绘制一些数字来表明您的网络的培训效果。</p>
<ul>
<li>众所周知，学习率非常敏感。从下面的图1可以看出，非常高的学习率将导致一个非常奇怪的损失曲线。即使在大量时期之后，低学习率也会使您的训练损失减慢得非常缓慢。相反，高学习率会使训练损失在开始时快速下降，但也会降低到局部最小值。因此，在这种情况下，您的网络可能无法取得令人满意的结果。为了获得良好的学习率，如图1所示的红线，其损耗曲线表现平稳，最终达到最佳性能。</li>
<li>现在让我们放大损失曲线。时期表示在训练数据上训练一次的次数，因此每个时期有多个小批量。如果我们在每个训练批次中绘制分类损失，则曲线如图2所示。与图1类似，如果损失曲线的趋势看起来过于线性，则表明您的学习率较低; 如果它没有减少太多，它会告诉你学习率可能太高了。而且，曲线的“宽度”与批量大小有关。如果“宽度”看起来太宽，也就是说每个批次之间的差异太大，这表明您应该增加批量大小。</li>
<li>另一个提示来自准确度曲线。如图3所示，红线是训练精度，绿线是验证线。当验证准确度收敛时，红线和绿线之间的差距将显示深层网络的有效性。如果差距很大，则表明您的网络可以在训练数据上获得良好的准确性，而在验证集上只能达到较低的准确度。很明显，你的深层模型会过度训练。因此，您应该增加深度网络的正则化强度。然而，同时在低精度水平上没有差距并不是一件好事，这表明你的深层模型具有较低的可学习性。在这种情况下，最好增加模型容量以获得更好的结果。</li>
</ul>
<p><img src="http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/trainfigs.png" alt="trainfigs"></p>
<h3 id="8-集合多个深度网络的办法"><a href="#8-集合多个深度网络的办法" class="headerlink" title="8. 集合多个深度网络的办法"></a>8. 集合多个深度网络的办法</h3><p>在机器学习中，训练多个学习者然后将它们组合起来使用的集合方法<a href="https://www.crcpress.com/Ensemble-Methods-Foundations-and-Algorithms/Zhou/9781439830031" target="_blank" rel="noopener">[8]</a>是一种最先进的学习方法。众所周知，集合通常比单个学习者更准确，并且集合方法已经在许多现实世界的任务中取得了巨大的成功。在实际应用中，特别是挑战或比赛中，几乎所有的第一名和第二名的获胜者都使用了整体方法。</p>
<p>在这里，我们介绍了深度学习场景中的集合的几种技能。</p>
<ul>
<li><strong>相同型号，不同的初始化</strong>。使用交叉验证来确定最佳超参数，然后使用最佳超参数集训练多个模型，但使用不同的随机初始化。这种方法的危险在于变化只是由于初始化。</li>
<li><strong>在交叉验证期间发现的顶级模型</strong>。使用交叉验证来确定最佳超参数，然后选择前几个（例如10个）模型来形成整体。这改善了整体的多样性，但存在包括次优模型的危险。在实践中，这可以更容易执行，因为在交叉验证之后不需要额外的模型再训练。实际上，您可以直接从<a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_blank" rel="noopener">Caffe Model Zoo中</a>选择几个最先进的深度模型来执行整体。</li>
<li><strong>单个模型的不同检查点</strong>。如果培训非常昂贵，那么有些人在一段时间内（例如在每个时期之后）采用单个网络的不同检查点并使用这些检查点形成整体的成功有限。显然，这种情况有些缺乏，但在实践中仍然可以很好地运作。这种方法的优点是非常便宜。</li>
<li><strong>一些实际的例子</strong>。如果您的视觉任务与高级图像语义相关，例如，来自静止图像的事件识别，则更好的集合方法是使用在不同数据源上训练的多个深度模型来提取不同的和互补的深度表示。例如，在与<a href="http://pamitc.org/iccv15/" target="_blank" rel="noopener">ICCV’15</a>相关的<a href="https://www.codalab.org/competitions/4081#learn_the_details" target="_blank" rel="noopener">文化事件识别</a>挑战中，我们使用了五种不同的深度模型，这些模型训练了<a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a>，<a href="http://places.csail.mit.edu/" target="_blank" rel="noopener">地方数据库</a>和<a href="http://gesture.chalearn.org/" target="_blank" rel="noopener">竞赛组织者</a>提供的文化图像。之后，我们提取了五个互补的深层特征，并将它们视为多视图数据。结合<a href="http://lamda.nju.edu.cn/weixs/publication/iccvw15_CER.pdf" target="_blank" rel="noopener">[7]中</a>描述的“早期融合”和“晚期融合”策略，我们取得了最佳表现之一，并在该挑战中排名第二。与我们的工作类似，<a href="http://cs231n.stanford.edu/reports/milad_final_report.pdf" target="_blank" rel="noopener">[9]</a>提出了<em>Stacked NN</em>框架，以同时融合更深层次的网络。</li>
</ul>
<h4 id="杂："><a href="#杂：" class="headerlink" title="杂："></a>杂：</h4><p>在实际应用中，数据通常是<strong>类不平衡的</strong>：一些类具有大量图像/训练实例，而一些类具有非常有限数量的图像。正如最近的技术报告<a href="http://www.diva-portal.org/smash/get/diva2:811111/FULLTEXT01.pdf" target="_blank" rel="noopener">[10]中</a>所讨论的，当深度CNN在这些不平衡训练集上进行训练时，结果表明不平衡训练数据可能对深度网络中的整体性能产生严重的负面影响。对于这个问题，最简单的方法是通过直接对不平衡数据进行上采样和下采样来平衡训练数据，如<a href="http://www.diva-portal.org/smash/get/diva2:811111/FULLTEXT01.pdf" target="_blank" rel="noopener">[10]</a>所示。另一个有趣的解决方案是我们的挑战解决方案中的一种特殊作物加工<a href="http://lamda.nju.edu.cn/weixs/publication/iccvw15_CER.pdf" target="_blank" rel="noopener">[7]</a>。由于原始文化事件图像不平衡，我们只从具有少量训练图像的类中提取作物，一方面可以提供不同的数据源，另一方面可以解决类不平衡问题。此外，您可以调整微调策略以克服阶级不平衡。例如，您可以将自己的数据集分为两部分：一部分包含具有大量训练样本（图像/作物）的类; 另一个包含有限数量的样本类。在每个部分，阶级不平衡的问题都不会很严重。在对数据集进行微调开始时，首先要对具有大量训练样本（图像/作物）的类进行微调，其次，继续微调，但是对有限数量的样本进行微调</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/15/常见的loss函数-TV-Loss/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/15/常见的loss函数-TV-Loss/" itemprop="url">常见的loss函数-TV_Loss</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-15T10:56:54+08:00">
                2019-08-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TV-Loss"><a href="#TV-Loss" class="headerlink" title="TV Loss"></a>TV Loss</h1><p>在图像复原过程中，图像上的一点点噪声可能就会对复原的结果产生非常大的影响，因为很过复原算法都会放大噪声。这时候我们就需要在最优化问题的模型中添加一些正则项来保持图像的光滑性，TV Loss是常用的一种正则项（<strong>注意是正则项，配合其他loss、</strong></p>
<p><strong>一起使用，约束噪声</strong>）。图片中相邻像素值的差异可以通过降低TV Loss来一定程度上解决。比如降噪，对抗checkboard等等。</p>
<h3 id="1-初始定义"><a href="#1-初始定义" class="headerlink" title="1.初始定义"></a>1.初始定义</h3><p>Rudin等人观察到，受噪声污染的图像的总变分比无噪图像的总变分明显的大。那么最小化TV理论上就可以最小化噪声。图片中相邻像素值的差异可以通过降低TV loss来一定程度上解决。</p>
<p>总编分定义为梯度幅值的积分：</p>
<p>$J_{T_{0}}(u)=\int_{\Omega_{u}}\left|\nabla_{u}\right| d x d y=\int_{D_{u}} \sqrt{u_{x}^{2}+u_{y}^{2}} d x d y$</p>
<p>其中$u_{x}=\frac{\partial u}{\partial x}$， $u_{y}=\frac{\partial u}{\partial y}$，$D_{u}$是图像的支持域。限制总变分就会限制噪声</p>
<h3 id="2-扩展定义"><a href="#2-扩展定义" class="headerlink" title="2.扩展定义"></a>2.扩展定义</h3><p>带阶数的TV loss定义如下：</p>
<p>$\Re_{V^{\beta}}(f)=\int_{\Omega}\left(\frac{\partial f}{\partial u}(u, v)^{2}+\frac{\partial f}{\partial v}(u, v)^{2}\right)^{\frac{\beta}{2}}$</p>
<p>但是在图像中，连续域的积分就变成了像素离散域中求和，所以可以这么算：<br>$\Re_{V^{\beta}}(x)=\sum_{i, j}\left(\left(x_{i, j-1}-x_{i, j}\right)^{2}+\left(x_{i+1, j}-x_{i, j}\right)^{2}\right)^{\frac{\beta}{2}}$</p>
<p>即：求每一个像素和横向下一个像素的差的平方，加上纵向下一个像素的差的平方。然后开β/2次根</p>
<h3 id="3-效果"><a href="#3-效果" class="headerlink" title="3.效果"></a>3.效果</h3><p>总变差（TV）损失促进了生成的图像中的空间平滑性，根据论文的描述，当β &lt; 1时，会出现下图左侧的小点点的artifact, 当β &gt; 1时，图像中的小点点会被消除，但是代价就是图像的清晰度</p>
<p><img src="https://img-blog.csdnimg.cn/20190311152735551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lleGlhb2d1MTEwNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/14/Pytorch-transforms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/14/Pytorch-transforms/" itemprop="url">Pytorch transforms</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-14T14:59:19+08:00">
                2019-08-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Transfroms"><a href="#Transfroms" class="headerlink" title="Transfroms"></a>Transfroms</h2><h3 id="1-裁剪——Crop"><a href="#1-裁剪——Crop" class="headerlink" title="1. 裁剪——Crop"></a>1. 裁剪——Crop</h3><h4 id="1-1-随机裁剪：transforms-RandomCrop"><a href="#1-1-随机裁剪：transforms-RandomCrop" class="headerlink" title="1.1 随机裁剪：transforms.RandomCrop()"></a>1.1 随机裁剪：transforms.RandomCrop()</h4><ul>
<li><p><code>torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=&#39;constant&#39;)</code></p>
<ul>
<li><p>功能：依据给定的size随机裁剪</p>
</li>
<li><p>参数：</p>
<ul>
<li><p><code>size-(sequence or int),若为sequence， 则为(h, w),若为int， 则(size, size)</code></p>
</li>
<li><p><code>padding-(sequence or int, optional)</code>,此参数是设置为多少个pixel，当为int时，图像上下左右均填充int个，例如<code>padding=4</code>，则上下左右均填充4个padding， 若为32 * 32，则会变成40 * 40；当为sequence时，若有两个数，则第一个数表示左右扩充多少，第二个数表示上下的，当有4个数是，则为左、上、右、下</p>
</li>
<li><p><code>fill-(int or tuple)</code>，填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值</p>
</li>
<li><p><code>padding-mode</code>,填充模式（constant（常量）， edge（按照图片边缘的像素值来填充），reflect， symmetric）</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="1-2-中心裁剪：transforms-CenterCrop"><a href="#1-2-中心裁剪：transforms-CenterCrop" class="headerlink" title="1.2 中心裁剪：transforms.CenterCrop()"></a>1.2 中心裁剪：transforms.CenterCrop()</h4><h4 id="1-3-随机长宽比裁剪-transforms-RandomResizedCrop"><a href="#1-3-随机长宽比裁剪-transforms-RandomResizedCrop" class="headerlink" title="1.3 随机长宽比裁剪 transforms.RandomResizedCrop()"></a>1.3 随机长宽比裁剪 transforms.RandomResizedCrop()</h4><h4 id="1-4-上下左右中心裁剪：transforms-FiveCrop"><a href="#1-4-上下左右中心裁剪：transforms-FiveCrop" class="headerlink" title="1.4 上下左右中心裁剪：transforms.FiveCrop()"></a>1.4 上下左右中心裁剪：transforms.FiveCrop()</h4><h4 id="1-5-上下左右中心裁剪后翻转：transform-TenCrop"><a href="#1-5-上下左右中心裁剪后翻转：transform-TenCrop" class="headerlink" title="1.5 上下左右中心裁剪后翻转：transform.TenCrop()"></a>1.5 上下左右中心裁剪后翻转：transform.TenCrop()</h4><h3 id="2-翻转和旋转——Flip-and-Rotation"><a href="#2-翻转和旋转——Flip-and-Rotation" class="headerlink" title="2. 翻转和旋转——Flip and Rotation"></a>2. 翻转和旋转——Flip and Rotation</h3><h4 id="2-1-依概率p水平翻转-transfroms-RandomHorizontalFlip"><a href="#2-1-依概率p水平翻转-transfroms-RandomHorizontalFlip" class="headerlink" title="2.1 依概率p水平翻转 transfroms. RandomHorizontalFlip()"></a>2.1 依概率p水平翻转 transfroms. RandomHorizontalFlip()</h4><h4 id="2-2-依概率p垂直翻转-transforms-RandomVerticalFlip"><a href="#2-2-依概率p垂直翻转-transforms-RandomVerticalFlip" class="headerlink" title="2.2 依概率p垂直翻转 transforms.RandomVerticalFlip()"></a>2.2 依概率p垂直翻转 transforms.RandomVerticalFlip()</h4><h4 id="2-3-随机旋转-transforms-RandomRotation"><a href="#2-3-随机旋转-transforms-RandomRotation" class="headerlink" title="2.3 随机旋转 transforms.RandomRotation()"></a>2.3 随机旋转 transforms.RandomRotation()</h4><h3 id="3-图像变换"><a href="#3-图像变换" class="headerlink" title="3. 图像变换"></a>3. 图像变换</h3><h4 id="3-1-resize-transform-Resize"><a href="#3-1-resize-transform-Resize" class="headerlink" title="3.1 resize  transform.Resize"></a>3.1 resize  transform.Resize</h4><h4 id="3-2-标准化-transform-Normalize"><a href="#3-2-标准化-transform-Normalize" class="headerlink" title="3.2 标准化 transform.Normalize"></a>3.2 标准化 transform.Normalize</h4><h4 id="3-3-转为tensor-transforms-ToTensor"><a href="#3-3-转为tensor-transforms-ToTensor" class="headerlink" title="3.3 转为tensor transforms.ToTensor"></a>3.3 转为tensor transforms.ToTensor</h4><h4 id="3-4-填充-transforms-Pad"><a href="#3-4-填充-transforms-Pad" class="headerlink" title="3.4 填充 transforms.Pad"></a>3.4 填充 transforms.Pad</h4><h4 id="3-5-修改亮度、对比度和饱和度-transforms-ColorJitter"><a href="#3-5-修改亮度、对比度和饱和度-transforms-ColorJitter" class="headerlink" title="3.5 修改亮度、对比度和饱和度 transforms.ColorJitter()"></a>3.5 修改亮度、对比度和饱和度 transforms.ColorJitter()</h4><h4 id="3-6-转灰度图-transforms-GrayScale"><a href="#3-6-转灰度图-transforms-GrayScale" class="headerlink" title="3.6 转灰度图 transforms.GrayScale"></a>3.6 转灰度图 transforms.GrayScale</h4><h4 id="3-7-线性变换-transforms-LinearTransformation"><a href="#3-7-线性变换-transforms-LinearTransformation" class="headerlink" title="3.7 线性变换 transforms.LinearTransformation()"></a>3.7 线性变换 transforms.LinearTransformation()</h4><h4 id="3-8-放射变换-transform-RandomAffine"><a href="#3-8-放射变换-transform-RandomAffine" class="headerlink" title="3.8 放射变换 transform.RandomAffine"></a>3.8 放射变换 transform.RandomAffine</h4><h4 id="3-9-依概率p转为灰度图-transforms-RandomGrayScale"><a href="#3-9-依概率p转为灰度图-transforms-RandomGrayScale" class="headerlink" title="3.9 依概率p转为灰度图 transforms.RandomGrayScale"></a>3.9 依概率p转为灰度图 transforms.RandomGrayScale</h4><h4 id="3-10-将数据转换为PILImage-transforms-ToPILImage"><a href="#3-10-将数据转换为PILImage-transforms-ToPILImage" class="headerlink" title="3.10 将数据转换为PILImage transforms.ToPILImage"></a>3.10 将数据转换为PILImage transforms.ToPILImage</h4><h4 id="3-11-transforms-Lambda"><a href="#3-11-transforms-Lambda" class="headerlink" title="3.11 transforms.Lambda"></a>3.11 transforms.Lambda</h4><h3 id="4-对transforms操作，使数据增强更灵活"><a href="#4-对transforms操作，使数据增强更灵活" class="headerlink" title="4. 对transforms操作，使数据增强更灵活"></a>4. 对transforms操作，使数据增强更灵活</h3><h4 id="4-1-transforms-RandomChoice-transfroms"><a href="#4-1-transforms-RandomChoice-transfroms" class="headerlink" title="4.1 transforms.RandomChoice(transfroms)"></a>4.1 transforms.RandomChoice(transfroms)</h4><h4 id="4-2-transforms-RandomApply-transforms-p-0-5"><a href="#4-2-transforms-RandomApply-transforms-p-0-5" class="headerlink" title="4.2 transforms.RandomApply(transforms, p=0.5)"></a>4.2 transforms.RandomApply(transforms, p=0.5)</h4><h4 id="4-3-transforms-RandomOrder"><a href="#4-3-transforms-RandomOrder" class="headerlink" title="4.3 transforms.RandomOrder"></a>4.3 transforms.RandomOrder</h4><h4 id><a href="#" class="headerlink" title=" "></a> </h4>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/14/Something-about-PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/14/Something-about-PyTorch/" itemprop="url">Something about PyTorch</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-14T14:50:32+08:00">
                2019-08-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Something-about-PyTorch"><a href="#Something-about-PyTorch" class="headerlink" title="Something about PyTorch"></a>Something about PyTorch</h2><ul>
<li><p><code>torchvision.transfroms.Compose(trandforms)</code></p>
<p>将多个transform组合起来使用</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transforms.Compose([</span><br><span class="line">	transfroms.CenterCrop(<span class="number">10</span>), </span><br><span class="line">	transforms.ToTensor(),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>OrderedDict</code></p>
<p>是dict的子类，其最大特征是，它可以“维护”添加key-value对的额顺序。简单的来说，就是先添加的key-value对排在前面，后添加的key-value对排在后面</p>
<p>由于OrderedDict能维护key-value对的添加顺序，因此即使两个OrderedDict照中的key-value对完全相同，但只要他们的顺序不同，程序在判断他们是否相等时也依然会返回false。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">dx = OrderedDict(b=<span class="number">5</span>, c=<span class="number">2</span>, a=<span class="number">7</span>)</span><br><span class="line">print(dx)</span><br><span class="line">d = OrderedDict()</span><br><span class="line">d[<span class="string">'Python'</span>] = <span class="number">89</span></span><br><span class="line">d[<span class="string">'Swift'</span>] = <span class="number">92</span></span><br><span class="line">d[<span class="string">'Kotlin'</span>] = <span class="number">97</span></span><br><span class="line">d[<span class="string">'Go'</span>] = <span class="number">87</span></span><br><span class="line">print(d)</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> d.items():</span><br><span class="line">    print(k, v)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">OrderedDict([('b', 5), ('c', 2), ('a', 7)])</span></span><br><span class="line"><span class="string">OrderedDict([('Python', 89), ('Swift', 92), ('Kotlin', 97), ('Go', 87)])</span></span><br><span class="line"><span class="string">Python 89</span></span><br><span class="line"><span class="string">Swift 92</span></span><br><span class="line"><span class="string">Kotlin 97</span></span><br><span class="line"><span class="string">Go 87</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>nn.LeakyReLU(inplace=True)</code></p>
<p><code>inplace=True</code>的意思是进行原地操作，例如<code>x=x+5</code>对于x就是一个原地操作，<code>y=x+5; x=y</code>完成了与<code>x=x+5</code>同样的功能但不是原地操作，与上面的<code>inplace=True</code>的含义是一样的，是对于Conv2d这样的上层网络传递下来的tensor直接进行修改，好处就是可以节省运算内存</p>
</li>
</ul>
<ul>
<li><p><code>batch normalization层的实现机理：</code></p>
<p>假设我们在网络中间经过某些卷积操作之后输出的feature map的尺寸为4 * 3 * 2 * 2</p>
<p>4为batch的大小， 3为channel的数目，2 * 2为feature map的长宽</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1565911499478.png" alt="1565911499478"></p>
<p>所以对于一个batch Normalization层而言，去求均值与方差是对于所有batch中的同一个channel进行求取，batch normalization中的batch体现在这个地方</p>
<p>batch normalization层能够学习到的参数，对于一个特定的channel而言实际上是两个参数，gamma与beta，对于total的channel而言实际上是channel数目的两倍</p>
</li>
<li><p><code>meshgrid()</code></p>
<p>引入创建网格点的矩阵</p>
<p>示例一：创建一个2行3列的网格点矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1</span>]])</span><br><span class="line">print(<span class="string">"X的维度: &#123;&#125;, shape: &#123;&#125;"</span>.format(X.ndim, X.shape))</span><br><span class="line">Y = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">print(<span class="string">"Y的维度: &#123;&#125;, shape: &#123;&#125;"</span>.format(Y.ndim, Y.shape))</span><br><span class="line"></span><br><span class="line">plt.plot(X, Y, <span class="string">'o--'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1566006916935.png" alt="1566006916935"></p>
</li>
</ul>
<pre><code>当要描绘的 矩阵网格点的数据量小的时候，可以用上述方法构造网格点坐标数据;</code></pre><p>但是如果是一个(256, 100)的整数矩阵网格,要怎样构造数据呢?<br>方法1:将x轴上的100个整数点组成的行向量，重复256次，构成shape(256,100)的X矩阵;将y轴上的256个整数点组成列向量,重复100次构成shape(256,100)的Y矩阵<br>显然方法1的数据构造过程很繁琐,也不方便调用,那么有没有更好的办法呢?of course!!!<br>那么meshgrid()就显示出它的作用了<br>使用meshgrid方法，你只需要构造一个表示x轴上的坐标的向量和一个表示y轴上的坐标的向量;然后作为参数给到meshgrid(),该函数就会返回相应维度的两个矩阵;<br>例如,你想构造一个2行3列的矩阵网格点,那么x生成一个shape(3,)的向量,y生成一个shape(2,)的向量,将x,y传入meshgrid(),最后返回的X,Y矩阵的shape(2,3)</p>
<p>示例二：使用<code>meshgrid()</code>生成示例一种的网格点矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1</span>])</span><br><span class="line">y = np.array([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">xv, yv = np.meshgrid(x, y)</span><br><span class="line">print(<span class="string">"xv的维度: &#123;&#125;, shape: &#123;&#125;"</span>.format(xv.ndim, xv.shape))</span><br><span class="line">print(<span class="string">"yv的维度: &#123;&#125;, shape: &#123;&#125;"</span>.format(yv.ndim, yv.shape))</span><br><span class="line"></span><br><span class="line">plt.plot(xv, yv, <span class="string">'o--'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">xv的维度: 2, shape: (2, 3)</span></span><br><span class="line"><span class="string">yv的维度: 2, shape: (2, 3)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>示例三：生成20行30列的网格点矩阵</p>
<p>只需要改变:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">500</span>, <span class="number">30</span>)</span><br><span class="line">y = np.linspace(<span class="number">0</span>, <span class="number">500</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1566007285866.png" alt="1566007285866"></p>
<ul>
<li><code>repeat()</code>和<code>expand()</code>的区别</li>
</ul>
<p>torch.Tensor是包含一种数据类型元素的多维矩阵，torch.Tensor有两个实例方法可以用来扩展某维的数据的尺寸，分别是<code>repeat()</code>和<code>expand()</code></p>
<p><code>expand()</code></p>
<p>返回当前张量在某维扩展更大后的张量， 扩展(expand)张量<strong>不会分配新的内存</strong>，只是在存在的张量上创建一个新的视图(view)，一个大小(size)等于1的维度扩展到更大的尺寸。示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x = x.expand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[1, 2, 3],</span></span><br><span class="line"><span class="string">        [1, 2, 3]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">print(x)</span><br><span class="line">x = x.expand(<span class="number">-1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">-1</span>)</span><br><span class="line">print(x.size())</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[[[ 0.6288, -0.3688,  1.0322,  0.8450]]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[[-1.5189, -0.6984, -1.0931,  0.0894]]]])</span></span><br><span class="line"><span class="string">torch.Size([2, 2, 3, 4])</span></span><br><span class="line"><span class="string">tensor([[[[ 0.6288, -0.3688,  1.0322,  0.8450],</span></span><br><span class="line"><span class="string">          [ 0.6288, -0.3688,  1.0322,  0.8450],</span></span><br><span class="line"><span class="string">          [ 0.6288, -0.3688,  1.0322,  0.8450]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[ 0.6288, -0.3688,  1.0322,  0.8450],</span></span><br><span class="line"><span class="string">          [ 0.6288, -0.3688,  1.0322,  0.8450],</span></span><br><span class="line"><span class="string">          [ 0.6288, -0.3688,  1.0322,  0.8450]]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[[-1.5189, -0.6984, -1.0931,  0.0894],</span></span><br><span class="line"><span class="string">          [-1.5189, -0.6984, -1.0931,  0.0894],</span></span><br><span class="line"><span class="string">          [-1.5189, -0.6984, -1.0931,  0.0894]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">         [[-1.5189, -0.6984, -1.0931,  0.0894],</span></span><br><span class="line"><span class="string">          [-1.5189, -0.6984, -1.0931,  0.0894],</span></span><br><span class="line"><span class="string">          [-1.5189, -0.6984, -1.0931,  0.0894]]]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p><code>repeat()</code></p>
<p>沿着特定的维度重复这个张量，和expand()不同的是，这个函数<strong>拷贝</strong>张量的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x = x.repeat(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[1, 2, 3, 1, 2, 3],</span></span><br><span class="line"><span class="string">        [1, 2, 3, 1, 2, 3],</span></span><br><span class="line"><span class="string">        [1, 2, 3, 1, 2, 3]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(x)</span><br><span class="line">x = x.repeat(<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[[-0.6915, -2.3975,  1.2687, -1.1976],</span></span><br><span class="line"><span class="string">         [ 0.5424,  0.9949,  0.8677, -0.2078],</span></span><br><span class="line"><span class="string">         [-1.5410, -0.0281,  1.4717, -0.7021]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.3378, -2.0375,  0.2226,  0.8865],</span></span><br><span class="line"><span class="string">         [ 0.0319,  0.2902,  0.0647,  0.5396],</span></span><br><span class="line"><span class="string">         [-1.0962, -2.3473, -0.2049, -0.2524]]])</span></span><br><span class="line"><span class="string">tensor([[[-0.6915, -2.3975,  1.2687, -1.1976, -0.6915, -2.3975,  1.2687,</span></span><br><span class="line"><span class="string">          -1.1976, -0.6915, -2.3975,  1.2687, -1.1976],</span></span><br><span class="line"><span class="string">         [ 0.5424,  0.9949,  0.8677, -0.2078,  0.5424,  0.9949,  0.8677,</span></span><br><span class="line"><span class="string">          -0.2078,  0.5424,  0.9949,  0.8677, -0.2078],</span></span><br><span class="line"><span class="string">         [-1.5410, -0.0281,  1.4717, -0.7021, -1.5410, -0.0281,  1.4717,</span></span><br><span class="line"><span class="string">          -0.7021, -1.5410, -0.0281,  1.4717, -0.7021]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.3378, -2.0375,  0.2226,  0.8865,  1.3378, -2.0375,  0.2226,</span></span><br><span class="line"><span class="string">           0.8865,  1.3378, -2.0375,  0.2226,  0.8865],</span></span><br><span class="line"><span class="string">         [ 0.0319,  0.2902,  0.0647,  0.5396,  0.0319,  0.2902,  0.0647,</span></span><br><span class="line"><span class="string">           0.5396,  0.0319,  0.2902,  0.0647,  0.5396],</span></span><br><span class="line"><span class="string">         [-1.0962, -2.3473, -0.2049, -0.2524, -1.0962, -2.3473, -0.2049,</span></span><br><span class="line"><span class="string">          -0.2524, -1.0962, -2.3473, -0.2049, -0.2524]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[-0.6915, -2.3975,  1.2687, -1.1976, -0.6915, -2.3975,  1.2687,</span></span><br><span class="line"><span class="string">          -1.1976, -0.6915, -2.3975,  1.2687, -1.1976],</span></span><br><span class="line"><span class="string">         [ 0.5424,  0.9949,  0.8677, -0.2078,  0.5424,  0.9949,  0.8677,</span></span><br><span class="line"><span class="string">          -0.2078,  0.5424,  0.9949,  0.8677, -0.2078],</span></span><br><span class="line"><span class="string">         [-1.5410, -0.0281,  1.4717, -0.7021, -1.5410, -0.0281,  1.4717,</span></span><br><span class="line"><span class="string">          -0.7021, -1.5410, -0.0281,  1.4717, -0.7021]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[ 1.3378, -2.0375,  0.2226,  0.8865,  1.3378, -2.0375,  0.2226,</span></span><br><span class="line"><span class="string">           0.8865,  1.3378, -2.0375,  0.2226,  0.8865],</span></span><br><span class="line"><span class="string">         [ 0.0319,  0.2902,  0.0647,  0.5396,  0.0319,  0.2902,  0.0647,</span></span><br><span class="line"><span class="string">           0.5396,  0.0319,  0.2902,  0.0647,  0.5396],</span></span><br><span class="line"><span class="string">         [-1.0962, -2.3473, -0.2049, -0.2524, -1.0962, -2.3473, -0.2049,</span></span><br><span class="line"><span class="string">          -0.2524, -1.0962, -2.3473, -0.2049, -0.2524]]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>squeeze()</code>  和<code>unsqueeze()</code></li>
</ul>
<p><code>unsqueeze()函数</code></p>
<p>首先初始化一个a</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = a.view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure>

<p>可以看到a的维度是(2, 3)</p>
<p>在第二维增加一个维度，使其维度变为(2, 1, 3)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = a.unsqueeze(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]]])</span><br></pre></td></tr></table></figure>

<p>可以看到维度已经变为(2, 1, 3)了，同样如果需要在倒数第二个维度上增加一个维度，可以用<code>a.unsqueeze(-2)</code></p>
<p><code>squeeze()函数</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = a.squeeze(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure>

<p>另外，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.squeeze(<span class="number">-1</span>)</span><br><span class="line">tensor([[[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]]]])</span><br></pre></td></tr></table></figure>

<p>此时可以看到维度没有发生变化，这是因为只有维度为1时才会去掉</p>
<ul>
<li><code>permute()</code></li>
</ul>
<p>将tensor的维度换位</p>
<p><strong>参数</strong>：参数是一系列的整数，代表原来张量的维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor(a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.size()</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">tensor([[[<span class="number">1</span>, <span class="number">4</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">2</span>, <span class="number">5</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">3</span>, <span class="number">6</span>]]], dtype=torch.int32)</span><br></pre></td></tr></table></figure>

<p>再比如，图片img的size比如是(28, 28, 3)，就可以利用<code>img.permute(2, 0, 1)</code>得到一个size为(3, 28, 28)的tensor</p>
<ul>
<li><code>contiguous()</code></li>
</ul>
<p>在PyTorch中，有一些对Tensor的操作不会真正改变Tensor中的内容，改变的仅仅是Tensor中字节位置的索引。这些操作有：</p>
<p><code>narrow(), view(), expand(), transpose()</code></p>
<p>例如在执行<code>view()</code>操作之后，不会开辟新的内存空间才存放处理之后的数据，实际上新数据与原始数据共享同一块内存</p>
<p>而在调用<code>contiguous()</code>之后，PyTorch会开辟出一块新的内存空间存放变换之后的数据，并会真正改变Tensor的内容，按照变换之后的顺序存放数据。</p>
<ul>
<li><p><code>grid_sample(input, frig, mode=&#39;linear&#39;, padding_mode=&#39;zeros&#39;)</code></p>
<p>用于图像的恢复</p>
<p>input (N, C, H_in, W_in)</p>
<p>output (N, H_out, W_out, 2)</p>
<p>grid的索引值（数据坐标）对应output索引值（数据坐标）</p>
<p>grid中数值对应input的索引值（数据坐标）</p>
<p>grid(u, v) = x, y</p>
<p>output(u, v) 数值对应 input(x, y)</p>
</li>
</ul>
<p>示意图：</p>
<p><img src="http://www.pianshen.com/images/289/6741fcf9e3022aa5e1b3f57d3d1cc261.png" alt="img"></p>
<ul>
<li><code>nn.Upsample()</code></li>
</ul>
<p>所用：上采样</p>
<p>定义：<code>CLASS torch.nn.Upsample(size=None, scale_factor=None, mode=&#39;nearest&#39;, align_corners=None)</code></p>
<p>计算shape</p>
<p><img src="https://img-blog.csdnimg.cn/20190711163324219.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lhbmd3YW5nbm5kZA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/14/parser-action/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/14/parser-action/" itemprop="url">parser-action</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-14T14:33:27+08:00">
                2019-08-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h2><p>举例1：</p>
<p><code>self.parser.add_argument(--&#39;lr_use&#39;, action=&#39;store_true&#39;, default=False, help=&#39;if or not use lr_loss&#39;)</code></p>
<p>当在终端运行的时候，如果不加入<code>--lr_use</code>， 那么程序running的时候，<code>lr_use</code>的值为<code>default:False</code></p>
<p><code>self.parser.add_argument(&#39;--no_flip&#39;, action=&#39;store_false&#39;, help=&#39;....&#39;)</code></p>
<p>当在终端运行的时候，并没有加入<code>no_flip</code>，数据集中的图片并不会翻转，打印出来看到<code>no_flip</code>的值为True</p>
<p><strong>Note:</strong></p>
<p>有default值的时候，running时不声明就为默认值</p>
<p>没有的话，如果是<code>store_false</code>，则默认值是True， 如果是<code>store_true</code>，则默认值是False</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/11/CenterLossTest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/11/CenterLossTest/" itemprop="url">CenterLossTest</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-11T15:26:38+08:00">
                2019-08-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="CenterLossTest"><a href="#CenterLossTest" class="headerlink" title="CenterLossTest"></a>CenterLossTest</h2><ul>
<li><p><code>class torch.nn.PReLU(num_paramenter=1, init=0.25)</code></p>
<p>对输入的每一个元素运用函数 $PReLU(x) = max(0,x) + a*min(0, x)$，<code>a</code>是一个可学习参数。当没有声明时，<code>nn.PReLU()</code>在所有的输入中只有一个参数a；如果是<code>nn.PReLU(nChannels)</code>，a将应用到每个输入。</p>
<p><strong>注意：</strong>当为了表现更加的模型而学习参数a时不要使用权重衰减</p>
<p>参数：</p>
<ul>
<li>num_parameters：需要学习的a的个数，默认等于1</li>
<li>init：a的初始值，默认等于0.25</li>
</ul>
<p>shape：</p>
<ul>
<li>输入：$(N, )$，代表任意数目附加维度</li>
<li>输出：$(N,*)$，与输入拥有同样的shape属性</li>
</ul>
</li>
<li><p><code>nn.Linear(in_features, out_features, bias=True)</code></p>
<p>具体形式为：<code>y = wx + b</code></p>
<p><code>weight = Parameter(torch.Tensor(out_features, in_features))</code></p>
<p><code>bias = Parameter(torch.Tensor(out_features))</code></p>
<p><code>bias</code>如果设置为False，则图层不会学习附加偏差。默认值：True</p>
</li>
<li><p><code>self.v = torch.nn.Parameters()</code></p>
<p>可以把这个函数理解为类型转换函数，讲一个不可训练的类型<code>Tensor</code>转换成可以训练的类型<code>parameter</code>，并将这个<code>parameter</code>绑定到这个<code>module</code>里面(<code>net.parameter()</code>中就有这个绑定的<code>parameter</code>，所以在参数优化的时候可以进行优化的)，所以经过类型转换这个<code>self.v</code>变成了模型的一部分，成为了模型中根据训练可以改动的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化</p>
</li>
<li><p><code>torch.pow()</code></p>
<p>这里对应的矩阵乘法只是每一位上的乘法</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(a)</span><br><span class="line">b = torch.pow(a)</span><br><span class="line">print(b)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output：</span></span><br><span class="line"><span class="string">tensor([[-0.6702,  0.1811],</span></span><br><span class="line"><span class="string">        [-0.7064, -0.3418]])</span></span><br><span class="line"><span class="string">tensor([[0.4491, 0.0328],</span></span><br><span class="line"><span class="string">        [0.4990, 0.1168]])</span></span><br><span class="line"><span class="string">0.4491 = (-0.6702)*(-0.6702)        </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.sum(input, dim, out=None)</code>  <code>----&gt;Tensor</code></p>
<p>返回输入张量给定维度上每行的和。输出形状与输入相同，除了给定维度上为1.</p>
<p>参数：</p>
<ul>
<li>input(Tensor)——输入张量</li>
<li>dim(int)——缩减的维度</li>
<li>out(Tensor, optional)——结果张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(torch.sum(a, <span class="number">1</span>))</span><br><span class="line">print(torch.sum(a))</span><br><span class="line">print(torch.sum(a, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[-0.9850, -0.6207, -0.6559, -0.1220],</span></span><br><span class="line"><span class="string">        [-1.0619,  0.0158, -1.0086,  0.3370],</span></span><br><span class="line"><span class="string">        [ 0.5729, -1.7753,  1.2464, -1.6284],</span></span><br><span class="line"><span class="string">        [-0.3275, -0.5711, -0.6691,  1.2357]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([-2.3836, -1.7177, -1.5843, -0.3320])	# 变成了行向量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor(-6.0177)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[-2.3836],</span></span><br><span class="line"><span class="string">        [-1.7177],</span></span><br><span class="line"><span class="string">        [-1.5843],</span></span><br><span class="line"><span class="string">        [-0.3320]])	# 仍然保持了列向量</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>expand</code></p>
<p>扩展某个size为1的维度。如(2, 2, 1)扩展为(2, 2, 3)</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = x.expand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(y)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[[ 0.3814],</span></span><br><span class="line"><span class="string">         [ 1.4493]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[-0.0204],</span></span><br><span class="line"><span class="string">         [-0.9141]]])</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">tensor([[[ 0.3814,  0.3814,  0.3814],</span></span><br><span class="line"><span class="string">         [ 1.4493,  1.4493,  1.4493]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[-0.0204, -0.0204, -0.0204],</span></span><br><span class="line"><span class="string">         [-0.9141, -0.9141, -0.9141]]])     </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.squeeze()</code></p>
<p>将维度为1的压缩掉。如size为(3, 1, 1, 2)，压缩之后为(3, 2)</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.squeeze())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[[[ 2.2045,  0.2968,  0.2945]]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[[ 0.2579,  0.9719, -0.8220]]]])</span></span><br><span class="line"><span class="string">tensor([[ 2.2045,  0.2968,  0.2945],</span></span><br><span class="line"><span class="string">        [ 0.2579,  0.9719, -0.8220]])        </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.unsqueeze(input, dim, out=None)</code></p>
<p>返回一个新的张量，对输入的指定位置插入维度1</p>
<p><strong>注意：</strong>返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个</p>
<p>如果dim为负，则会被转换为<code>dim + input.dim() + 1</code></p>
<p>参数：</p>
<ul>
<li>tensor(Tensor)——输入张量</li>
<li>dim(int)——插入维度的索引</li>
<li>out(Tensor, optional)——结果张量</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.randn(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(a)</span><br><span class="line">b = a.unsqueeze(<span class="number">1</span>)</span><br><span class="line">print(b)</span><br><span class="line">print(b.size())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[-0.2329,  0.0805]])</span></span><br><span class="line"><span class="string">tensor([[[-0.2329,  0.0805]]])</span></span><br><span class="line"><span class="string">torch.Size([1, 1, 2])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>t()</code>     转置</p>
</li>
<li><p><code>torch.addmm(beta-1, mat, alpha=1, mat1, mat2, out=None)    ----&gt;Tensor</code></p>
<p>对矩阵mat1和mat2进行矩阵乘操作，矩阵mat加到最终结果。alpha和beta分别是两个矩阵mat1×mat2和mat的比例因子，即$out=(beta * M) + (alpha * mat1 × mat2)$</p>
<p>对类型为FloatTensor或DoubleTensor的输入，beta和alpha必须为实数，否则两个参数必须为整数</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">b = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">c = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line">print(c)</span><br><span class="line">print(a.addmm(<span class="number">1</span>, <span class="number">2</span>, b, c))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 1.2774,  0.2344],</span></span><br><span class="line"><span class="string">        [-0.2572,  0.0019]])</span></span><br><span class="line"><span class="string">tensor([[0.4277, 0.8812, 0.7919],</span></span><br><span class="line"><span class="string">        [0.5476, 0.2299, 0.9781]])</span></span><br><span class="line"><span class="string">tensor([[-1.2772, -0.9458],</span></span><br><span class="line"><span class="string">        [ 1.6094,  0.7200],</span></span><br><span class="line"><span class="string">        [ 0.0633,  0.0571]])</span></span><br><span class="line"><span class="string">tensor([[ 3.1216,  0.7847],</span></span><br><span class="line"><span class="string">        [-0.7920, -0.5911]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.clamp(input, min, max, out=None)   ---&gt;Tensor</code></p>
<p>将输入input张量每个元素都夹紧到区间[min, max]，并返回结果到一个新张量。</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.randint(low=<span class="number">0</span>, high=<span class="number">10</span>, size=(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">print(a)</span><br><span class="line">a = torch.clamp(a, <span class="number">3</span>, <span class="number">7</span>)</span><br><span class="line">print(a)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[1, 1, 4],</span></span><br><span class="line"><span class="string">        [8, 9, 2]])</span></span><br><span class="line"><span class="string">tensor([[3, 3, 4],</span></span><br><span class="line"><span class="string">        [7, 7, 3]])        </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>$y_{i}=\left{\begin{array}{l}{ min, x_{i} &lt; min} \ {x_{i}, min \leq x_{i} \leq max} \ {max, x_{i} &gt; max}\end{array}\right.$</p>
</li>
<li><p><code>torch.eq()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">outputs = torch.FloatTensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">targets = torch.FloatTensor([[<span class="number">0</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">print(targets.eq(outputs.data))	<span class="comment"># 比较相等</span></span><br><span class="line">print(targets.eq(outputs.data).cpu().sum())	<span class="comment"># 统计相等的个数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[False],</span></span><br><span class="line"><span class="string">        [ True],</span></span><br><span class="line"><span class="string">        [ True]])</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">tensor(2)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Mariana</p>
              <p class="site-description motion-element" itemprop="description">a study blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">77</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mariana</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
