<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="a study blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Celery Fairy">
<meta property="og:url" content="https://woojoo520.github.io/page/5/index.html">
<meta property="og:site_name" content="Celery Fairy">
<meta property="og:description" content="a study blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Celery Fairy">
<meta name="twitter:description" content="a study blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://woojoo520.github.io/page/5/">





  <title>Celery Fairy</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <a href="https://github.com/woojoo520" class="github-corner" aria-label="View source on GitHub">
      <svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/>
      </svg>
    </a>
    <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Celery Fairy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Celery's Blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/卷积/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/卷积/" itemprop="url">卷积在图像处理中的应用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T15:09:44+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>卷积的用途：</p>
<p>将图像相邻子区域的像素值与卷积核执行“卷积”操作，可以获取相邻数据之间的统计关系，从而可挖掘图像中的某些重要特征。</p>
<p>比较抽象…所以特征到底是什么？用图像来形象的说明一下</p>
<p><img src="https://pic4.zhimg.com/v2-b0f198aa46872eb91cb7f1f593073f13_b.jpg" alt="img"></p>
<p>下面我们简单介绍一下常用的“久经考验”的卷积核。<br>（1）同一化核（Identity）。从图13-6可见，这个滤波器什么也没有做，卷积后得到的图像和原图一样。因为这个核只有中心点的值是1。邻域点的权值都是0，所以对滤波后的取值没有任何影响。<br>（2）边缘检测核（Edge Detection），也称为高斯-拉普拉斯算子。需要注意的是，这个核矩阵的元素总和为0（即中间元素为8，而周围8个元素之和为-8），所以滤波后的图像会很暗，而只有边缘位置是有亮度的。<br>（3）图像锐化核（Sharpness Filter）。图像的锐化和边缘检测比较相似。首先找到边缘，然后再把边缘加到原来的图像上面，如此一来，就强化了图像的边缘，使得图像看起来更加锐利。<br>（4）均值模糊（Box Blur /Averaging）。这个核矩阵的每个元素值都是1，它将当前像素和它的四邻域的像素一起取平均，然后再除以9。均值模糊比较简单，但图像处理得不够平滑。因此，还可以采用高斯模糊核（Gaussian Blur），这个核被广泛用在图像降噪上。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/center-loss/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/center-loss/" itemprop="url">center loss</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T14:31:07+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Center-Loss"><a href="#Center-Loss" class="headerlink" title="Center Loss"></a>Center Loss</h2><h3 id="一-简介"><a href="#一-简介" class="headerlink" title="一. 简介"></a>一. 简介</h3><p>论文链接：<a href="http://ydwen.github.io/papers/WenECCV16.pdf" target="_blank" rel="noopener">http://ydwen.github.io/papers/WenECCV16.pdf</a> </p>
<h3 id="二-为什么要使用Center-Loss？"><a href="#二-为什么要使用Center-Loss？" class="headerlink" title="二. 为什么要使用Center Loss？"></a>二. 为什么要使用Center Loss？</h3><p>简单的来说，我们在做分类的时候，不光需要学得separable的特征，更想要这些特征是discriminative的，这就意味着我们需要在loss上做更多的约束。</p>
<p>仅仅使用softmax作为监督信号的输出处理就只能做到seperable而不是discriminative，如下图:</p>
<p><img src="https://img-blog.csdn.net/20180727140845416?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<h3 id="三-如何使学到的特征差异化更大——Center-Loss"><a href="#三-如何使学到的特征差异化更大——Center-Loss" class="headerlink" title="三. 如何使学到的特征差异化更大——Center Loss"></a>三. 如何使学到的特征差异化更大——Center Loss</h3><p>融合Softmax Loss与Center loss</p>
<p><strong>Softmax Loss（保证类之间的feature距离最大）与Center Loss（保证类内的feature距离最小，更接近于类中心）</strong></p>
<p><img src="https://img-blog.csdn.net/20180727150130651?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p><img src="https://img-blog.csdn.net/2018072715022915?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>m是mini-batch、n是class。在Lc公式中有一个缺陷，就是$C_{y_{i}}$是i这个样本对应的类别yi所属于的类中心C∈ Rd，d代表d维。</p>
<p>理想情况下，Cyi需要随着学到的feature变化而实时更新，也就是要在每一次迭代中用整个数据集的feature来算每个类的中心。</p>
<p>但这显然不现实，做以下两个修改：</p>
<p>1、由整个训练集更新center改为mini-batch更改center  </p>
<p>2、避免错误分类的样本的干扰，使用scalar α 来控制center的学习率  </p>
<p>因此求算梯度的公式如下：</p>
<p><img src="https://img-blog.csdn.net/20180727153311160?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>即：当yi = j，也就是mini-batch中某一个sample是对应要更新的那一个类的center的时候就累加起来除以某类的个数+1。</p>
<p><img src="https://img-blog.csdn.net/2018072715370270?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>最终loss联立起来如上图，λ用于平衡softmax loss与center loss，越大则区分度 越大，如下图效果：</p>
<p><img src="https://img-blog.csdn.net/20180727150702547?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<h3 id="四-Center-Loss的实现"><a href="#四-Center-Loss的实现" class="headerlink" title="四. Center Loss的实现"></a>四. Center Loss的实现</h3><p>pytorch实现：<a href="https://github.com/jxgu1016/MNIST_center_loss_pytorch" target="_blank" rel="noopener">https://github.com/jxgu1016/MNIST_center_loss_pytorch</a></p>
<ul>
<li>网络结构</li>
</ul>
<p><img src="https://img-blog.csdn.net/20180727162522989?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>即在特征层输出（classification前最后一层）引入center loss：</p>
<p><img src="https://img-blog.csdn.net/20180727162755579?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<h4 id="fully-connected-和-local-connected"><a href="#fully-connected-和-local-connected" class="headerlink" title="fully-connected 和 local-connected"></a>fully-connected 和 local-connected</h4><h5 id="判断fully-connected的方法："><a href="#判断fully-connected的方法：" class="headerlink" title="判断fully-connected的方法："></a>判断fully-connected的方法：</h5><ul>
<li><p>对于neuron的链接（点对点的链接）都是fully connected（这里其实就是MLP）</p>
</li>
<li><p>对于有filter的network，不是看filter的size，而是看output的feature map的size。如果output feature map的size还是1 * 1 * N的话，这个layer就是fully connected layer</p>
<p><strong>解释第二个判断方法：</strong></p>
<ul>
<li>1 * 1的filter size不一定是fully connected。比如input size是10 * 10 * 100， filter size是1 * 1 * 100， 重复50次，则该layer的总weights是：1 * 1 * 100 * 50</li>
<li>1 * 1的filter size如果要是 fully connected， 则input size必须是1 * 1</li>
<li>input size是10x10的时候却是fully connected的情况：这里我们的output size肯定是1x1，且我们的filter size肯定是10x10。</li>
</ul>
<p><strong>总结：filter size等于input size则是fully connected</strong></p>
</li>
</ul>
<p>综上：</p>
<ul>
<li>fully connected没有weight share</li>
<li>对于neuron的连接（点对点的链接）都是fully connected（MLP——多层感知器）</li>
<li>Convolution中当filter size等于input size时，就是fully connected，此时的output size为1 * 1 * N</li>
<li>当1 *1不等于input size时，1 * 1一样具备weights share的能力。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/softmax-and-softmax-loss-and-BP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/softmax-and-softmax-loss-and-BP/" itemprop="url">softmax and softmax-loss and BP</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T13:57:43+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/" target="_blank" rel="noopener">http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/</a> </p>
<p><img src="https://img-blog.csdn.net/20170504203817251?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>联想逻辑回归的重要公式就是<img src="https://img-blog.csdn.net/20170504203942767?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img">，得到预测结果，然后再经过sigmoid转换成0到1的概率值，而在softmax中则通过取exponential的方式并进行归一化得到某个样本属于某类的概率。非负的意义不用说，就是避免正负值抵消。</p>
<p><img src="https://img-blog.csdn.net/20170504204409994?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>逻辑回归的推导可以用最大似然或最小损失函数，本质是一样的，可以简单理解成加了一个负号，这里的y指的是真实类别。注意下softmax-loss可以看做是softmax和multinomial logistic loss两步，正如上述所写公式，把变量展开即softmax-loss。</p>
<p><img src="https://img-blog.csdn.net/20170504204847454?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>原博客的重点在于介绍softmax-loss是分成两步还是一步到位比较好，而我这则重点说下BP。上面这个神经网络的图应该不陌生，这个公式也是在逻辑回归的核心（通过迭代得到w，然后在测试时按照上面这个公式计算类别概率）</p>
<p><img src="https://img-blog.csdn.net/20170504205237865?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>这里第一个公式是损失函数对权重w求导，其实就是梯度，红色那部分可以看前面O是怎么算出来的，就知道其导数的形式非常简单，就是输入I。蓝色部分就是BP的核心，回传就是通过这个达到的，回传的东西就是损失函数对该层输出的导数，只有把这个往前回传，才能计算前面的梯度。所以回传的不是对权重的求导，对每层权重的求导的结果会保留在该层，等待权重更新时候使用。具体看上面最后一个公式。</p>
<p><img src="https://img-blog.csdn.net/20170504210106340?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>这部分的求导：l(y,z)函数是log函数，log（x）函数求导是取1/x，后面的那个数是zy对zk的导数，当k=y时，那就是1，k不等于y时就是两个不同的常数求导，就是0。</p>
<p><img src="https://img-blog.csdn.net/20170504210625060?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>这一部分就是把softmax-loss分成两步来做，第一个求导可以先找到最前面l(y,o)的公式，也是log函数，所以求导比较简单。第二个求导也是查看前面Oi的公式，分母取平方的那种求导。最后链式相乘的结果和原来合并算的结果一样。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/PyTorch-中的-dim/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/PyTorch-中的-dim/" itemprop="url">PyTorch 中的 dim</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T10:24:23+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="PyTorch中的dim"><a href="#PyTorch中的dim" class="headerlink" title="PyTorch中的dim"></a>PyTorch中的dim</h2><h3 id="dim概念"><a href="#dim概念" class="headerlink" title="dim概念"></a>dim概念</h3><p>dim的不同值表示不同维度。特别的在dim=0表示二维中的行，dim=1在二维矩阵中表示行。广泛的来说，我们不管一个矩阵是几维的，比如一个矩阵维度如下：${d_{0},d_{1},…,d_{n-1}}$，那么dim=0就表示对应到$d_{0}$也就是第一个维度，dim=1,表示对应到$d_{1}$也就是第二个维度，以此类推</p>
<h3 id="dim在函数中的作用"><a href="#dim在函数中的作用" class="headerlink" title="dim在函数中的作用"></a>dim在函数中的作用</h3><h4 id="例一-torch-argmax"><a href="#例一-torch-argmax" class="headerlink" title="例一. torch.argmax()"></a>例一. torch.argmax()</h4><p>函数中dim表示该维度会消失。</p>
<p>这个消失是什么意思？官方英文解释是：dim (int) – the dimension to reduce.</p>
<p>我们知道argmax就是得到最大值的序号索引，对于一个维度为$(d_0,d_1)$的矩阵来说，我们想要求每一行中最大数的在该行中的列号，最后我们得到的就是一个维度为$(d_0,1)$的一矩阵。这时候，列就要消失了。</p>
<p>因此，我们想要求每一行最大的列标号，我们就要指定dim=1，表示我们不要列了，保留行的size就可以了。<br>假如我们想求每一列的最大行标，就可以指定dim=0，表示我们不要行了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">a = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a.size())</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">b = torch.argmax(a, dim=<span class="number">1</span>)</span><br><span class="line">print(b)</span><br><span class="line">print(b.size())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.Size([3, 4])</span></span><br><span class="line"><span class="string">tensor([[0.9120, 0.4805, 0.6701, 0.5446],</span></span><br><span class="line"><span class="string">        [0.6273, 0.1295, 0.3416, 0.2213],</span></span><br><span class="line"><span class="string">        [0.6068, 0.8448, 0.8452, 0.4931]])</span></span><br><span class="line"><span class="string">tensor([0, 0, 2])</span></span><br><span class="line"><span class="string">torch.Size([3])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 可以看到，指定dim=1时，列的size没有了</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.argmax(input, dim=<span class="literal">None</span>, keepdim=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>返回指定维度最大的序号</p>
<p>dim给定的定义是：the dimention to reduce.也就是吧dim这个维度的，变成这个维度的最大值</p>
<p>如果上面的代码改成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">b = torch.argmax(a, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output：</span></span><br><span class="line"><span class="string">tensor([[0],</span></span><br><span class="line"><span class="string">		[0],</span></span><br><span class="line"><span class="string">        [2]])</span></span><br><span class="line"><span class="string">torch.Size([3, 1])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/model-train-and-model-eval/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/model-train-and-model-eval/" itemprop="url">model.train() and model.eval()</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T09:46:19+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>model.train()     model.eval()</p>
<p>一般在模型训练和评价的时候会加上这两句</p>
<p>主要是针对model在训练时和评价时不同的 <strong>Batch Normalization</strong> 和 <strong>Dropout</strong> 方法模式</p>
<p><strong>注意：</strong>使用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval， eval()时， 框架会自动把BN和DropOut固定住，不会取平均，而是用训练好的值，不然的话，一旦test的batch_size过小，很容易就会被BN层导致生成图片颜色失真极大！</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/Python格式化输出/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/Python格式化输出/" itemprop="url">Python格式化输出</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T09:25:45+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Python格式化输出"><a href="#Python格式化输出" class="headerlink" title="Python格式化输出"></a>Python格式化输出</h2><h3 id="格式化输出字符串"><a href="#格式化输出字符串" class="headerlink" title="格式化输出字符串"></a>格式化输出字符串</h3><h4 id="用法一："><a href="#用法一：" class="headerlink" title="用法一："></a>用法一：</h4><p>与%s类似，不同指出是将<code>%s</code>换乘了<code>‘{ }’</code>大括号，调用时依然需要按照顺序对应</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">"My name is &#123;&#125;, &#123;&#125; years old, and my hobby is &#123;&#125;"</span></span><br><span class="line">s1 = s.format(<span class="string">'MMMMMQ'</span>, <span class="string">'25'</span>, <span class="string">'watching TV'</span>)</span><br><span class="line">print(s1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">My name is MMMMMQ, 25 years old, My hobby is watching TV</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="用法二："><a href="#用法二：" class="headerlink" title="用法二："></a>用法二：</h4><p>通过<code>{n}</code>方式来指定接收参数的位置，将调用时传入的参数按照位置进行传入。相比%s可以减少参数的个数，实现了参数的复用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">"My name is &#123;0&#125;, &#123;1&#125; years old, and my name is still &#123;0&#125;"</span></span><br><span class="line">s1 = s.format(<span class="string">'MMMMQ'</span>, <span class="string">'25'</span>, <span class="string">'watching TV'</span>)</span><br><span class="line">print(s1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">My name is MMMMQ, 25 years old, and my name is still MMMMQ</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="用法三："><a href="#用法三：" class="headerlink" title="用法三："></a>用法三：</h4><p>通过<code>str{}</code>方式来指定名字，调用时使用<code>str=&#39;xxx&#39;</code>，确定参数传入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">"My name is &#123;name&#125;, &#123;age&#125; years old, and my hobby is &#123;hobby&#125;"</span></span><br><span class="line">s1 = s.format(age=<span class="number">25</span>, hobby=<span class="string">'watching TV'</span>, name=<span class="string">'MMMMQ'</span>)</span><br><span class="line">print(s1)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">My name is MMMMQ, 25 years old, and my hobby is watching TV</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h3 id="保留n位小数："><a href="#保留n位小数：" class="headerlink" title="保留n位小数："></a>保留n位小数：</h3><h4 id="保留一个数字"><a href="#保留一个数字" class="headerlink" title="保留一个数字"></a>保留一个数字</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'This is a float number &#123;:.2f&#125;'</span>.format(<span class="number">123.432423432</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">This is a float number 123.43</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="保留两个数字"><a href="#保留两个数字" class="headerlink" title="保留两个数字"></a>保留两个数字</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'This are two float numbers &#123;:.2f&#125; and &#123;:.4f&#125;'</span>.format(<span class="number">123.432423432</span>, <span class="number">0.321423423</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">This are two float numbers 123.43 and 0.3214</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="保留数字和文字"><a href="#保留数字和文字" class="headerlink" title="保留数字和文字"></a>保留数字和文字</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'This are two float numbers and a string &#123;:.2f&#125; , &#123;:.4f&#125; and &#123;&#125; '</span>.format(<span class="number">123.432423432</span>, <span class="number">0.321423423</span>, <span class="string">'MMMMQ'</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">This are two float numbers and a string 123.43 , 0.3214 and MMMMQ </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/08/Neural-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/08/Neural-Network/" itemprop="url">Neural Network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-08T15:45:06+08:00">
                2019-08-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>可以使用<code>torch.nn</code>包构造神经网络</p>
<p>nn取决于autograd定义的模型并区分它们。一个<code>nn.Module</code>包含层和一种方法<code>forward(input)</code>，它返回output</p>
<p>例如，查看对数字图像进行分类的网络</p>
<p><img src="https://pytorch.org/tutorials/_images/mnist.png" alt="convnet"></p>
<p>它是一个简单的前馈网络。它接受输入，一个接一个地通过及各层输入，然后最终给出输出</p>
<p><strong>神经网络的典型训练程序如下：</strong></p>
<ul>
<li><p>定义一些具有可学习参数（或权重）的神经网络</p>
</li>
<li><p>迭代输入数据集</p>
</li>
<li><p>通过网络处理输入</p>
</li>
<li><p>计算损失（输出距离正确多远）</p>
</li>
<li><p>将渐变传播回网络参数</p>
</li>
<li><p>通常使用简单的更新规则更新网络权重</p>
<p><code>weight = weight - learning_rate * gradient</code></p>
</li>
</ul>
<h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/07/什么是PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/什么是PyTorch/" itemprop="url">什么是PyTorch?</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-07T16:37:37+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="什么是PyTorch？"><a href="#什么是PyTorch？" class="headerlink" title="什么是PyTorch？"></a>什么是PyTorch？</h3><p>这是一个基于Python的科学计算软件包，针对两组受众：</p>
<ul>
<li>Numpy的替代品，可以使用GPU的强大功能</li>
<li>深入学习研究平台，提供最大的灵活性和速度</li>
</ul>
<h3 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h3><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><p>张量与Numpy的ndarray类似，另外还有Tensor也可用于GPU以加速计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<p>构造一个未初始化的5 * 3矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[1.0561e-38, 1.0653e-38, 4.1327e-39],</span></span><br><span class="line"><span class="string">        [8.9082e-39, 9.8265e-39, 9.4592e-39],</span></span><br><span class="line"><span class="string">        [1.0561e-38, 1.0653e-38, 1.0469e-38],</span></span><br><span class="line"><span class="string">        [9.5510e-39, 1.0378e-38, 8.9082e-39],</span></span><br><span class="line"><span class="string">        [9.6429e-39, 8.9082e-39, 9.1837e-39]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>构造一个随机初始化的矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[0.4904, 0.0543, 0.2698],</span></span><br><span class="line"><span class="string">        [0.4626, 0.9494, 0.6573],</span></span><br><span class="line"><span class="string">        [0.2933, 0.7157, 0.5195],</span></span><br><span class="line"><span class="string">        [0.3329, 0.3446, 0.3271],</span></span><br><span class="line"><span class="string">        [0.3962, 0.9864, 0.2137]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>构造一个0填充的矩阵 of dtype long：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>直接从数据构造Tensor：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([5.5000, 3.0000])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>或者根据现有的张量创建张量。除非用户提供新值，否则这些方法将重用与输入张量的属性，例如dtype</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)	<span class="comment"># new method take in size</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)	<span class="comment"># override dtype!</span></span><br><span class="line">print(x)	<span class="comment"># result has the same size</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]], dtype=torch.float64)</span></span><br><span class="line"><span class="string">tensor([[-1.5400,  0.6350,  1.6990],</span></span><br><span class="line"><span class="string">        [ 0.0767,  0.3982, -0.0827],</span></span><br><span class="line"><span class="string">        [ 0.1511,  0.8096,  0.0600],</span></span><br><span class="line"><span class="string">        [ 0.2931,  0.8122, -0.3534],</span></span><br><span class="line"><span class="string">        [ 0.7164, -0.0334,  0.2272]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>得到它的大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(x, size())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.Size([5, 3])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong><code>torch.Size()</code>实际上是一个元组，因此它支持所有元组操作</p>
<h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>操作有多种语法。下面的示例中，我们将查看添加操作：</p>
<p>增加：语法1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>增加：语法2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>增加：提供输出张量作为参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>增加：就地</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 0.9977,  0.2656,  2.2842],</span></span><br><span class="line"><span class="string">        [ 0.5430, -0.5758,  1.1323],</span></span><br><span class="line"><span class="string">        [ 0.5859,  2.5553, -0.9625],</span></span><br><span class="line"><span class="string">        [-0.2265,  1.2381,  0.8061],</span></span><br><span class="line"><span class="string">        [-0.2200, -0.0597,  0.8266]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<p>任何使原张量变形的操作都是用 _ 后固定的。例如：x.copy_(y), x.t_(), 将改变x。</p>
<p>可以使用标准的NumPy索引与所有bells和whistles</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x[:, <span class="number">1</span>])</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[0.3356, 0.3659, 0.7898],</span></span><br><span class="line"><span class="string">        [0.3474, 0.4648, 0.2795],</span></span><br><span class="line"><span class="string">        [0.0268, 0.8986, 0.5615],</span></span><br><span class="line"><span class="string">        [0.8278, 0.4778, 0.0131],</span></span><br><span class="line"><span class="string">        [0.2451, 0.6178, 0.0125]])</span></span><br><span class="line"><span class="string">tensor([0.3659, 0.4648, 0.8986, 0.4778, 0.6178])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>调整大小：如果要调整tensor/重塑tensor，可以使用<code>torch.view</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)	<span class="comment"># the size of -1 is inferred from other dimensions</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br><span class="line">print(z)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</span></span><br><span class="line"><span class="string">tensor([[0.3415, 0.9245, 0.5641, 0.9463],</span></span><br><span class="line"><span class="string">        [0.5833, 0.5632, 0.3456, 0.6338],</span></span><br><span class="line"><span class="string">        [0.2562, 0.6854, 0.1495, 0.0351],</span></span><br><span class="line"><span class="string">        [0.6777, 0.8511, 0.9998, 0.7963]])</span></span><br><span class="line"><span class="string">tensor([0.3415, 0.9245, 0.5641, 0.9463, 0.5833, 0.5632, 0.3456, 0.6338, 0.2562,</span></span><br><span class="line"><span class="string">        0.6854, 0.1495, 0.0351, 0.6777, 0.8511, 0.9998, 0.7963])</span></span><br><span class="line"><span class="string">tensor([[0.3415, 0.9245, 0.5641, 0.9463, 0.5833, 0.5632, 0.3456, 0.6338],</span></span><br><span class="line"><span class="string">        [0.2562, 0.6854, 0.1495, 0.0351, 0.6777, 0.8511, 0.9998, 0.7963]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>假如你有一个元素张量，可以用item()获取值作为python数字</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([-0.2729])</span></span><br><span class="line"><span class="string">-0.2729296088218689</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>torch包含用于多维张量的数据结构，并且定义了浙西额数学运算。此外还提供了徐国使用程序，用于搞笑序列化Tensor和任意类型，以及其他有用的实用程序。</p>
<p>它有一个CUDA对应物，能够在计算能力 &gt;= 3.0的NVIDIA GPU上运行张量计算</p>
<p>关于张量的操作：</p>
<h3 id="NumPy-Bridge："><a href="#NumPy-Bridge：" class="headerlink" title="NumPy Bridge："></a>NumPy Bridge：</h3><p>将Torch Tensor转换为NumPy阵列（反之亦然）是一件轻而易举的事</p>
<p>Torch Tensor和NumPy阵列将共享其底层内存位置（如果Torch Tensor在CPU上），更改一个将改变另一个。</p>
<h4 id="将Torch-Tensor转换为NumPy数组"><a href="#将Torch-Tensor转换为NumPy数组" class="headerlink" title="将Torch Tensor转换为NumPy数组"></a>将Torch Tensor转换为NumPy数组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br><span class="line">b = a.numpy()</span><br><span class="line">print(b)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([1., 1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">[1. 1. 1. 1. 1.]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>了解numpy数组的值如何变化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([2., 2., 2., 2., 2.])</span></span><br><span class="line"><span class="string">[2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<h4 id="将NumPy数组转换为Torch-Tensor"><a href="#将NumPy数组转换为Torch-Tensor" class="headerlink" title="将NumPy数组转换为Torch Tensor"></a>将NumPy数组转换为Torch Tensor</h4><p>了解更改np阵列如何自动更改Torch Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">torch.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">[2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong>除了CharTensor之外，CPU上的所有Tensor都支持转换为NumPy并返回</p>
<h4 id="CUDA-Tensors"><a href="#CUDA-Tensors" class="headerlink" title="CUDA Tensors"></a>CUDA Tensors</h4><p>可以使用<code>.to</code>方法将张量移动到任何设备上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)			<span class="comment"># a CUDA device object </span></span><br><span class="line">   	y = torch.ones_like(x, device=device)	<span class="comment"># directly create a tensor on GPU</span></span><br><span class="line">    x = x.to(device)						<span class="comment"># or just use strings ``.to("cuda")``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))		<span class="comment"># ``.to`` can also change dtype together!</span></span><br><span class="line"> <span class="string">"""</span></span><br><span class="line"><span class="string"> output:</span></span><br><span class="line"><span class="string"> tensor([[-0.6816,  0.2219, -0.4269],</span></span><br><span class="line"><span class="string">        [-1.1169, -1.7020,  0.6161],</span></span><br><span class="line"><span class="string">        [-0.2312, -0.8429,  0.9232],</span></span><br><span class="line"><span class="string">        [ 0.8789, -0.4840,  0.8420],</span></span><br><span class="line"><span class="string">        [-1.4957, -0.8483,  0.0130]])</span></span><br><span class="line"><span class="string">tensor([[ 0.3184,  1.2219,  0.5731],</span></span><br><span class="line"><span class="string">        [-0.1169, -0.7020,  1.6161],</span></span><br><span class="line"><span class="string">        [ 0.7688,  0.1571,  1.9232],</span></span><br><span class="line"><span class="string">        [ 1.8789,  0.5160,  1.8420],</span></span><br><span class="line"><span class="string">        [-0.4957,  0.1517,  1.0130]], device='cuda:0')</span></span><br><span class="line"><span class="string">tensor([[ 0.3184,  1.2219,  0.5731],</span></span><br><span class="line"><span class="string">        [-0.1169, -0.7020,  1.6161],</span></span><br><span class="line"><span class="string">        [ 0.7688,  0.1571,  1.9232],</span></span><br><span class="line"><span class="string">        [ 1.8789,  0.5160,  1.8420],</span></span><br><span class="line"><span class="string">        [-0.4957,  0.1517,  1.0130]], dtype=torch.float64)</span></span><br><span class="line"><span class="string"> """</span></span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/07/PyTorch-Study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/PyTorch-Study/" itemprop="url">PyTorch Study</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-07T14:57:07+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="PyTorch搭建神经网络"><a href="#PyTorch搭建神经网络" class="headerlink" title="PyTorch搭建神经网络"></a>PyTorch搭建神经网络</h3><h4 id="1-导入库"><a href="#1-导入库" class="headerlink" title="1. 导入库"></a>1. 导入库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br></pre></td></tr></table></figure>

<h4 id="2-搭建卷积神经网络"><a href="#2-搭建卷积神经网络" class="headerlink" title="2. 搭建卷积神经网络"></a>2. 搭建卷积神经网络</h4><p>网络定义一般由两部分组成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span>		<span class="comment"># 用来定义网络节点参数</span></span><br></pre></td></tr></table></figure>

<p>将节点连接成图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br></pre></td></tr></table></figure>

<p>卷积计算规则：</p>
<p>对我们输入形状1， 1， 64， 64.四个维度分别是<code>(batch, channel, height, width)</code></p>
<p><code>new_height = (height - kernel_size) + s * padding / (stride[0]) + 1</code></p>
<p>即在周围补一圈0， stride默认为1.因此</p>
<p><code>new_height = new_width = (64 - 3) / 1 + 1 = 62</code></p>
<p>由于输出通道是6，所以通过卷积层后维度为<code>(1, 6, 62, 62)</code></p>
<p>经过pooling后，<code>(1, 6, 31, 31)</code></p>
<p><code>x.view(1, -1)</code>把x伸缩为(1 : ?)的维度。这样整个网络其实输入<code>(1, 1, 64, 64)</code>，输出为(1, 10)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.linear = nn.Leaner(<span class="number">5766</span>, <span class="number">10</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpooling = nn.MaxPool2d((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="comment"># print (x.shape)</span></span><br><span class="line">        x = self.maxpooling(x)</span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = view(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># print (x.shape)</span></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h4 id="3-添加训练数据"><a href="#3-添加训练数据" class="headerlink" title="3. 添加训练数据"></a>3. 添加训练数据</h4><p>optimizer：是优化器，即所谓的反向传播算法。<code>criterion = nn.MSELoss()</code>定义损失函数。</p>
<p><code>input = torch.randn(1， 1， 64， 64).cuda()</code></p>
<p><code>output = torch.ones(1, 10).cuda()</code></p>
<p>定义训练样本，注意如果实在gpu中训练，在pytorch中需要.cuda()把数据从cpu中导入到gpu中</p>
<p>网络的功能是给定随机噪声向量，输出是逼近1的单位向量</p>
<h4 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(epoch):</span><br><span class="line">    prediction = net(input)</span><br><span class="line">    loss = creterion(prediction, output)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 消除优化器梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 指自动求导</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># 根据自动求导反向传播优化参数</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"EPOCH: &#123;&#125;, Loss:&#123;:4f&#125;"</span>).format(step, loss))</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/07/Inception/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/07/Inception/" itemprop="url">Inception</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-07T12:55:01+08:00">
                2019-08-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Inception-v1"><a href="#Inception-v1" class="headerlink" title="Inception-v1"></a>Inception-v1</h3><p>在这篇论文之前，卷积神经网络的性能高都是依赖于提高网络的深度和宽度，而这篇论文是从网络结构上入手，改变了网络结构</p>
<h4 id="为什么提出Inception？"><a href="#为什么提出Inception？" class="headerlink" title="为什么提出Inception？"></a>为什么提出Inception？</h4><p>提高网络最简单粗暴的方法就是提高网络的深度和宽度，即增加隐层以及各层神经元数目。但这种简单粗暴的方法存在一些问题：</p>
<ul>
<li>会导致更大的参数空间，更容易过拟合</li>
<li>需要更多的计算资源</li>
<li>网络越深，梯度容易消失，优化困难（这时还没有提出BN，网络优化极其困难）</li>
</ul>
<p>基于此，我们的目标就是，提高网络计算资源的利用率，在计算率不变的情况下，提高网络的宽度和深度</p>
<p>作者认为，解决这种困难的方法就是，把全连接改成稀疏连接，卷积层也是稀疏连接，但是不对称稀疏数据数值计算效率低下，因为硬件全是针对密集矩阵优化的，所以，我们要找到卷积网络可以近似优化的最优局部稀疏结构，并且该结构下可以用现有的密度矩阵计算硬件实现，产生的结果就是Inception</p>
<h4 id="Inception结构"><a href="#Inception结构" class="headerlink" title="Inception结构"></a>Inception结构</h4><p><img src="https://img-blog.csdn.net/20180709151519421?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTk1MzUwMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>首先看第一个结构，有四个通道，有1 * 1、3 * 3、5 * 5卷积核，该结构有几个特点：</p>
<ul>
<li>使用这些大小卷积核，没有什么特殊含义，主要方便对齐，只要padding = 0、1、2，就可以得到相同大小的特征图，可以顺利concat</li>
<li>采用大小不同的卷积核，以为着感受野的大小不同，就可以得到不同尺度的特征</li>
<li>采用比较大的卷积核，即5 * 5，意味着有些相关性可能隔得比较远，用大的卷积核才能学到此特征</li>
</ul>
<p>当时，这个结构有个缺点，5 * 5的卷积核的计算量太大。那么作者想到的第二个结构，用1 * 1的卷积核进行降维。</p>
<p>这个1 * 1的卷积核，它的作用就是：</p>
<ul>
<li>降低维度，减少计算瓶颈</li>
<li>增加网络层数，提高网络的表达能力。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Mariana</p>
              <p class="site-description motion-element" itemprop="description">a study blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">66</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mariana</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
