<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>Something about TensorFlow | Celery Fairy</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1. TensorFlow中tf.train.slice_input_producer和tf.train.batch函数TensorFlow数据读取机制TensorFlow中为了充分利用GPU，减少GPU等待数据的空闲时间，使用了两个线程分别执行数据读入和数据计算。 具体来说就是使用一个线程源源不断的将硬盘中的图片数据读入到一个内存队列中，另一个线程负责计算任务，所需数据直接从内存队列中获取。 t">
<meta property="og:type" content="article">
<meta property="og:title" content="Something about TensorFlow">
<meta property="og:url" content="https://woojoo520.github.io/2019/07/25/Something-about-TensorFlow/index.html">
<meta property="og:site_name" content="Celery Fairy">
<meta property="og:description" content="1. TensorFlow中tf.train.slice_input_producer和tf.train.batch函数TensorFlow数据读取机制TensorFlow中为了充分利用GPU，减少GPU等待数据的空闲时间，使用了两个线程分别执行数据读入和数据计算。 具体来说就是使用一个线程源源不断的将硬盘中的图片数据读入到一个内存队列中，另一个线程负责计算任务，所需数据直接从内存队列中获取。 t">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://img-blog.csdn.net/20180401131308152">
<meta property="og:updated_time" content="2019-08-15T08:48:10.294Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Something about TensorFlow">
<meta name="twitter:description" content="1. TensorFlow中tf.train.slice_input_producer和tf.train.batch函数TensorFlow数据读取机制TensorFlow中为了充分利用GPU，减少GPU等待数据的空闲时间，使用了两个线程分别执行数据读入和数据计算。 具体来说就是使用一个线程源源不断的将硬盘中的图片数据读入到一个内存队列中，另一个线程负责计算任务，所需数据直接从内存队列中获取。 t">
<meta name="twitter:image" content="https://img-blog.csdn.net/20180401131308152">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Celery Fairy</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://woojoo520.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Something-about-TensorFlow" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/07/25/Something-about-TensorFlow/" class="article-date">
  <time datetime="2019-07-25T09:16:29.000Z" itemprop="datePublished">2019-07-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Something about TensorFlow
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="1-TensorFlow中tf-train-slice-input-producer和tf-train-batch函数"><a href="#1-TensorFlow中tf-train-slice-input-producer和tf-train-batch函数" class="headerlink" title="1. TensorFlow中tf.train.slice_input_producer和tf.train.batch函数"></a>1. TensorFlow中tf.train.slice_input_producer和tf.train.batch函数</h3><h4 id="TensorFlow数据读取机制"><a href="#TensorFlow数据读取机制" class="headerlink" title="TensorFlow数据读取机制"></a>TensorFlow数据读取机制</h4><p>TensorFlow中为了充分利用GPU，减少GPU等待数据的空闲时间，使用了两个线程分别执行数据读入和数据计算。</p>
<p>具体来说就是使用一个线程源源不断的将硬盘中的图片数据读入到一个内存队列中，另一个线程负责计算任务，所需数据直接从内存队列中获取。</p>
<p>tf在内存队列之前，还设立了一个文件名队列，文件名队列存放的是参与训练的文件名，要训练N个epoch，则文件名队列中就含有N个批次的所有文件名。</p>
<p><img src="https://img-blog.csdn.net/20180401131308152" alt="img"></p>
<p>在N个epoch的文件名最后是一个结束标志，当tf读到这个结束标志的时候，会抛出一个AutoRange的异常，外部捕获到之歌异常之后皆可以结束进程了。而创建tf的文件名队列就需要使用到tf.train.slice_input_producer函数</p>
<h4 id="tf-train-slice-input-producer"><a href="#tf-train-slice-input-producer" class="headerlink" title="tf.train.slice_input_producer"></a>tf.train.slice_input_producer</h4><p>tf.train.slice_input_producer是一个tensor生成器，作用是按照规定，每次从一个tensor列表中按顺序或者随机抽取出一个tensor放入文件名队列。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None,                         capacity=32, shared_name=None, name=None)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>第一个参数 tensor_list：包含一系列tensor的列表，表中tensor的第一维度的值必须相等，即个数必须相等，有多少个图像，就应该有多少个对应的标签。</p>
</li>
<li><p>第二个参数<strong>num_epochs: 可选参数，是一个整数值，代表迭代的次数</strong>，如果设置 num_epochs=None,生成器可以无限次遍历tensor列表，如果设置为 num_epochs=N，生成器只能遍历tensor列表N次。</p>
</li>
<li><p>第三个参数shuffle： bool类型，设置是否打乱样本的顺序。<strong>一般情况下，如果shuffle=True，生成的样本顺序就被打乱了，在批处理的时候不需要再次打乱样本，使用 tf.train.batch函数就可以了;如果shuffle=False,就需要在批处理时候使用 tf.train.shuffle_batch函数打乱样本。</strong></p>
</li>
<li><p>第四个参数seed: <strong>可选的整数，是生成随机数的种子，在第三个参数设置为shuffle=True的情况下才有用。</strong></p>
</li>
<li><p>第五个参数capacity：设置tensor列表的容量。</p>
</li>
<li><p>第六个参数shared_name：可选参数，如果设置一个‘shared_name’，则在不同的上下文环境（Session）中可以通过这个名字共享生成的tensor。</p>
</li>
<li><p>第七个参数name：可选，设置操作的名称。</p>
</li>
</ul>
<p>tf.train.slice_input_producer定义了样本放入文件名队列的方式，包括迭代次数，是否乱序等，<strong>要真正将文件放入文件名队列，还需要调用tf.train.start_queue_runners 函数来启动执行文件名队列填充的线程，之后计算单元才可以把数据读出来，否则文件名队列为空的，计算单元就会处于一直等待状态，导致系统阻塞。</strong></p>
<h4 id="tf-contrib-layers-batch-norm"><a href="#tf-contrib-layers-batch-norm" class="headerlink" title="tf.contrib.layers.batch_norm"></a><strong>tf.contrib.layers.batch_norm</strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tf.contrib.layers.batch_norm(</span><br><span class="line">    inputs,</span><br><span class="line">    decay=0.999,</span><br><span class="line">    center=True,</span><br><span class="line">    scale=False,</span><br><span class="line">    epsilon=0.001,</span><br><span class="line">    activation_fn=None,</span><br><span class="line">    param_initializers=None,</span><br><span class="line">    param_regularizers=None,</span><br><span class="line">    updates_collections=tf.GraphKeys.UPDATE_OPS,</span><br><span class="line">    is_training=True,</span><br><span class="line">    reuse=None,</span><br><span class="line">    variables_collections=None,</span><br><span class="line">    outputs_collections=None,</span><br><span class="line">    trainable=True,</span><br><span class="line">    batch_weights=None,</span><br><span class="line">    fused=None,</span><br><span class="line">    data_format=DATA_FORMAT_NHWC,</span><br><span class="line">    zero_debias_moving_mean=False,</span><br><span class="line">    scope=None,</span><br><span class="line">    renorm=False,</span><br><span class="line">    renorm_clipping=None,</span><br><span class="line">    renorm_decay=0.99,</span><br><span class="line">    adjustment=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>Batch Normalization 通过减少内部使用协变量加速神经网络的训练</p>
<p>可以用作conv2d或fully_connected的标准化函数</p>
<p>参数：</p>
<p>1 inputs： 输入</p>
<p>2 decay ：衰减系数。合适的衰减系数值接近1.0,特别是含多个9的值：0.999,0.99,0.9。如果训练集表现很好而验证/测试集表现得不好，选择小的系数（推荐使用0.9）。如果想要提高稳定性，zero_debias_moving_mean设为True</p>
<p>3 center：如果为True，有beta偏移量；如果为False，无beta偏移量</p>
<p>4 scale：如果为True，则乘以gamma。如果为False，gamma则不使用。当下一层是线性的时（例如nn.relu），由于缩放可以由下一层完成，所以可以禁用该层。</p>
<p>5 epsilon：避免被零除</p>
<p>6 activation_fn：用于激活，默认为线性激活函数</p>
<p>7 param_initializers ： beta, gamma, moving mean and moving variance的优化初始化</p>
<p>8 param_regularizers ： beta and gamma正则化优化</p>
<p>9 updates_collections ：Collections来收集计算的更新操作。updates_ops需要使用train_op来执行。如果为None，则会添加控件依赖项以确保更新已计算到位。</p>
<p>10 is_training:图层是否处于训练模式。在训练模式下，它将积累转入的统计量moving_mean并 moving_variance使用给定的指数移动平均值 decay。当它不是在训练模式，那么它将使用的数值moving_mean和moving_variance。<br>11 scope：可选范围variable_scope</p>
<h4 id="tf-contrib-layers-convolution2d"><a href="#tf-contrib-layers-convolution2d" class="headerlink" title="tf.contrib.layers.convolution2d"></a>tf.contrib.layers.convolution2d</h4><h4 id="tf-reduce-mean"><a href="#tf-reduce-mean" class="headerlink" title="tf.reduce_mean"></a>tf.reduce_mean</h4><p>用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的平均值，主要用作降维或者计算tensor（图像）的平均值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">reduce_mean(</span><br><span class="line">	input_tensor， </span><br><span class="line">    axis=<span class="literal">None</span>, </span><br><span class="line">	keep_dims=<span class="literal">False</span>,</span><br><span class="line">	name=<span class="literal">None</span>, </span><br><span class="line">	reduction_indices=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>参数：<ul>
<li>axis：指定的轴，如果不指定，则计算所有元素的均值</li>
<li>keep_dims：是否降维度，设置为True， 则输出的结果保持输入tensor的结果，设置为False， 输出结果会降低维度</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"></span><br><span class="line">x = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">     [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br><span class="line"></span><br><span class="line">xx = tf.cast(x, tf.float32)</span><br><span class="line">mean_all = tf.reduce_mean(xx, keep_dims=<span class="literal">False</span>)</span><br><span class="line">mean_0 = tf.reduce_mean(xx, axis=<span class="number">0</span>, keep_dims=<span class="literal">False</span>)</span><br><span class="line">mean_1 = tf.reduce_mean(xx, axis=<span class="number">1</span>, keep_dims=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    m_a, m_0, m_1 = sess.run([mean_all, mean_0, mean_1])</span><br><span class="line"></span><br><span class="line">print(mean_a)</span><br><span class="line">print(mean_0)</span><br><span class="line">print(mean_1)    </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string"> 2.0</span></span><br><span class="line"><span class="string">[ 1. 2. 3.]</span></span><br><span class="line"><span class="string">[ 2. 2.]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">如果keep_dims=True，则结果为</span></span><br><span class="line"><span class="string">[[ 2.]]</span></span><br><span class="line"><span class="string">[[ 1. 2. 3.]]</span></span><br><span class="line"><span class="string">[[ 2.], [ 2.]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://woojoo520.github.io/2019/07/25/Something-about-TensorFlow/" data-id="cjzyyls20000sqsujkcchl548" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/07/25/YOLO/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          YOLO
        
      </div>
    </a>
  
  
    <a href="/2019/07/25/循环神经网络RNN/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">循环神经网络RNN</div>
    </a>
  
</nav>

  
</article>

</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Mariana<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<script src="/js/script.js"></script>

  </div>
</body>
</html>