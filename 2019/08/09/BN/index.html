<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="Celery Fairy" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://woojoo520.github.io').hostname,
    root: '/',
    scheme: 'Pisces',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="BN的理解  0.问题 ​	机器学习领域有个很重要的假设：IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。 ​	思考一个问题：为什么传统的神经网络在训练开始之前，要对输入的数据做Normali">
<meta name="keywords" content="PyTorch">
<meta property="og:type" content="article">
<meta property="og:title" content="BN">
<meta property="og:url" content="https://woojoo520.github.io/2019/08/09/BN/index.html">
<meta property="og:site_name" content="Celery Fairy">
<meta property="og:description" content="BN的理解  0.问题 ​	机器学习领域有个很重要的假设：IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。 ​	思考一个问题：为什么传统的神经网络在训练开始之前，要对输入的数据做Normali">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200545466-1938722586.png">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200638763-60126435.png">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200847123-286502102.png">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200911290-1256604003.png">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012204510101-2110717569.jpg">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/1393464/201805/1393464-20180528103414148-1206065150.png">
<meta property="og:updated_time" content="2019-11-26T09:25:56.263Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BN">
<meta name="twitter:description" content="BN的理解  0.问题 ​	机器学习领域有个很重要的假设：IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。 ​	思考一个问题：为什么传统的神经网络在训练开始之前，要对输入的数据做Normali">
<meta name="twitter:image" content="https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200545466-1938722586.png">

<link rel="canonical" href="https://woojoo520.github.io/2019/08/09/BN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>BN | Celery Fairy</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Celery Fairy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Celery's Blog</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/BN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/logo.gif">
      <meta itemprop="name" content="Woojoo">
      <meta itemprop="description" content="a study blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          BN
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-09 09:57:45" itemprop="dateCreated datePublished" datetime="2019-08-09T09:57:45+08:00">2019-08-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-26 17:25:56" itemprop="dateModified" datetime="2019-11-26T17:25:56+08:00">2019-11-26</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/08/09/BN/#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/08/09/BN/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="bn的理解"><a class="markdownIt-Anchor" href="#bn的理解"></a> BN的理解</h2>
<h3 id="0问题"><a class="markdownIt-Anchor" href="#0问题"></a> 0.问题</h3>
<p>​	机器学习领域有个很重要的假设：<strong>IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的</strong>，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？<strong>BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。</strong></p>
<p>​	思考一个问题：为什么传统的神经网络在训练开始之前，要对输入的数据做Normalization?原因在于神经网络学习过程<strong>本质上是为了学习数据的分布</strong>，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另一方面，一旦在mini-batch梯度下降训练的时候，每批训练数据的分布不相同，那么网络就要在每次迭代的时候去学习以适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对所有训练数据做一个Normalization预处理的原因。</p>
<p>为什么深度神经网络随着网络深度加深，训练起来越困难，收敛越来越慢？这是个在DL领域很接近本质的好问题。很多论文都是解决这个问题的，比如ReLU激活函数，再比如Residual Network，<strong>BN本质上也是解释并从某个不同的角度来解决这个问题的。</strong></p>
<p><strong>1、“Internal Covariate Shift”问题</strong></p>
<p>从论文名字可以看出，BN是用来解决“Internal Covariate Shift”问题的，那么首先得理解什么是“Internal Covariate Shift”？</p>
<p>论文首先说明Mini-Batch SGD相对于One Example SGD的两个优势：<strong>梯度更新方向更准确；并行计算速度快；</strong>（为什么要说这些？因为BatchNorm是基于Mini-Batch SGD的，所以先夸下Mini-Batch SGD，当然也是大实话）；然后吐槽下SGD训练的缺点：<strong>超参数调起来很麻烦</strong>。（作者隐含意思是用BN就能解决很多SGD的缺点）</p>
<p>接着<strong>引入covariate shift的概念</strong>：如果ML系统实例集合&lt;X,Y&gt;中的输入值X的分布老是变，这不符合IID假设，网络模型很难稳定的学规律，这不得引入迁移学习才能搞定吗，我们的ML系统还得去学习怎么迎合这种分布变化啊。对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是<strong>在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。</strong></p>
<p>然后提出了BatchNorm的基本思想：能不能让每个隐层节点的激活输入分布固定下来呢？这样就避免了“Internal Covariate Shift”问题了，顺带解决反向传播中梯度消失问题。BN 其实就是在做 feature scaling，而且它的目的也是为了在训练的时候避免这种 Internal Covariate Shift 的问题，只是刚好也解决了 sigmoid 函数梯度消失的问题。</p>
<p>BN不是凭空拍脑袋拍出来的好点子，它是有启发来源的：之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓<strong>白化，就是对输入数据分布变换到0均值，单位方差的正态分布——那么神经网络会较快收敛</strong>，那么BN作者就开始推论了：图像是深度神经网络的输入层，做白化能加快收敛，那么其实对于深度网络来说，其中某个隐层的神经元是下一层的输入，意思是其实<strong>深度神经网络的每一个隐层都是输入层，不过是相对下一层来说而已</strong>，那么能不能对每个隐层都做白化呢？这就是启发BN产生的原初想法，而BN也确实就是这么做的，<strong>可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作。</strong></p>
<h3 id="2-batchnorm的本质思想"><a class="markdownIt-Anchor" href="#2-batchnorm的本质思想"></a> <strong>2、BatchNorm的本质思想</strong></h3>
<p>BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，<strong>之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因</strong>，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其实<strong>就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p>
<p>THAT’S IT。其实一句话就是：<strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题</strong>。因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。BN说到底就是这么个机制，方法很简单，道理很深刻。</p>
<p>从上面几个图应该看出来BN在干什么了吧？其实就是把隐层神经元激活输入x=WU+B从变化不拘一格的正态分布通过BN操作拉回到了均值为0，方差为1的正态分布，即原始正态分布中心左移或者右移到以0为均值，拉伸或者缩减形态形成以1为方差的图形。什么意思？<strong>就是说经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。</strong></p>
<p>但是很明显，看到这里，稍微了解神经网络的读者一般会提出一个疑问：如果都通过BN，那么不就跟把非线性函数替换成线性函数效果相同了？这意味着什么？我们知道，如果是多层的线性函数变换其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的表达能力下降了，这也意味着深度的意义就没有了。所以BN<strong>为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者右移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动。</strong></p>
<p>**核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。**当然，这是我的理解，论文作者并未明确这样说。但是很明显这里的scale和shift操作是会有争议的，因为按照论文作者论文里写的理想状态，就会又通过scale和shift操作把变换后的x调整回未变换的状态，那不是饶了一圈又绕回去原始的“Internal Covariate Shift”问题里去了吗，感觉论文作者并未能够清楚地解释scale和shift操作的理论原因。</p>
<h3 id="3-训练阶段如何做batchnorm"><a class="markdownIt-Anchor" href="#3-训练阶段如何做batchnorm"></a> <strong>3、训练阶段如何做BatchNorm</strong></h3>
<p><img src="https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200545466-1938722586.png" alt="img"></p>
<p>对于Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：</p>
<p><img src="https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200638763-60126435.png" alt="img"></p>
<p>要注意，这里t层某个神经元的x(k)不是指原始输入，就是说不是t-1层每个神经元的输出，而是t层这个神经元的线性激活x=WU+B，这里的U才是t-1层神经元的输出。变换的意思是：某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)来进行转换。</p>
<p>上文说过**经过这个变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。**但是这样会导致网络表达能力下降，为了防止这一点，<strong>每个神经元增加两个调节参数（scale和shift），这两个参数是通过训练来学习到的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scale和shift操作</strong>，这其实是变换的反操作：</p>
<p><img src="https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200847123-286502102.png" alt="img"></p>
<p>BN其具体操作流程，如论文中描述的一样：</p>
<p><img src="https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200911290-1256604003.png" alt="img"></p>
<p>走一遍Batch Normalization网络层的前向传播过程。</p>
<p><img src="https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012204510101-2110717569.jpg" alt="img"></p>
<h3 id="4-batchnorm的推理inference过程"><a class="markdownIt-Anchor" href="#4-batchnorm的推理inference过程"></a> <strong>4、BatchNorm的推理(Inference)过程</strong></h3>
<p>BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，很明显输入就只有一个实例，看不到Mini-Batch其它实例，那么这时候怎么对输入做BN呢？因为很明显一个实例是没法算实例集合求出的均值和方差的。这可如何是好？既然没有从Mini-Batch数据里可以得到的统计量，那就想其它办法来获得这个统计量，就是均值和方差。<strong>可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量</strong>，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。</p>
<p>决定了获得统计量的数据范围，那么接下来的问题是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-Batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量</p>
<h3 id="5-batchnorm的好处"><a class="markdownIt-Anchor" href="#5-batchnorm的好处"></a> <strong>5、BatchNorm的好处</strong></h3>
<p>BatchNorm为什么NB呢，关键还是效果好。</p>
<p>①不仅仅极大提升了训练速度，收敛过程大大加快；</p>
<p>②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；</p>
<p>③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。</p>
<p>总而言之，经过这么简单的变换，带来的好处多得很，这也是为何现在BN这么快流行起来的原因。</p>
<h3 id="6-tensorflow中的bn"><a class="markdownIt-Anchor" href="#6-tensorflow中的bn"></a> <strong>6、tensorflow中的BN</strong></h3>
<p>为了activation能更有效地使用输入信息，所以一般BN放在激活函数之前。</p>
<p>一个batch里的128个图，经过一个64 kernels卷积层处理，得到了128×64个图，再<strong>针对每一个kernel所对应的128个图，求它们所有像素的mean和variance</strong>，因为总共有64个kernels，输出的结果就是一个<strong>一维长度64的数组</strong>啦！最后输出是（64，）的数组向量。</p>
<p><img src="https://images2018.cnblogs.com/blog/1393464/201805/1393464-20180528103414148-1206065150.png" alt="img"></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/08/09/model-train-and-model-eval/" rel="prev" title="model.train() and model.eval()">
      <i class="fa fa-chevron-left"></i> model.train() and model.eval()
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/08/09/PyTorch-中的-dim/" rel="next" title="PyTorch 中的 dim">
      PyTorch 中的 dim <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="comments"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#bn的理解"><span class="nav-number">1.</span> <span class="nav-text"> BN的理解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#0问题"><span class="nav-number">1.1.</span> <span class="nav-text"> 0.问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-batchnorm的本质思想"><span class="nav-number">1.2.</span> <span class="nav-text"> 2、BatchNorm的本质思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-训练阶段如何做batchnorm"><span class="nav-number">1.3.</span> <span class="nav-text"> 3、训练阶段如何做BatchNorm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-batchnorm的推理inference过程"><span class="nav-number">1.4.</span> <span class="nav-text"> 4、BatchNorm的推理(Inference)过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-batchnorm的好处"><span class="nav-number">1.5.</span> <span class="nav-text"> 5、BatchNorm的好处</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-tensorflow中的bn"><span class="nav-number">1.6.</span> <span class="nav-text"> 6、tensorflow中的BN</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Woojoo"
      src="/images/logo.gif">
  <p class="site-author-name" itemprop="name">Woojoo</p>
  <div class="site-description" itemprop="description">a study blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">140</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Woojoo</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.5.0
  </div>

        








        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
















  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'aKRj8pzPJm0LdONJb0Ci0U5L-gzGzoHsz',
    appKey: 'vA8nbcq2HgWrqovGq6LwXRG1',
    placeholder: "Just go go",
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn',
    path: location.pathname,
    recordIP: false,
    serverURLs: ''
  });
}, window.Valine);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
