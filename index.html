<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="a study blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Celery Fairy">
<meta property="og:url" content="https://woojoo520.github.io/index.html">
<meta property="og:site_name" content="Celery Fairy">
<meta property="og:description" content="a study blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Celery Fairy">
<meta name="twitter:description" content="a study blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://woojoo520.github.io/">





  <title>Celery Fairy</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <a href="https://github.com/woojoo520" class="github-corner" aria-label="View source on GitHub">
      <svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/>
      </svg>
    </a>
    <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Celery Fairy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Celery's Blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/11/26/认识数据/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/26/认识数据/" itemprop="url">认识数据</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-26T10:52:23+08:00">
                2019-11-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="认识数据"><a href="#认识数据" class="headerlink" title="认识数据"></a>认识数据</h2><h3 id="数据对象和属性类型"><a href="#数据对象和属性类型" class="headerlink" title="数据对象和属性类型"></a>数据对象和属性类型</h3><h4 id="数据对象"><a href="#数据对象" class="headerlink" title="数据对象"></a>数据对象</h4><p>数据集由数据对象构成，一个数据对象代表一个实体。由称为样本、示例、事例、数据点、对象、元组等等</p>
<p>数据对象由属性来描述</p>
<h4 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h4><p>属性（Attribute）或维度，特征，变量</p>
<h5 id="标称属性或名词性属性-Nominal-attribute"><a href="#标称属性或名词性属性-Nominal-attribute" class="headerlink" title="标称属性或名词性属性(Nominal attribute)"></a>标称属性或名词性属性(Nominal attribute)</h5><p>属性的值是<strong>一些符号或事物的名称</strong>。每个值代表某种类别、编码或状态，因此标称属性有被看做是<strong>分类</strong>的</p>
<p>属性的值不必具有有意义的序，因此是无序的，或是枚举的</p>
<p><strong>属性的值没有数学运算的意义</strong>，即均值、中位数等没有意义</p>
<p>例如：婚姻状态={单身、结婚、离异、丧偶}</p>
<h5 id="二元属性-Binary-attribute"><a href="#二元属性-Binary-attribute" class="headerlink" title="二元属性(Binary attribute)"></a>二元属性(Binary attribute)</h5><p>布尔属性的名词性属性：只有<strong>两个状态</strong>的<strong>名词性属性</strong></p>
<ul>
<li><p>对称二元(Symmetric binary)</p>
<p>同等重要的两种状态（例如：性别）</p>
</li>
<li><p>非对称(Asymmetric binary)</p>
<p>非同等重要的两种状态（例如：医疗检查中的阴性和阳性）</p>
</li>
</ul>
<h5 id="序数属性-Ordinal-attribute"><a href="#序数属性-Ordinal-attribute" class="headerlink" title="序数属性(Ordinal attribute)"></a>序数属性(Ordinal attribute)</h5><p>属性值之间具有有意义的序或级别（Ranking），但相继值之间的差是未知的</p>
<p>例如：drink_size：大、中、小， grade：A+, A, A-, B+等等</p>
<p><strong>对于记录不能客观度量的主观质量评估，使用序数属性</strong>，如等级评定调查，顾客满意度</p>
<p><strong>数值属性的离散化：</strong>将某种属性的数值量划分成有限个有序类别，标称、二元和序数属性都是<strong>定性的，</strong>他们描述数据对象的特征，而不给出实际的大小或数量，是一种代表类别的词</p>
<h5 id="数值属性-Numeric-attribute"><a href="#数值属性-Numeric-attribute" class="headerlink" title="数值属性(Numeric attribute)"></a>数值属性(Numeric attribute)</h5><p>是定量的，是可度量的两，用整数或实数值表示。分为区间标度或比率标度</p>
<ul>
<li>区间（interval-scaled）<ul>
<li>用相等的单位尺度度量</li>
<li>属性值有序，可以为正、零、负</li>
<li><strong>没有真正的零点，无法计算倍数</strong></li>
<li>例如：摄氏度</li>
</ul>
</li>
<li>比率标度属性（Ratio-scaled）<ul>
<li><strong>有真正的零点</strong>，被测量单位一个数量级</li>
<li>开尔文温度，长度，计数，货币的数量等等</li>
</ul>
</li>
</ul>
<h4 id="离散属性与连续属性"><a href="#离散属性与连续属性" class="headerlink" title="离散属性与连续属性"></a>离散属性与连续属性</h4><h5 id="离散属性（Discrete-Attribute）"><a href="#离散属性（Discrete-Attribute）" class="headerlink" title="离散属性（Discrete Attribute）"></a>离散属性（Discrete Attribute）</h5><ul>
<li>一个有限的或可数无限集值</li>
<li>有时，表示为整数变量</li>
<li>注：二元属性是离散属性的一个特殊情况</li>
</ul>
<h5 id="连续属性（Continuous-Attribute）"><a href="#连续属性（Continuous-Attribute）" class="headerlink" title="连续属性（Continuous Attribute）"></a>连续属性（Continuous Attribute）</h5><ul>
<li>属性值为实数</li>
<li>实际上，实值只能使用有限位数进行测量和代表</li>
<li>连续属性通常表示为浮点变量</li>
</ul>
<h3 id="数据的基本统计描述"><a href="#数据的基本统计描述" class="headerlink" title="数据的基本统计描述"></a>数据的基本统计描述</h3><p>基本统计描述可以用来识别数据的性质，凸显那些数据值应该视为噪声或离群点；选择何种适用的数据挖掘算法等等</p>
<h4 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h4><h5 id="数据的计量尺度"><a href="#数据的计量尺度" class="headerlink" title="数据的计量尺度"></a>数据的计量尺度</h5><p>按照对事物计量的准确程度，可将所采用的计量尺度由低级到高级分为四个层次</p>
<ul>
<li><p>定类尺度    </p>
<ul>
<li><p><strong>按照事物的某种属性对其进行平行的分类或分组</strong>，计量层次最低，各类别可以指定数字代码表示，具有$=$或$\neq$的数学特性，数据表现为“类别”  </p>
</li>
<li><p>只测度了事物之间的类别差，而对各类之间的其他差别却无法从中得知，因此，<strong>各类地位相同，顺序可以任意改变</strong></p>
</li>
<li><p>对定类尺度的计量结果，可以且只能计算每一类别中个元素出现的频数</p>
</li>
<li><p>对事物进行分类时，必须<strong>符合穷尽和互斥的要求</strong></p>
</li>
</ul>
</li>
<li><p>定序尺度</p>
<ul>
<li><p>对事物之间等级或顺序差别的一种测度。比定类尺度精确</p>
</li>
<li><p>不仅可以测度类别差（分类），还可以测度次序差（比较优劣或排序）。<strong>数据表现为“类别”，但有序</strong></p>
</li>
<li><p>无法测出类别之间的准确差值。该尺度的计量结果只能排序，不能进行算数运算，具有$&lt;$ 或$&gt;$的数学特性</p>
</li>
</ul>
</li>
<li><p>定距尺度（间隔尺度）</p>
<ul>
<li><p>是对事物类别或次序之间间距的测度（例如：100分制考试成绩）</p>
</li>
<li><p><strong>不仅能将事物区分为不同类型并进行排序，而且可准确指出类别之间的差距是多少</strong></p>
</li>
<li><p>比定序尺度精确。定距尺度通常以自然或物理单位为几辆尺度，因此数据表现为“数值”</p>
</li>
<li><p>没有绝对零点，“0”是测量尺度上的一个测量点，并不代表“没有”</p>
</li>
<li><p>计量结果可以进行加减运算，具有$+$ 或 $-$的数学特性</p>
</li>
</ul>
</li>
<li><p>定比尺度（比率尺度）</p>
<ul>
<li><p>是能够计算两个测度值之间比值的一种计量方式。（例如：职工月收入，企业产值等等）</p>
</li>
<li><p>与定距尺度属于同一层次，计量结果也表现为数值，除了具有其他三种计量尺度的全部特点之外，还具有可计算两个测度值之间比值的特点</p>
</li>
<li><p>“0”表示没有，即它有一固定的绝对“零点”，因此它可以进行加、减、乘、除运算（而定距尺度只可进行加减运算）</p>
</li>
</ul>
</li>
</ul>
<h3 id="数据分布特征的描述"><a href="#数据分布特征的描述" class="headerlink" title="数据分布特征的描述"></a>数据分布特征的描述</h3><h4 id="数据分布的特征"><a href="#数据分布的特征" class="headerlink" title="数据分布的特征"></a>数据分布的特征</h4><p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574741746381.png" alt="1574741746381"></p>
<h4 id="中心趋势度量"><a href="#中心趋势度量" class="headerlink" title="中心趋势度量"></a>中心趋势度量</h4><p>定类数据：众数</p>
<p>定序数据：中位数和四分位数</p>
<p>定距和定比数据：平均数（均值）</p>
<h4 id="集中趋势"><a href="#集中趋势" class="headerlink" title="集中趋势"></a>集中趋势</h4><p>一组数据向其中心支靠拢的倾向和程度</p>
<p><strong>测度集中趋势就是寻找数据一般水平的代表值或中心值</strong></p>
<p>不同类型的数据用不同的集中趋势测度值</p>
<p>低层次数据的集中趋势测度值适用于高层次的测量数据，翻过来高层次的集中趋势测度值并不适用于低层次的测量数据</p>
<h5 id="定类数据：众数"><a href="#定类数据：众数" class="headerlink" title="定类数据：众数"></a>定类数据：众数</h5><p>不受极端值的影响，可能没有众数或有几个众数</p>
<h5 id="定序数据：中位数"><a href="#定序数据：中位数" class="headerlink" title="定序数据：中位数"></a>定序数据：中位数</h5><p>不受极端值的影响。主要用于定序数据，也可用于数值型数据，但不能用于定类数据</p>
<p>各变量值与中位数的离差绝对值之和最小，即$\sum_{i=1}^n|x_i-M_e|$最小</p>
<p>中位数的位置：$\frac {N+1} {2}$</p>
<p>$M_e=\begin{cases} X_{(\frac{N+1} {2}) }, &amp;\mbox{if }N \mbox{ is odd} \  \frac{1}{2}(X_{(\frac{N}{2})}+X_{(\frac{N}{2}+1)}), &amp; \mbox{if} N \mbox{ is even}  \end{cases}$</p>
<h5 id="定序数据：四分位数"><a href="#定序数据：四分位数" class="headerlink" title="定序数据：四分位数"></a>定序数据：四分位数</h5><p>排序后处于25%和75%位置上的值，不受极端值的影响，主要用于定序数据，也可用于数值型数据，但不能用于定类数据</p>
<p>数据散布度量：极差、四分位数、方差、标准差和四分位数极差</p>
<p>数据的基本统计描述的图形显示</p>
<h5 id="数值型数据：平均数"><a href="#数值型数据：平均数" class="headerlink" title="数值型数据：平均数"></a>数值型数据：平均数</h5><p>集中趋势的最常用测度值，易受极端值的影响，根据总体数据计算的，称为平均数，记为$\mu$，根据样本数据计算的，称为样本平均数，记为$\bar x$</p>
<p>简单平均数，加权平均数</p>
<h6 id="平均数的数学性质"><a href="#平均数的数学性质" class="headerlink" title="平均数的数学性质"></a>平均数的数学性质</h6><ul>
<li>各变量值与平均值的离差之和等于零：$\sum_{i=1}^n(x_i-\bar{x}=0)$</li>
<li>各变量值与平均值的离差平方和最小：$\sum_{i=1}^n(x_i-\bar{x})^2=min$</li>
</ul>
<h6 id="几何平均数"><a href="#几何平均数" class="headerlink" title="几何平均数"></a>几何平均数</h6><p>n个变量值乘积的n次方根，适用于对比率数据的平均，<strong>主要用于计算平均增长率</strong>，计算公式为：</p>
<p>$G_m=\sqrt[n]{x_1 \times x_2 \times…\times x_n}=\sqrt[n]{\prod_{i=1}^n(x_i)}$</p>
<p>可以看作是平均数的一种变形</p>
<p>$\lg(G_m)=\frac{1}{n}(\lg{x_1}+\lg{x_2}+…+\lg{x_n})=\frac {\sum_{i=1}^n\lg{x_i}} {n}$</p>
<p>众数、平均数和中位数的关系</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574743466822.png" alt="1574743466822"></p>
<p>均值往哪里偏就是什么偏分布</p>
<h4 id="离中趋势-离散趋势"><a href="#离中趋势-离散趋势" class="headerlink" title="离中趋势/离散趋势"></a>离中趋势/离散趋势</h4><p>离中趋势的各测度值是对数据离散程度所做的描述，反应各变量值原理其中心值的程度，因此也称为离中趋势。从另一个侧面说明了集中趋势测度值的代表程度，不同类型的数据有不同的离散程度测度值</p>
<h4 id="离中-离散程度的度量"><a href="#离中-离散程度的度量" class="headerlink" title="离中/离散程度的度量"></a>离中/离散程度的度量</h4><h5 id="定类数据：异众比率"><a href="#定类数据：异众比率" class="headerlink" title="定类数据：异众比率"></a>定类数据：异众比率</h5><p>非众数组的频数占总频数的比例，<strong>用于衡量众数的代表性</strong></p>
<p>$v_r=\frac{\sum f_i-f_m}{\sum f_i} = 1 - \frac{f_m}{\sum f_i}$</p>
<h5 id="定序数据：四分位差"><a href="#定序数据：四分位差" class="headerlink" title="定序数据：四分位差"></a>定序数据：四分位差</h5><p>也称为内聚或四分位距，上四分位数与下四分位数$Q_d=D_U-Q_L$，反映了中间50%数据的离散程度，不受极端值的影响，<strong>用于衡量中位数的代表性</strong></p>
<h5 id="数值型数据：极差（range）"><a href="#数值型数据：极差（range）" class="headerlink" title="数值型数据：极差（range）"></a>数值型数据：极差（range）</h5><p>一组数据的最大值与最小值之差，离散程度的最简单测度值，易受极端值影响，<strong>未考虑数据的分布</strong>，$R=max(x_i)-min(x_i)$</p>
<h5 id="数据数值型数据：平均差（mean-deviation）"><a href="#数据数值型数据：平均差（mean-deviation）" class="headerlink" title="数据数值型数据：平均差（mean deviation）"></a>数据数值型数据：平均差（mean deviation）</h5><p>各变量值与其平均数离差绝对值的平均数，能全面反映一组数据的离散程度，数学性质较差，实际中应用较少</p>
<p>未分组数据 $M_d=\frac{\sum_{i=1}^{n} |x_i-\bar{x}|}{n}$</p>
<p>组距分组数据 $M_d=\frac{\sum_{i=1}^k|M_i-\bar{x}|f_i}{n}$</p>
<h5 id="方差和标准差（variance-and-standard-deviation）"><a href="#方差和标准差（variance-and-standard-deviation）" class="headerlink" title="方差和标准差（variance and standard deviation）"></a>方差和标准差（variance and standard deviation）</h5><p>数据离散程度的最常用测度值，反映了各变量值与均值的平均差异</p>
<p>方差:各变量值与其平均数离差平方的平均数</p>
<p>标准差：方差的平方根（总体方差/标准差（根据总体数据计算的） 或者 样本方差/标准差（根据样本数据计算的））</p>
<h5 id="相对位置的度量：标准分数（standard-score）"><a href="#相对位置的度量：标准分数（standard-score）" class="headerlink" title="相对位置的度量：标准分数（standard score）"></a>相对位置的度量：标准分数（standard score）</h5><p>也称为标准化值，<strong>对某一个值在一组数据中相对位置的度量，也用于判断一组数据中是否有离群点</strong>，用于对标量的标准化处理</p>
<p>$x_i=\frac{x_i-\bar{x}}{S}$</p>
<h6 id="标准分数的性质"><a href="#标准分数的性质" class="headerlink" title="标准分数的性质"></a>标准分数的性质</h6><ul>
<li><p>均值等于0 $\bar{z}=\frac{\sum z_i}{n} = \frac{1}{n} \frac{\sum(x_i-\bar{x})}{S}=\frac{1}{n}\cdot \frac{0}{S}=0$</p>
</li>
<li><p>方差等于1</p>
<p>$s^2=\frac{\sum(z_i-\bar{z})^2}{n}=\frac{\sum(z_i-0)^2}{n}=\frac{z^2}{n}=\frac{1}{n} \cdot \frac{\sum(x_I-\bar{x})^2}{S^2}=\frac{s^2}{s^2} = 1$</p>
</li>
</ul>
<p>标准分数只是将原始数据进行了线性变换，它并没有改变一个数据在该组中的位置，也没有改变该组数据分布的形状，而只是将该组数据变为均值为0，方差为1</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574745476109.png" alt="1574745476109"></p>
<h5 id="经验法则"><a href="#经验法则" class="headerlink" title="经验法则"></a>经验法则</h5><p>当一组数据<strong>对称</strong>分布时：</p>
<ul>
<li>约有68%的数据在平均数加减1个标准差的范围之内</li>
<li>约有95%的数据在平均数加减2个标准差的范围之内</li>
<li>约有99%的数据在平均数加减3个标准差的范围之内</li>
</ul>
<p>在三个标准差之外的数据，称为异常值或离群点</p>
<h5 id="切比雪夫不等式"><a href="#切比雪夫不等式" class="headerlink" title="切比雪夫不等式"></a>切比雪夫不等式</h5><p>对于<strong>任意分布形态的数据</strong>，切比雪夫不等式指出：至少有$1-\frac{1}{k^2}$的数据落在$k$个标准差之内</p>
<ul>
<li>至少有75%的数据落在平均数加减2个标准差的范围之内</li>
<li>至少有89%的数据落在平均数加减3个标准差的范围之内</li>
<li>至少有94%的数据落在平均数加减4个标准差的范围之内</li>
</ul>
<h5 id="相对离散程度：离散系数（coeddicient-of-variation）比较时用"><a href="#相对离散程度：离散系数（coeddicient-of-variation）比较时用" class="headerlink" title="相对离散程度：离散系数（coeddicient of variation）比较时用"></a>相对离散程度：离散系数（coeddicient of variation）比较时用</h5><p>标准差与其相应的均值之比，对数据相对离散程度的测度，消除了数据水平高低和计量单位的影响，用于对不同组别数据离散程度的比较</p>
<p>$x_s=\frac{S}{\bar{x}}$</p>
<h3 id="数据的整理与显示"><a href="#数据的整理与显示" class="headerlink" title="数据的整理与显示"></a>数据的整理与显示</h3><h4 id="定类数据的整理与显示"><a href="#定类数据的整理与显示" class="headerlink" title="定类数据的整理与显示"></a>定类数据的整理与显示</h4><h5 id="定类数据的整理"><a href="#定类数据的整理" class="headerlink" title="定类数据的整理"></a>定类数据的整理</h5><p>基本过程：</p>
<ul>
<li>列出各类别</li>
<li>计算各类别的频数</li>
<li>制作频数分布表</li>
<li>用图形显示数据</li>
</ul>
<p>主要指标：</p>
<ul>
<li>频数：落在各类别中的数据个数</li>
<li>比例：某一类别数据占全部数据的比值</li>
<li>比率：不同类别数值的比值</li>
<li>百分比：将对比的基数作为100而计算的比值</li>
</ul>
<h5 id="定类数据的显示——条形图"><a href="#定类数据的显示——条形图" class="headerlink" title="定类数据的显示——条形图"></a>定类数据的显示——条形图</h5><p>用条形图高度来表示个类别数据的频数或频率</p>
<p>绘制时，各类别可以放在纵轴，称为条形图，也可以放在横轴，称为柱形图</p>
<p>条形图：</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574746192581.png" alt="1574746192581"></p>
<p>柱形图：</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574746204733.png" alt="1574746204733"></p>
<p>对比柱形图：分类变量在不同时间或不同空间上有多个取值，对比分类变量的取值在不同时间或不同空间上的差异或变化趋势</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574746228384.png" alt="1574746228384"></p>
<p>Pareto图：按各类别数据出现的频数多少排序后绘制的柱形图</p>
<p>圆形图/饼图：主要用于表示总体中各组成部分所占的比例，对于研究结构性问题十分有用</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574746354305.png" alt="1574746354305"></p>
<h4 id="定序数据的整理与显示"><a href="#定序数据的整理与显示" class="headerlink" title="定序数据的整理与显示"></a>定序数据的整理与显示</h4><h5 id="定序数据的整理"><a href="#定序数据的整理" class="headerlink" title="定序数据的整理"></a>定序数据的整理</h5><p>主要指标：</p>
<p>累计频数：将给类别的频数逐级累加</p>
<p>累计频率：将各类别的频率（百分比）逐级累加</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574746475405.png" alt="1574746475405"></p>
<h5 id="定序数据的显示——累计频数分布图"><a href="#定序数据的显示——累计频数分布图" class="headerlink" title="定序数据的显示——累计频数分布图"></a>定序数据的显示——累计频数分布图</h5><p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574746517201.png" alt="1574746517201"></p>
<p>环形图：可以同时绘制多个总体的数据系列，每一个总体的数据系列为一个环，可以用于进行比较研究，可用于展示定类和定序的数据（圆形图只能显示一个总体各部分所占的比例）</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574746605998.png" alt="1574746605998"></p>
<h4 id="数值型数据的整理与显示"><a href="#数值型数据的整理与显示" class="headerlink" title="数值型数据的整理与显示"></a>数值型数据的整理与显示</h4><h5 id="数值型数据的整理"><a href="#数值型数据的整理" class="headerlink" title="数值型数据的整理"></a>数值型数据的整理</h5><p>将原始数据按照某种标准分成不同的组别，称为数据分组</p>
<p>数据分组的方法：</p>
<ul>
<li>单变量值分组：把每一个变量值作为一组</li>
<li>组距分组：将全部变量值一次划分为若干个区间，并将这一区间的变量值作为一组<ul>
<li>分类：等距分组、异距分组</li>
<li>特点：将变量值的一个区间作为一组，适合于连续变量，适合于变量值较多的情况，必须遵守“不重不漏”的原则</li>
<li>步骤：<ul>
<li>确定组数，可以按照Sturges提出的经验公式来确定组数K $K=1+ \frac{\lg{(n)}} {\lg(2)}$</li>
<li>确定各组的组距，可根据全部数据的最大值好最小值及所分的组数来确定，即 组距=（最大值$-$最小值）/ 组数</li>
<li>根据分组整理成频数分布表</li>
</ul>
</li>
<li>组中值：下限与上限之间的中点值，即$组中值=\frac{上限值+下限值}{2}$</li>
</ul>
</li>
</ul>
<h5 id="等距分组与异距分组"><a href="#等距分组与异距分组" class="headerlink" title="等距分组与异距分组"></a>等距分组与异距分组</h5><p>等距分组：可以直接根据绝对频数来观察频数分布的特征和规律</p>
<p>异距分组：需要用频数密度（频数密度=频数 / 组距）反映频数分布的实际情况</p>
<h4 id="数值型数据的显示"><a href="#数值型数据的显示" class="headerlink" title="数值型数据的显示"></a>数值型数据的显示</h4><h5 id="分组数据的显示——直方图"><a href="#分组数据的显示——直方图" class="headerlink" title="分组数据的显示——直方图"></a>分组数据的显示——直方图</h5><p>实际上是用矩形的“面积”来表示各组的频数分布，在直角坐标中，用横轴表示数据分组（宽度表示类别，是固定的），纵轴表示频数或频率（长度表示各类别的频数的多少），各组与相应的频数就形成了一个矩形，即直方图，<strong>直方图下的面积之和等于1</strong></p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574747351035.png" alt="1574747351035"></p>
<p>直方图的各矩形通常是连续排列，条形图则是分开排列</p>
<h5 id="分组数据的显示——折线图"><a href="#分组数据的显示——折线图" class="headerlink" title="分组数据的显示——折线图"></a>分组数据的显示——折线图</h5><p>折线图也称为频数多边形图，折线图的两个终点要与横轴相交，具体做法是：</p>
<ul>
<li>第一个矩形顶部中点通过竖边中点（即该组频数一半的位置）连接到横轴，最后一个矩形顶部中点与其竖边中点连接到横轴</li>
<li>折线图下所围成的面积与直方图的面积相等，二者所表示的频数分布是一致的</li>
</ul>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574748670352.png" alt="1574748670352"></p>
<h5 id="原始数据的显示——茎叶图"><a href="#原始数据的显示——茎叶图" class="headerlink" title="原始数据的显示——茎叶图"></a>原始数据的显示——茎叶图</h5><p>用于显示未分组的原始数据的分布</p>
<p>由“茎”和“叶”两部分构成，其图形是由数字组成的</p>
<p>以该组数据的高位数值作树茎，低位数字作树叶，对于$n(20 \leq n \neq 300)$个数据，茎叶图最大行数不超过$L=[10 \times 10\log_{10}n]$</p>
<p>类似于直方图，但是直方图只能大体上看出一组数据的分布状况，但没有给出具体的数值</p>
<p>茎叶图既能给出数据的分布状况，又能给出每一个原始数据，保留了原始数据的信息</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574748949760.png" alt="1574748949760"></p>
<h5 id="原始数据的显示——箱线图"><a href="#原始数据的显示——箱线图" class="headerlink" title="原始数据的显示——箱线图"></a>原始数据的显示——箱线图</h5><p>箱线图由一组数据的5个特征值绘制而成，它由一个箱子和两条线段组成</p>
<p>绘制方法：</p>
<ul>
<li>首先找出一组数据的5个特征值，即最大值、最小是、中位数Me和两个四分位数</li>
<li>连接两个四分位数画出箱子，再将两个极值点与箱子相连接</li>
</ul>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574749080067.png" alt="1574749080067"></p>
<h5 id="时间序列数据的显示——线图"><a href="#时间序列数据的显示——线图" class="headerlink" title="时间序列数据的显示——线图"></a>时间序列数据的显示——线图</h5><p>线图是在平面坐标上用折线表现数据变化特征的图形，时间一般绘在横轴，指标数据绘在纵轴</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574749176117.png" alt="1574749176117"></p>
<h5 id="多变量的数据表示——雷达图"><a href="#多变量的数据表示——雷达图" class="headerlink" title="多变量的数据表示——雷达图"></a>多变量的数据表示——雷达图</h5><p>可用于研究多个样本之间的相似程度</p>
<p>设有n组样本$S_1, S_2, …, S_n$，每个样本测得p个变量$X_1, X_2, …, X_p$，要绘制这P个变量的雷达图，具体做法是：</p>
<ul>
<li>先做一个圆，然后将圆p等分，得到p个点，令这p个点分别对应p个变量，再将这p个点与圆心连线，得到p个辐射状的半径，这p个半径分别作为p个变量的坐标轴，每个变量值的大小由半径上的点到圆心的距离表示</li>
<li>再将统一样本的值在p个坐标上的点连线。这样，n个样本形成的n个多边形就是一个雷达图</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574749418707.png" alt="1574749418707"></p>
<h4 id="集中常见的频数分布类型"><a href="#集中常见的频数分布类型" class="headerlink" title="集中常见的频数分布类型"></a>集中常见的频数分布类型</h4><p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574749433670.png" alt="1574749433670"></p>
<h3 id="度量数据的相似性和相异性"><a href="#度量数据的相似性和相异性" class="headerlink" title="度量数据的相似性和相异性"></a>度量数据的相似性和相异性</h3><p>邻近性（Proximity）是用来表示相似性（Similarity）和相异性（Dissimilarity）的</p>
<h4 id="简单属性的相似度-相异度"><a href="#简单属性的相似度-相异度" class="headerlink" title="简单属性的相似度/相异度"></a>简单属性的相似度/相异度</h4><p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574749568150.png" alt="1574749568150"></p>
<h4 id="数据对象的相异度：欧式距离"><a href="#数据对象的相异度：欧式距离" class="headerlink" title="数据对象的相异度：欧式距离"></a>数据对象的相异度：欧式距离</h4><p>欧式距离：$d(x, y)=\sqrt{\sum_{k=1}^n(x_k-y_k)^2}$</p>
<p>n是维数，而$x_k$和$y_k$分别是x和y的第k个属性（分量）</p>
<h4 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a>闵可夫斯基距离</h4><p>Minkowski距离是欧式距离的推广：$d(x, y)=(\sum_{k=1}^n|x_k-y_k|^r)^{1/r}$</p>
<p>其中r是参数：</p>
<ul>
<li><p>r=1，城市街区（也称曼哈顿，出租车，L1范数）距离，他是两个具有二维属性的对象（即两个二元向量之间不同的二进制位个数）</p>
</li>
<li><p>r=2，欧几里得距离</p>
</li>
<li><p>$r\rightarrow \infty$，上确界距离，这是对象属性之间的最大距离，更正式地，最大距离由下列公式定义：</p>
<p>$d(x, y)=\underset{r\rightarrow \infty}\lim(\sum_{k=1}^n|x_k-y_k|^r)^{1/r}$</p>
</li>
</ul>
<p>距离的性质：非负性、对称性、三角不等式（$d(x, z) \leq d(x, y)+d(y, z)$）</p>
<h4 id="非度量的相异度"><a href="#非度量的相异度" class="headerlink" title="非度量的相异度"></a>非度量的相异度</h4><p>有些相异度都不满足一个或多个度量性质，例如集合差、时间</p>
<h4 id="数据对象之间的相似度"><a href="#数据对象之间的相似度" class="headerlink" title="数据对象之间的相似度"></a>数据对象之间的相似度</h4><p>设$s(x, y)$是数据点x和y之间的相似度</p>
<p>通常，$0 \leq s(x, y) \leq 1$, $s(x, y)=1, \text{if x=y}$</p>
<p>三角不等式或类似的性质通常不成立</p>
<p>有时，可以将相似度变换成一种度量距离，例如，余弦相似度量，Jaccard相似性度量</p>
<h4 id="简单匹配系数-Jaccard系数"><a href="#简单匹配系数-Jaccard系数" class="headerlink" title="简单匹配系数/Jaccard系数"></a>简单匹配系数/Jaccard系数</h4><p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574750444469.png" alt="1574750444469"></p>
<h4 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h4><p>设x和y是两个向量，则$cos(x,y)=\frac{x \cdot y}{||x||||y||}$</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574750535637.png" alt="1574750535637"></p>
<p>几何解释：$\cos(x, y)=\frac{x}{||x||} \cdot \frac{y}{||y||}=x’ \cdot y’$</p>
<p>其中$x’$ 和$y’$是长度为1的单位向量</p>
<h4 id="广义Jaccard系数"><a href="#广义Jaccard系数" class="headerlink" title="广义Jaccard系数"></a>广义Jaccard系数</h4><p>广义Jaccard系数——Tanimoto系数：$EJ(x, y)=\frac{x \cdot y}{||x||^2+||y||^2-x \cdot y}$</p>
<h4 id="相关性的度量"><a href="#相关性的度量" class="headerlink" title="相关性的度量"></a>相关性的度量</h4><p>对象之间的相关性是对象属性之间线性联系的度量</p>
<p>设x和y是两个向量，标准差$s_x=\sqrt{\frac{1}{n-1} \sum_{k=1}^n (x_k-\bar{x})^2}$， $s_y=\sqrt{\frac{1}{n-1} \sum_{k=1}^n (y_k-\bar{y})^2}$</p>
<p>协方差： $s_xy=\frac{1}{n-1} \sum_{k=1}^n (x_k-\bar{x})(y_k-\bar{y})$</p>
<p>皮尔森相关系数（Pearson’s correlation）$corr(x, y)=\frac{s_xy}{s_xs_y}$</p>
<p>$-1 \leq corr(x, y) \leq 1, corr(x, y)=0$不相关，$corr(x, y)=1(-1)$正（负）相关</p>
<h3 id="邻近度计算问题"><a href="#邻近度计算问题" class="headerlink" title="邻近度计算问题"></a>邻近度计算问题</h3><h4 id="距离度量的标准化和相关性"><a href="#距离度量的标准化和相关性" class="headerlink" title="距离度量的标准化和相关性"></a>距离度量的标准化和相关性</h4><ul>
<li>属性具有不同的值域<ul>
<li>问题：距离可能被具有较大值域的属性左右</li>
<li>处理：变换到相同值域</li>
</ul>
</li>
<li>某些属性之间相关<ul>
<li>使用Mahalanobis距离， $mahalanobis(x, y)=(x-y)\sum^{-1}(x-y)^T$, $\sum^{-1}$是数据协方差矩阵的逆</li>
</ul>
</li>
</ul>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574751272508.png" alt="1574751272508"></p>
<h4 id="组合异种属性的相似度"><a href="#组合异种属性的相似度" class="headerlink" title="组合异种属性的相似度"></a>组合异种属性的相似度</h4><h5 id="异种对象的相似度"><a href="#异种对象的相似度" class="headerlink" title="异种对象的相似度"></a>异种对象的相似度</h5><ul>
<li><p>对于第k个属性，计算相似度$s_k(x, y)$，在区间[0, 1]中</p>
</li>
<li><p>对于第k个属性，定义一个指示变量$\delta_k$，如下</p>
<ul>
<li>$\delta_k=0$，如果第k个属性是非对称属性，并且两个对象在该属性上的值都是0，或者如果一个对象的第k个属性具有遗漏值</li>
<li>$\delta_k=1$，否则</li>
</ul>
</li>
<li><p>使用如下公式计算两个对象的总相似度：</p>
<p>$similarity(x, y)=\frac{\sum_{k=1}^n \delta_k s_k(x, y)}{\sum)_{k=1}^n \delta_k}$</p>
</li>
</ul>
<p>加权的相似度：$similariy(p, q)=\frac{\sum_{k=1}^n w_k\delta_ks_k}{\sum_{k=1}^n \delta_k}$</p>
<p>加权的闵可夫斯基距离：$distance(p, q)=(\sum_{k=1}^n w_k|p_k-q_k|^r)^{1/r}$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/11/25/机器学习概述/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/25/机器学习概述/" itemprop="url">机器学习概述</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-25T20:52:32+08:00">
                2019-11-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="机器学习概述"><a href="#机器学习概述" class="headerlink" title="机器学习概述"></a>机器学习概述</h2><p>写这几篇的原因，是因为该复习了……</p>
<p>没错，不然就落下太多了…每次课上一提问就满脑子空白</p>
<h3 id="1-机器学习"><a href="#1-机器学习" class="headerlink" title="1. 机器学习"></a>1. 机器学习</h3><p>机器学习的种类：</p>
<p>根据处理的数据种类不同，可以分为监督学习、无监督学习和强化学习</p>
<h4 id="1-1-机器学习的概念和种类"><a href="#1-1-机器学习的概念和种类" class="headerlink" title="1.1 机器学习的概念和种类"></a>1.1 机器学习的概念和种类</h4><h5 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h5><p>指计算机算法从监督者（周围的环境）获取知识、信息，并有监督者提供对错指示、告知最终答案的学习过程（训练集、测试集）</p>
<p>泛华能力：根据在学习过程中所获得的经验、技能，对没有学习过的问题也能做出正确解答，使计算机获得某种泛华能力是监督学习的最终目标</p>
<p>应用：预测数值型数据的回归、预测分类标签的分类等</p>
<h5 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h5><p>指计算机在没有监督者的情况下，自学获取知识、信息</p>
<p>无监督学习的应用：不仅仅局限于解决像监督学习那样的有明确答案的问题，因而学习目标不必十分明确</p>
<p>在人造卫星故障诊断，视频分析、社交网站分析和声音信号分析等方面应用十分广泛</p>
<h5 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h5><p>是指在没有监督者提示的情况下，计算机算法对自己预测的结果进行评价的方法</p>
<p>与监督学习的关系：</p>
<p>与监督学习类似，也是使计算机获得对没有学习过的问题作出正确加大的泛化能力为目标，但是在学习过程中，不设置老师提示对错，告知最终答案的环节，然而，如果真的在学习过程中不能从周围环境中获得任何信息的话，强化学习就变成无监督学习了</p>
<h4 id="1-2-机器学习的例子"><a href="#1-2-机器学习的例子" class="headerlink" title="1.2 机器学习的例子"></a>1.2 机器学习的例子</h4><h5 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h5><p>是对一个或多的自变量和因变量之间的关系进行建模、求解的一种统计方法；即把实函数在样本点附近加以近似的有监督的函数近似方法</p>
<p>以d为实向量x作为输入，实数y作为输出的函数y=f(x)的学习问题</p>
<p>在监督学习力，这里的真实函数关系f是未知的，作为训练集的输入输出样本${(x_i, y_i)}^n_{i=1}$是已知的</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574687336324.png" alt="1574687336324"></p>
<h5 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h5><p>指对于指定的模式进行识别的有监督的模型识别问题</p>
<p>以d次方的实数向量x作为输入样本，而所有的输入样本，可以被划分为c个类别的问题来进行说明。作为训练集的输入输出样本${(xi,<br>yi)}^n_{i=1}$是已知的。但是分类问题中的输出样本$y_i$并不是具体的实数，而是分别代表类别1, 2, …, c</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574687716951.png" alt="1574687716951"></p>
<h5 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h5><p>与分类问题相同，也是模式识别的问题，但是属于无监督学习的一种</p>
<p>即只给出输入样本${x_i}^n_{i=1}$，然后判断各个样本分别属于1, 2, …, c中的哪个簇。隶属于相同簇的样本之间具有相似的性质，不同簇的样本之间具有不同的性质。在聚类问题中，如何准确的计算样本之间的相似度是很重要的课题</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574687725551.png" alt="1574687725551"></p>
<h5 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h5><p>指从高纬度数据中提取关键信息，将其转换为易于计算的低纬度问题进而求解的方法</p>
<p>当输入样本${x_i}^n_{i=1}$的维数d非常大的时候，可以把样本转换为较低维度的样本${x_i}^n_{i=1}$降维，根据数据种类的不同，可以分为监督学习和无监督学习两种</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574687881922.png" alt="1574687881922"></p>
<h5 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h5><p>是指寻找输入样本${x_i}^n_{i=1}$中所包含的异常数据的问题</p>
<p>在已知正常数据与异常数据的例子的情况下，其与有监督的分类问题是相同的。但是一般情况下，在异常检测任务重，对于什么样的数据是正常的，在事先是位置的。在这样的无监督的异常检测问题中，一般采用密度估计的方法。把靠近密度中心的数据作为正常的数据，把偏离密度中心的数据作为异常的数据</p>
<h4 id="1-3-机器学习的方法"><a href="#1-3-机器学习的方法" class="headerlink" title="1.3 机器学习的方法"></a>1.3 机器学习的方法</h4><p>不同的流派：</p>
<p>产生式分类、判别式分类、频率派、贝叶斯派</p>
<h5 id="1-3-1-生成的分类和识别的分类"><a href="#1-3-1-生成的分类和识别的分类" class="headerlink" title="1.3.1 生成的分类和识别的分类"></a>1.3.1 生成的分类和识别的分类</h5><h6 id="判别式的分类"><a href="#判别式的分类" class="headerlink" title="判别式的分类"></a>判别式的分类</h6><p>在已知模式x的时候，如果能求得使分类类别y的条件概率$p(y|x)$达到最大值的类别y的话，就可以进行模式识别了</p>
<p>$\hat{y}=\underset{y}{\operatorname{argmax}} p(y | x)$</p>
<p>“argmax”是取得最大值时的参数的意思。所以$\underset{y}\max p(y | x)$是指当y取特定值时$p(y|x)$的最大值，而$\hat{y}=\underset{y}{\operatorname{argmax}} p(y | x)$是指当$p(y|x)$取最大值时对应的y的值。在模式识别里，条件概率$p(y | x)$通常也称为后验概率</p>
<h6 id="生成式的分类"><a href="#生成式的分类" class="headerlink" title="生成式的分类"></a>生成式的分类</h6><p>在模式识别里，联合概率$p(x, y)$也称为数据生成概率，通过预测数据生成概率$p(x, y)$来进行模式识别的分类方法，称为生成的分类</p>
<p>$p(y | \boldsymbol{x})=\frac{p(\boldsymbol{x}, y)}{p(\boldsymbol{x})} \propto p(\boldsymbol{x}, y)$</p>
<p>通过上式，我们可以发现模式$x$和类别$y$的联合概率$p(x, y)$与后验概率$p(y|x)$是成比例的。正因为有这样的关系，我们可以通过使联合概率$p(x, y)$达到最大值的方法，来得到使后验概率$p(y|x)$达到最大值的类别$\hat y$</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574688708591.png" alt="1574688708591"></p>
<h6 id="判别式VS生成式"><a href="#判别式VS生成式" class="headerlink" title="判别式VS生成式"></a>判别式VS生成式</h6><p>在实际问题中，信息往往是有限的。在解决一个感兴趣的问题中，<strong>不要把解决一个更一般的问题作为一个中间步骤</strong>。要试图得到所需要的答案，而不是更一般的答案。很可能你拥有足够的信息来很好的解决一个感兴趣的特定问题，但却没有足够的信息来解决一个一般性的问题</p>
<p>$p(y | \boldsymbol{x})=\frac{p(\boldsymbol{x}, y)}{p(\boldsymbol{x})}=\frac{p(\boldsymbol{x}, y)}{\sum_{y} p(\boldsymbol{x}, y)}$</p>
<p>即使，手头的信息量不足以解决一般性问题，但对于解决特定问题，很可能信息是足够的</p>
<p>如果数据生成概率$p(x, y)$是一致的，那么，从上面的式子，就可以推出后验概率$p(y|x)$。然而，如果后验概率已知，却不能由此推导出数据生成概率$P(x, y)$</p>
<h5 id="1-3-2-统计概率与朴素贝叶斯"><a href="#1-3-2-统计概率与朴素贝叶斯" class="headerlink" title="1.3.2 统计概率与朴素贝叶斯"></a>1.3.2 统计概率与朴素贝叶斯</h5><h6 id="统计概率方案"><a href="#统计概率方案" class="headerlink" title="统计概率方案"></a>统计概率方案</h6><p>在包含参数$\theta$的模型$(x, y)$为例，将模式$\theta$作为决定论的变量，使用手头的训练样本$D={x_i, y_i}^n_{i=1}$对模式$\theta$进行学习。开入没在最大似然估计算法中，一般对生成训练集D的最容易的方法所对应的模式$\theta$进行学习</p>
<p>$\max <em>{\boldsymbol{\theta}} \prod</em>{i=1}^{y} q\left(\boldsymbol{x}<em>{i}, y</em>{i} ; \boldsymbol{\theta}\right)$</p>
<p>在统计概率方法中，如何由训练集D得到高精度的模式$\theta$是主要的研究课题</p>
<h6 id="朴素贝叶斯方法"><a href="#朴素贝叶斯方法" class="headerlink" title="朴素贝叶斯方法"></a>朴素贝叶斯方法</h6><p>以包含参数$\theta$的模型$q(x, y)$为例，将模式$\theta$作为概率变量，对其先验概率$p(\theta)$加以考虑，计算与训练集D相对应的后验概率$p(\theta|D)$。通过运用贝叶斯定理，就可以使用先验概率$p(\theta)$来求解后验概率$p(\theta|D)$</p>
<p>$p(\boldsymbol{\theta} | \mathcal{D})=\frac{p(\mathcal{D} | \boldsymbol{\theta}) p(\boldsymbol{\theta})}{p(\mathcal{D})}=\frac{\prod_{i=1}^{n} q\left(\boldsymbol{x}<em>{i}, y</em>{i} | \boldsymbol{\theta}\right) p(\boldsymbol{\theta})}{\int \prod_{i=1}^{n} q\left(\boldsymbol{x}<em>{i}, y</em>{i} | \boldsymbol{\theta}\right) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}}$</p>
<p>在朴素贝叶斯算法中，如何精确的计算后验概率是一个主要的研究课题</p>
<h3 id="2-学习模型——如何使特定函数与数据集相近似"><a href="#2-学习模型——如何使特定函数与数据集相近似" class="headerlink" title="2. 学习模型——如何使特定函数与数据集相近似"></a>2. 学习模型——如何使特定函数与数据集相近似</h3><p>主要学习模型：</p>
<p>线性模型——线性回归模型</p>
<p>核模型——高斯核函数</p>
<p>层次模型——逻辑回归</p>
<h4 id="2-1-线性模型"><a href="#2-1-线性模型" class="headerlink" title="2.1 线性模型"></a>2.1 线性模型</h4><p>$f_\theta(x)= \underset{j=1} \sum\theta_j\phi_j(x)=\theta^T\phi(x)$</p>
<p>在上式中，$\phi_j(x)$是基函数向量$\phi(x)=(\phi_1(x), …, \phi_b(x))^T$的第$j$个因子，$\theta_j$是参数向量$\theta=(\theta_1, …, \theta_b)^T$的第$j$个因子。另外，b是基函数的个数，T表示转置，我们可以看到，虽然上式依然是基于参数向量$\theta$的线性形式，但是，如果把基函数变为多项式的形式</p>
<p>$\phi(x)=\left(1, x, x^{2}, \cdots, x^{b-1}\right)^{\top}$</p>
<p>或者变成$b=2m+1$的三角多项式形式：</p>
<p>$\phi(x)=(1, \sin x, \cos x, \sin 2 x, \cos 2 x, \cdots, \sin m x, \cos m x)^{\top}$</p>
<p>上述的线性模型就可以表示复杂的非线性模型了</p>
<p>在上述模型中，一维的输入x还可以扩展为d维的向量形式</p>
<p>$\boldsymbol{x}=\left(x^{(1)}, \cdots, x^{(d)}\right)^{\top}$</p>
<p><strong>乘法模型</strong>：把一维的基函数作为因子，通过使其<strong>相乘</strong>而获得多维基函数的方法</p>
<p>$f_{\boldsymbol{\theta}}(\boldsymbol{x})=\sum_{j_{1}=1}^{b^{\prime}} \cdots \sum_{j_{d}=1}^{b^{\prime}} \theta_{j_{1}, \cdots, j_{d}} \phi_{j_{1}}\left(x^{(1)}\right) \cdots \phi_{j_{d}}\left(x^{(d)}\right)$</p>
<p><strong>加法模型</strong>：把一维的基函数作为因子，通过使其<strong>相加</strong>而获得多维基函数的方法</p>
<p>$f_{\theta}(x)=\sum_{k=1}^{d} \sum_{j=1}^{b^{\prime}} \theta_{k, j} \phi_{j}\left(x^{(k)}\right)$</p>
<p>乘法模型 VS 加法模型</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574694623618.png" alt="1574694623618"></p>
<p>乘法模型的<strong>表现力丰富</strong>，但是<strong>参数的个数会随着输入维数d呈指数增长</strong>。另一方面，虽然加法模型的参数个数是随着输入维数d呈线性增长，但其表现力又相对较弱</p>
<h5 id="线性模型——回归模型"><a href="#线性模型——回归模型" class="headerlink" title="线性模型——回归模型"></a>线性模型——回归模型</h5><p>回归：属于监督学习中的一种方法。该方法的核心思想是从连续型统计数据中得到数学模型，然后将该数学模型用于预测或者分类</p>
<p>回归方法能够解决特征多维，结果是一维多离散值或一维连续值的问题</p>
<h6 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h6><p>首先给出一个输入数据，算法通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574732837157.png" alt="1574732837157"></p>
<h6 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h6><p>线性回归是利用称为<strong>线性回归方程</strong>的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析</p>
<p>线性回归属于<strong>监督学习</strong>，因此方法和监督学习是一样的，先给定一个训练集，根据这个训练集学习出一个<strong>线性函数</strong>，然后测试这个函数训练的好不好（即此函数是否足够拟合训练集数据），挑选出最好的函数即可</p>
<h6 id="如何评判函数拟合的好不好？"><a href="#如何评判函数拟合的好不好？" class="headerlink" title="如何评判函数拟合的好不好？"></a>如何评判函数拟合的好不好？</h6><p><strong>代价函数</strong>：对假设的函数进行评价，cost function越小的函数，说明拟合训练数据拟合的越好</p>
<h6 id="代价函数与参数的关系"><a href="#代价函数与参数的关系" class="headerlink" title="代价函数与参数的关系"></a>代价函数与参数的关系</h6><p>$J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574733110185.png" alt="1574733110185"></p>
<p>注意：如果是线性回归，则$cost function J$与$\theta$与$\theta_1$的函数一定是碗状的，即只有一个最小点</p>
<p>一般情况：</p>
<p> $x^{(i)} \in R^n$         $h_{\theta}(x)=\theta_{0}+\theta_{0} x_{1}+\cdots \theta_{\mathrm{n}} x_{n}=\boldsymbol{\theta} X$</p>
<p>$\begin{array}{l}{J(\theta)=\frac{1}{2} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}} \ {\min <em>{\theta} J</em>{\theta}}\end{array}$</p>
<p>最小二乘损失函数</p>
<h6 id="求解方法"><a href="#求解方法" class="headerlink" title="求解方法"></a>求解方法</h6><ul>
<li><p>最小二乘法：$\theta=\left(X^{T} X\right)^{-1} X^{T} \vec{y}$</p>
<p>是一个直接的数学求解公式，不过它要求X是列满秩的</p>
</li>
<li><p>梯度下降法</p>
<ul>
<li><p>先确定向下一步的步伐大小，我们称为learning rate</p>
</li>
<li><p>任意给定一个初始值：$\theta_0, \theta_1$</p>
</li>
<li><p>确定一个向下的方向，并向下走预先规定的步伐，并更新$\theta_0, \theta_1$</p>
</li>
<li><p>当下降的高度小于某个定义的值，则停止下降</p>
</li>
</ul>
</li>
</ul>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574733439432.png" alt="1574733439432"></p>
<h6 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h6><ul>
<li>初始点不同，获得的最小值可能也不同，因此梯度下降求得的只是局部最小值</li>
<li>越接近最小值时，下降速度越慢</li>
</ul>
<h6 id="如何取-alpha-值？"><a href="#如何取-alpha-值？" class="headerlink" title="如何取$\alpha$值？"></a>如何取$\alpha$值？</h6><p>随时观察$\alpha$值，如果cost function变小了，则OK，反之，再取一个更小的值</p>
<p>注意：下降的步伐代销非常重要，因为如果太小，则得到函数最小值的速度就很慢，如果太大，则可能会出现overshoot the minimum的现象</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574733668371.png" alt="1574733668371"></p>
<p>如果Learning rate取值后，发现J function增长了，则需要减小Learning rate的值</p>
<h4 id="2-2-核模型"><a href="#2-2-核模型" class="headerlink" title="2.2 核模型"></a>2.2 核模型</h4><p>在线性模型中，多项式或者三角多项式等基函数与训练样本${(x_1, y_i)}^n_{j=1}$是毫不相关的。而核模型在进行基函数的设计时就要用到输入样本${x_i}^n_{j=1}$</p>
<p>核模型：以使用被称为核函数的二元函数$K(, …)$，以$\boldsymbol{K}(\boldsymbol{x}, \boldsymbol{x})^{\mathrm{n}}_{j=1}$的线性结合方式加以定义</p>
<p>$f_{\theta}(x)=\underset{j=1}\sum(\theta_jK(x, x_j))$</p>
<p>在众多的核函数中，以高斯核函数的使用，最为广泛</p>
<p>$K(x, c) = exp(- \frac{||x-c||^2} {2h^2})$</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574734055109.png" alt="1574734055109"></p>
<p>$||x||$表示2范数，即$||x||=\sqrt {x^Tx}$。h和c分别对应于高斯核函数的带宽与均值</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574734188553.png" alt="1574734188553"></p>
<p>只在训练集的输入样本附近对函数进行近似，可以减轻维数灾难的影响：</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574734267528.png" alt="1574734267528"></p>
<h4 id="2-3-层次模型"><a href="#2-3-层次模型" class="headerlink" title="2.3 层次模型"></a>2.3 层次模型</h4><p>与参数相关的非线性模型，称之为非线性模型</p>
<p>层次模型是常用的非线性模型，在神经网络中应用广泛</p>
<p>（逻辑回归也是一种层次模型）</p>
<h5 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h5><p><strong>sigmoid函数</strong>，又称<strong>逻辑回归函数</strong>。但是它本质上又是一个线性回归模型，因为除去sigmoid映射函数关系，其他的步骤、算法都是线性回归的，但是sigmoid可以轻松处理0/1分类问题</p>
<h6 id="Logistic函数"><a href="#Logistic函数" class="headerlink" title="Logistic函数"></a>Logistic函数</h6><p>如果我们忽略二分类问题中y的取值是一个离散的取值（0或1），我们继续使用线性回归来预测y的取值。这样做会导致y的取值并不为0或1。逻辑回归使用一个函数来归一 化y值，使y的取值在区间(0,1)内，这个函数称为Logistic函数(logistic function)，也称为Sigmoid函数(sigmoid function)。函数公式如下：</p>
<p>$g(z)=\frac {1} {1+e^{-z}}$</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574734548274.png" alt="1574734548274"></p>
<h6 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h6><p>$g’(z) = g(z)(1-g(z))$</p>
<h5 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h5><p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574734653196.png" alt="1574734653196"></p>
<h6 id="线性决策边界"><a href="#线性决策边界" class="headerlink" title="线性决策边界"></a>线性决策边界</h6><p>对于线性决策边界的情况，边界形式如下:</p>
<p>$\theta_0+\theta_1x_1+,…+\theta_{n} x_{n}=  \sum_{i=1}^n\theta_ix_i=\theta^Tx$</p>
<p>构造预测函数为：$h_\theta(x)=g(\theta^Tx)=\frac{1} {1+e^{-\theta^T}}$</p>
<p>函数$h_\theta(x)$的值有着特殊的含义，它表示结果取1个概率，因此对于输入x分类结果为类别1的概率和类别为0的概率分别为</p>
<p>$P(y=1|x;\theta)=h_\theta(x)$</p>
<p>$P(y=0|x;\theta)=1-h_\theta(x)$</p>
<p><strong>构造损失函数</strong></p>
<p>它们是基于最大似然估计推导得到的：</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574735271784.png" alt="1574735271784"></p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574735278632.png" alt="1574735278632"></p>
<h5 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化(Regularization)"></a>正则化(Regularization)</h5><h6 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h6><p>对于线性回归或逻辑回归的损失函数构成的模型，可能会有些权重很大，有些权重很小，导致过拟合（就是过分拟合了训练数据），使得模型的复杂度提高，泛 化能力较差（对未知数据的预测能力）。</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574735342490.png" alt="1574735342490"></p>
<p>问题的主因：过拟合问题往往源自过多的特征</p>
<p>解决方法：</p>
<ul>
<li><p>减少特征数量（减少特征会失去一些信息，即使特征选的很好）</p>
</li>
<li><p>可用人工选择要保留的数据</p>
</li>
<li><p>模型选择算法</p>
<ul>
<li>正则化（特征较多时比较有用）</li>
<li>保留所有特征，但减少$\theta$的大小</li>
</ul>
</li>
</ul>
<h6 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a>正则化方法</h6><p>正则化是结构风险最小化策略的实现，是在经验风险上增加一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大</p>
<p>例如，在房价预测问题上，多项式回归：</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574735634696.png" alt="1574735634696"></p>
<p>直观上来看，如果我们想解决这个例子中的过拟合问题，最好能将$x^3$和$x^4$的影响消除，也就是让$\theta_3 \approx 0, \theta_4 \approx 0$，假设我们对$\theta_3, \theta_4$进行惩罚，并且令其很小，一个简单的办法就是给原有的Cost函数加上两个略大的惩罚项</p>
<p>$\underset{\theta} min \frac{1}{2m} \sum_{i=1}^n (h_{\theta}(x_i)-y_i)^2+1000\theta_3^2+1000\theta_4^2$</p>
<p>这样在最小化cost函数的时候，$\theta_3 \approx 0, \theta_4 \approx 0$</p>
<p>正则项可以取不同的形式，在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数。取平方损失时，模型的损失函数变为</p>
<p>$J(\theta) = \frac{1}{2m} \sum_{i=1}^n(h_\theta(x_i)-y_i)^2+\lambda\sum_{j=1}^n\theta_j^2$</p>
<p>如果$\lambda$是正则项系数：</p>
<ul>
<li>如果它的值很大，说明对模型的复杂惩罚大，对拟合数据的损失惩罚小，这样它就不会过拟合数据，在训练数据上的偏差越大，在未知数据上的方差越小，但是可能出现欠拟合的现象</li>
<li>如果它的值很大，说明比较注重对训练数据的拟合，在训练数据上的偏差小，但是可能会导致过拟合</li>
</ul>
<p>有监督回归</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/11/25/PCA原理详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/25/PCA原理详解/" itemprop="url">PCA原理详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-25T17:09:29+08:00">
                2019-11-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="PCA原理详解"><a href="#PCA原理详解" class="headerlink" title="PCA原理详解"></a>PCA原理详解</h2><p>虽然之前看过PCA，但说实话不是特别理解，今天在人工智能的课堂上再次学了一下，感觉还是有点收获的。就随便写写吧</p>
<h3 id="What-is-PCA？"><a href="#What-is-PCA？" class="headerlink" title="What is PCA？"></a>What is PCA？</h3><p>PCA（Principle Component Analysis）是一种常用的<strong>数据分析方法</strong></p>
<p>PCA通过线性变换<strong>将原始数据变换为一组各维度线性无关的表示</strong>，可用于<strong>提取数据的主要特征分量</strong>，常用于高维数据的<strong>降维</strong></p>
<h3 id="数据的向量表示"><a href="#数据的向量表示" class="headerlink" title="数据的向量表示"></a>数据的向量表示</h3><p>一般情况下，在数据挖掘和机器学习中，数据被表示为向量</p>
<p>例如：某宝2012年全年的流量以及交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，列向量格式如下：</p>
<p>$（浏览量，访客数，下单数，成交数，成交金额）^T$</p>
<p>$(500, 240, 25, 13, 2312.15)^T$</p>
<h3 id="降维问题：目的和原则"><a href="#降维问题：目的和原则" class="headerlink" title="降维问题：目的和原则"></a>降维问题：目的和原则</h3><p>机器学习算法的<strong>复杂度通常与数据的维数有着密切关系</strong>，甚至与维数呈指数级关联</p>
<p>机器学习在实际中处理成千上万甚至几十万维的情况也不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维</p>
<p><strong>降维当然意味着信息的丢失</strong>，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低</p>
<h3 id="降维的数学支撑"><a href="#降维的数学支撑" class="headerlink" title="降维的数学支撑"></a>降维的数学支撑</h3><h4 id="向量的表示和基变换：内积和投影"><a href="#向量的表示和基变换：内积和投影" class="headerlink" title="向量的表示和基变换：内积和投影"></a>向量的表示和基变换：内积和投影</h4><p>两个维数相同的向量的内积被定义为：</p>
<p>$(a_1, a_2, …, a_n) * (b_1, b_2,…, b_n)^T = a_1 b_1 + 2_2 b_2 +… + a_n b_n$</p>
<p>内积运算将两个向量映射为一个实数</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574673944019.png" alt="1574673944019"></p>
<p>设A和B是两个n维向量，n维向量可以等价表示为n维空间中的一条从原点发射的有向线段：</p>
<p>$A=(x_1, y_1)$   $B=(x_2, y_2)$</p>
<p>从A点向B所在直线上引一条垂线。垂线与B的交点叫做A在B上的投影，设A与B的夹角为$\alpha$，则投影的矢量长度为$|A|cos(\alpha)$</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574674147016.png" alt="1574674147016"></p>
<p>其中$|A|=\sqrt{x_1^2+y_1^2}$是向量$A$的模，也就是$A$线段的标量长度</p>
<p>可以将内积表示成另外一种我们熟悉的形式</p>
<p>$A \cdot B = |A||B|cos(\alpha)$</p>
<p>A与B的内积等于A到B的投影长度乘以B的模</p>
<p>设B的模为1，即让$|B| = 1$,那么就变成了$A \cdot B = |A|cos(\alpha)$</p>
<p>即A与B的内积等于A向B所在直线投影的矢量长度</p>
<h4 id="基的数学含义"><a href="#基的数学含义" class="headerlink" title="基的数学含义"></a>基的数学含义</h4><p>要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574674438265.png" alt="1574674438265"></p>
<p>例如，$(1, 1)$和$(-1, 1)$也可以成为一组基，标准化之后变成$(\frac 1 {\sqrt{2}}, \frac 1 {\sqrt{2}})$和$(-\frac 1 {\sqrt{2}}, \frac 1 {\sqrt{2}})$</p>
<p>$(3, 2)$在新基上的坐标为$(\frac 5 {\sqrt{2}}, -\frac 1 {\sqrt{2}})$</p>
<h4 id="基变换的矩阵表示"><a href="#基变换的矩阵表示" class="headerlink" title="基变换的矩阵表示"></a>基变换的矩阵表示</h4><p>$(3, 2)$的基变换（基是：$\left(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right) 和\left(-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right)$）</p>
<p>$\left(\begin{array}{cc}{1 / \sqrt{2}} &amp; {1 / \sqrt{2}} \ {-1 / \sqrt{2}} &amp; {1 / \sqrt{2}}\end{array}\right)\left(\begin{array}{l}{3} \ {2}\end{array}\right)=\left(\begin{array}{c}{5 / \sqrt{2}} \ {-1 / \sqrt{2}}\end{array}\right)$</p>
<p>例如$(1,1),(2,2),(3,3)$的基变换：</p>
<p>$\left(\begin{array}{cc}{1 / \sqrt{2}} &amp; {1 / \sqrt{2}} \ {-1 / \sqrt{2}} &amp; {1 / \sqrt{2}}\end{array}\right)\left(\begin{array}{ccc}{1} &amp; {2} &amp; {3} \ {1} &amp; {2} &amp; {3}\end{array}\right)=\left(\begin{array}{ccc}{2 / \sqrt{2}} &amp; {4 / \sqrt{2}} &amp; {6 / \sqrt{2}} \ {0} &amp; {0} &amp; {0}\end{array}\right)$</p>
<p>注意，这里的每一组向量都需要以<strong>列向量</strong>的形式来表示</p>
<p>一般的，如果有M个N维向量，想将其变换为由R个N维向量表示的新空间。那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果</p>
<p>数学表示为：</p>
<p>$\left(\begin{array}{c}{p_{1}} \ {p_{2}} \ {\vdots} \ {p_{R}}\end{array}\right)\left(\begin{array}{cccc}{a_{1}} &amp; {a_{2}} &amp; {\cdots} &amp; {a_{M}}\end{array}\right)=\left(\begin{array}{cccc}{p_{1} a_{1}} &amp; {p_{1} a_{2}} &amp; {\cdots} &amp; {p_{1} a_{M}} \ {p_{2} a_{1}} &amp; {p_{2} a_{2}} &amp; {\cdots} &amp; {p_{2} a_{M}} \ {\vdots} &amp; {\vdots} &amp; {\ddots} &amp; {\vdots} \ {p_{R} a_{1}} &amp; {p_{R} a_{2}} &amp; {\cdots} &amp; {p_{R} a_{M}}\end{array}\right)$</p>
<p>其中$p_i$是一个行向量，表示第$i$个基，$a_j$是一个列向量，表示第$j$个原始数据记录</p>
<h4 id="协方差矩阵及优化目标"><a href="#协方差矩阵及优化目标" class="headerlink" title="协方差矩阵及优化目标"></a>协方差矩阵及优化目标</h4><p>如果我们有一组N维向量，现在要将其降到K维（K小于N），应该如何选择K个基才能最大程度保留原有信息呢？</p>
<p>假设数据由五条记录构成，将他们表示成矩阵形式：<br>$\left(\begin{array}{lllll}{1} &amp; {1} &amp; {2} &amp; {4} &amp; {2} \ {1} &amp; {3} &amp; {3} &amp; {4} &amp; {4}\end{array}\right)$</p>
<p>其中每一列为一条数据，而一行为一个字段，为了后续处理方便，首先<strong>将每个字段的所有值都减去字段均值</strong>（后面会解释原因）</p>
<p>结果如下：</p>
<p>$\left(\begin{array}{ccccc}{-1} &amp; {-1} &amp; {0} &amp; {2} &amp; {0} \ {-2} &amp; {0} &amp; {0} &amp; {1} &amp; {1}\end{array}\right)$</p>
<p>图上显示为：</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574675289268.png" alt="1574675289268"></p>
<p>我们希望使用一维来表示这些数据，但是又希望尽量保留原始数据，要如何选择呢？</p>
<p>通过基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。 </p>
<p>如果向X轴投影，则最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身五个各不相同的点投影之后只剩下两个不同的值了，这是一种严重的信息丢失。投影到Y轴也会发生先这样的问题，所以，单独选择X轴和Y轴都不是最好的投影选择</p>
<h4 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h4><p>目标：投影后投影值尽可能分散，而这种程度的分散，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即</p>
<p>$\operatorname{Var}(a)=\frac{1}{m} \sum_{i=1}^{m}\left(a_{i}-\mu\right)^{2}$</p>
<p>由于上面已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示：</p>
<p>$\operatorname{Var}(a)=\frac{1}{m} \sum_{i=1}^{m} a_{i}^{2}$</p>
<p>于是上面的问题被形式化的表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大</p>
<h4 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h4><p>目的：从直观上说，让两个字段尽可能表示更多的原始信息，不希望他们之间存在线性（相关性）的，因为相关性意味着这两个字段不是完全独立，必然存在重复表示的信息</p>
<p>数学上可以用两个字段的协方差来表示其相关性，由于已经让每个字段均值为0，则：</p>
<p>$\operatorname{Cov}(a, b)=\frac{1}{m} \sum_{i=1}^{m} a_{i} b_{i}$</p>
<p>可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m</p>
<p>当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时，只能与在第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的</p>
<p>至此，我们得到了降维问题的优化目标：将一组N为向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两辆间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）</p>
<h4 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h4><p>目的：降维的指标与字段内方差及字段间协方差有密切关系。因此，可将两者统一表示</p>
<p>$X=\left(\begin{array}{llll}{a_{1}} &amp; {a_{2}} &amp; {\cdots} &amp; {a_{m}} \ {b_{1}} &amp; {b_{2}} &amp; {\cdots} &amp; {b_{m}}\end{array}\right)$</p>
<p>用X乘以X的转置，并乘上系数1/m</p>
<p>$\frac{1}{m} X X^{\top}=\left(\begin{array}{cc}{\frac{1}{m} \sum_{i=1}^{m} a_{i}^{2}} &amp; {\frac{1}{m} \sum_{i=1}^{m} a_{i} b_{i}} \ {\frac{1}{m} \sum_{i=1}^{m} a_{i} b_{i}} &amp; {\frac{1}{m} \sum_{i=1}^{m} b_{i}^{2}}\end{array}\right)$</p>
<ul>
<li>矩阵对角线上的两个元素分别是两个字段的方差，而其他元素是a和b的协方差</li>
<li>两者被统一到了一个矩阵</li>
</ul>
<h4 id="协方差矩阵推广"><a href="#协方差矩阵推广" class="headerlink" title="协方差矩阵推广"></a>协方差矩阵推广</h4><p>我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设$C=\frac 1 m X X^T$，则C是一个对称矩阵，其对角线分别为各个字段的方差</p>
<p>而第i行j列的元素和第j行i列的元素相同，表示i和j两个字段的协方差</p>
<p>$\frac{1}{m} X X^{\top}=\left(\begin{array}{cc}{\frac{1}{m} \sum_{i=1}^{m} a_{i}^{2}} &amp; {\frac{1}{m} \sum_{i=1}^{m} a_{i} b_{i}} \ {\frac{1}{m} \sum_{i=1}^{m} a_{i} b_{i}} &amp; {\frac{1}{m} \sum_{i=1}^{m} b_{i}^{2}}\end{array}\right)$</p>
<h4 id="协方差矩阵对角化"><a href="#协方差矩阵对角化" class="headerlink" title="协方差矩阵对角化"></a>协方差矩阵对角化</h4><p>目标：将协方差矩阵对角化，即除对角线外的其他元素化为0，并且在对角线上将元素按大小从上到下排列</p>
<p>原矩阵与基变换后矩阵协方差矩阵的关系：</p>
<p>设原始数据矩阵$X$对应的协方差矩阵为$C$，而$P$是一组基按行组成的矩阵，设$Y=PX$，则$Y$为$X$对$P$做基变换后的数据。设$Y$的协方差矩阵为$D$，我们推导一下$D$与$C$的关系：</p>
<p>$\begin{aligned} D &amp;=\frac{1}{m} Y Y^{\top} \ &amp;=\frac{1}{m}(P X)(P X)^{\top} \ &amp;=\frac{1}{m} P X X^{\top} P^{\top} \ &amp;=P\left(\frac{1}{m} X X^{\top}\right) P^{\top} \ &amp;=P C P^{\top} \end{aligned}$</p>
<p>$P$能让原始的协方差矩阵对角化的$P$。换句话说，优化目标变成了寻找一个矩阵$P$，满足$PCP^T$是一个对角矩阵，并且对角元素按大到小一次排列，那么$P$的前$K$行就是要寻找的基，用$P$的前$K$行组成的矩阵乘以$X$就是的$X$从$N$维降到了$K$维并满足上述优化条件</p>
<p>由上文知道，协方差矩阵C是一个对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：</p>
<ul>
<li><p>实对称矩阵不同特征值对应的特征向量必然正交</p>
</li>
<li><p>设特征向量$\lambda$重数为$r$，则必然存在r个线性无关的特征向量对应于$\lambda$，因此可以将这r个特征向量单位正交化。由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为e1, e2, …, en，将其按列组成矩阵：</p>
<p>$E=(e_1, e_2, …, e_n)$</p>
<p>则对协方差矩阵C有如下结论：</p>
<p>$E^{\top} C E=\Lambda=\left(\begin{array}{cccc}{\lambda_{1}} &amp; {} &amp; {} &amp; {} \ {} &amp; {\lambda_{2}} &amp; {} &amp; {} \ {} &amp; {} &amp; {\ddots} &amp; {} \ {} &amp; {} &amp; {} &amp; {\lambda_{n}}\end{array}\right)$</p>
</li>
</ul>
<p>其中$\Lambda$为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）</p>
<p>到这里，我们已经找到了需要的矩阵P：</p>
<p>$P=E^T$</p>
<p>P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照$\Lambda$中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y</p>
<h3 id="PCA算法"><a href="#PCA算法" class="headerlink" title="PCA算法"></a>PCA算法</h3><p>设有m条n维数据</p>
<ul>
<li><p>将原始数据按列组成n行m列矩阵X</p>
</li>
<li><p>将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</p>
</li>
<li><p>求出协方差矩阵$C=\frac 1 m X X^T$</p>
</li>
<li><p>求出协方差矩阵的特征值以及对应的特征向量</p>
</li>
<li><p>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</p>
</li>
<li><p>$Y=PX$即为降维到K为之后的数据</p>
</li>
</ul>
<h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>以上文提到的</p>
<p>$\left(\begin{array}{ccccc}{-1} &amp; {-1} &amp; {0} &amp; {2} &amp; {0} \ {-2} &amp; {0} &amp; {0} &amp; {1} &amp; {1}\end{array}\right)$</p>
<p>为例，我们用PCA方法将这组二维数据降到一维</p>
<p>因为每行已经是零均值，所以直接求协方差矩阵</p>
<p>$C=\frac{1}{5}\left(\begin{array}{ccccc}{-1} &amp; {-1} &amp; {0} &amp; {2} &amp; {0} \ {-2} &amp; {0} &amp; {0} &amp; {1} &amp; {1}\end{array}\right)\left(\begin{array}{cc}{-1} &amp; {-2} \ {-1} &amp; {0} \ {0} &amp; {0} \ {2} &amp; {1} \ {0} &amp; {1}\end{array}\right)=\left(\begin{array}{cc}{\frac{6}{5}} &amp; {\frac{4}{5}} \ {\frac{4}{5}} &amp; {\frac{6}{5}}\end{array}\right)$</p>
<p>然后求其特征值和特征向量</p>
<p>$\lambda_{1}=2, \lambda_{2}=2 / 5$</p>
<p>其对应的特征向量分别是：</p>
<p>$c_{1}\left(\begin{array}{c}{1} \ {1}\end{array}\right), c_{2}\left(\begin{array}{c}{-1} \ {1}\end{array}\right)$</p>
<p>其中对应的特征向量分别是一个通解，$c_1$和$c_2$可取任意实数，那么标准化后的特征向量为：</p>
<p>$\left(\begin{array}{c}{1 / \sqrt{2}} \ {1 / \sqrt{2}}\end{array}\right),\left(\begin{array}{c}{-1 / \sqrt{2}} \ {1 / \sqrt{2}}\end{array}\right)$</p>
<p>矩阵P为：</p>
<p>$P=\left(\begin{array}{cc}{1 / \sqrt{2}} &amp; {1 / \sqrt{2}} \ {-1 / \sqrt{2}} &amp; {1 / \sqrt{2}}\end{array}\right)$</p>
<p>可以验证协方差矩阵C的对角化：</p>
<p>$P C P^{\top}=\left(\begin{array}{cc}{1 / \sqrt{2}} &amp; {1 / \sqrt{2}} \ {-1 / \sqrt{2}} &amp; {1 / \sqrt{2}}\end{array}\right)\left(\begin{array}{cc}{6 / 5} &amp; {4 / 5} \ {4 / 5} &amp; {6 / 5}\end{array}\right)\left(\begin{array}{cc}{1 / \sqrt{2}} &amp; {-1 / \sqrt{2}} \ {1 / \sqrt{2}} &amp; {1 / \sqrt{2}}\end{array}\right)=\left(\begin{array}{cc}{2} &amp; {0} \ {0} &amp; {2 / 5}\end{array}\right)$</p>
<p>最后我们用P的第一行乘以数据矩阵，就得到的降维后的表示：</p>
<p>$Y=\left(\begin{array}{llllll}{-1} &amp; {-1} &amp; {0} &amp; {2} &amp; {0} \ {-2} &amp; {0} &amp; {0} &amp; {1} &amp; {1}\end{array}\right)=\left(\begin{array}{cccc}{-3 / \sqrt{2}} &amp; {-1 / \sqrt{2}} &amp; {0} &amp; {3 / \sqrt{2}} &amp; {-1 / \sqrt{2}}\end{array}\right)$</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574686204617.png" alt="1574686204617"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/11/24/损失函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/24/损失函数/" itemprop="url">损失函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-24T13:36:54+08:00">
                2019-11-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="平方误差损失"><a href="#平方误差损失" class="headerlink" title="平方误差损失"></a>平方误差损失</h3><p>每个训练样本的平方误差损失（也称为L2 Loss）是实际值和预测值之差的平方</p>
<p><img src="https://pic2.zhimg.com/v2-776284ef8ef1892adca72454c80723dd_b.jpg" alt="img"></p>
<p>相应的成本函数是这些平方误差的平均值（MSE），它是一个二次函数（形式为ax^2+bx+c），并且值大于等于0,。二次函数具有全局最小值，由于没有局部最小值，所以永远不会陷入它。因此，可以保证梯度下降将收敛到全局最小值（如果完全收敛的话）</p>
<p>MSE损失函数通过使用平方误差来惩罚模型，有一个缺点，把一个比较大的数的平方会使它变得更大。但有一点需要注意，这个属性使<strong>MSE成本函数对异常值的健壮性降低。因此，如果我们的数据容易出现许多的异常值，则不应该使用它</strong></p>
<h3 id="绝对误差损失"><a href="#绝对误差损失" class="headerlink" title="绝对误差损失"></a>绝对误差损失</h3><p>每个训练样本的绝对误差是预测值和实际值之间的距离，与符号无关。绝对误差也称为L1 Loss</p>
<p><img src="https://pic2.zhimg.com/v2-77f5689dd45999c25bb55a1008e2cdf1_b.jpg" alt="img"></p>
<p>成本是这些绝对误差的平均值（MAE）。与MSE相比，<strong>MAE成本对异常值更加健壮</strong>。但是，<strong>在数学方程中处理绝对或模数运算符并不容易</strong>。</p>
<h3 id="Huber损失"><a href="#Huber损失" class="headerlink" title="Huber损失"></a>Huber损失</h3><p>结合了MSE和MAE的最佳特性。对于较小的误差，他是二次的，否则是线性的（对于其梯度也是如此）。Huber损失需要确定$\delta$参数</p>
<p><img src="https://pic2.zhimg.com/v2-333ad2d90605b5d0de9ee4ec509ee49d_b.jpg" alt="img"></p>
<p>Huber损失对于异常值比MSE更强。<strong>它用于稳健回归，M估计法（M-estimator）和可加模型（additive model）。Huber损失的变体也可用于分类。</strong></p>
<h3 id="二分类损失函数"><a href="#二分类损失函数" class="headerlink" title="二分类损失函数"></a>二分类损失函数</h3><p>给分类基于应用于输入特征向量的规则则</p>
<h4 id="二元交叉熵损失"><a href="#二元交叉熵损失" class="headerlink" title="二元交叉熵损失"></a>二元交叉熵损失</h4><p>熵是用来表示无序性和不确定性。测量具有芥蓝菜分布$P(X)$的随机变量X</p>
<p><img src="https://pic4.zhimg.com/v2-dd86aad2ba3885b36dfc9db902baadc3_b.jpg" alt="img"></p>
<p>负号使用于最后结果为正数</p>
<blockquote>
<p>概率分布的熵值越大，表明分布的不确定性越大。同样，一个较小的值代表一个更确定的分布</p>
</blockquote>
<p>这使得二元交叉熵适合作为损失函数（你希望最小化其值）。我们对输出概率p的分类模型使用二元交叉熵损失</p>
<blockquote>
<p>元素属于第1类（或正类）的概率=p</p>
<p>元素属于第0类（或负类）的概率=1-p</p>
</blockquote>
<p>然后，输出标签y（可以取值0和1）的交叉熵损失和预测概率p定义为：</p>
<p><img src="https://pic2.zhimg.com/v2-47ddb7df995d3978f53bd2b877e4c25d_b.jpg" alt="img"></p>
<p>这也称为Log-Loss（对数损失）。为了计算概率p，可以使用sigmoid函数。其中，z是输入功能的函数</p>
<p><img src="https://pic1.zhimg.com/v2-d6a0a7f3bb804cd5c0dcaa2f1b908ee8_b.jpg" alt="img"></p>
<p>sigmoid函数的范围是[0, 1]这使得它适合于计算概率</p>
<p><img src="https://pic1.zhimg.com/v2-95dba4bbda5ed968482df244137cbcf4_b.jpg" alt="img"></p>
<h4 id="Hinge损失"><a href="#Hinge损失" class="headerlink" title="Hinge损失"></a>Hinge损失</h4><p><strong>Hinge损失主要用于带有类标签-1和1的支持向量机（SVM）</strong>。因此，请确保将数据集中“恶性”类标签从0更改为-1</p>
<blockquote>
<p>Hinge损失不仅会惩罚错误的预测，还会惩罚不自信的正确预测</p>
</blockquote>
<p>数据对<code>(x, y)</code>的Hinge损失如图：</p>
<p><img src="https://pic4.zhimg.com/v2-739534b9c71db800ca910f72a3152e13_b.jpg" alt="img"></p>
<p>Hinge损失简化了SVM的数学运算，同时最大化了损失（与对数损失（Log-Loss）相比）。当我们想要做实时决策而不是高度关注准确性时，就可以使用它</p>
<h3 id="多分类损失函数"><a href="#多分类损失函数" class="headerlink" title="多分类损失函数"></a>多分类损失函数</h3><h4 id="多分类交叉熵"><a href="#多分类交叉熵" class="headerlink" title="多分类交叉熵"></a>多分类交叉熵</h4><p>多分类交叉熵损失是二元交叉熵损失的退矿。输入向量$X_i$和相应的ont-hot编码目标向量$Y_i$的损失是：</p>
<p><img src="https://pic2.zhimg.com/v2-c465dd0ad165c0e3501d7d6608188c41_b.jpg" alt="img"></p>
<p>使用softmax函数来找到概率$P_{ij}$</p>
<p><img src="https://pic3.zhimg.com/v2-953d3605358f94ee5833084e75705756_b.jpg" alt="img"></p>
<p>softmax层是接在神经网络的输出层前。Softmax层必须与输出层具有相同数量的节点</p>
<p><img src="https://pic3.zhimg.com/v2-be2d1fea2669befa924891c3bd7c8e12_b.jpg" alt="img"></p>
<p>最后，我们的输出是具有给定输入的最大概率的类别</p>
<h4 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h4><p>KL散度是概率分布与另一个概率分布区别的度量。KL散度为零表示分布相同</p>
<p><img src="https://pic3.zhimg.com/v2-e78029297910deac90414234151aca9e_b.jpg" alt="img"></p>
<p>注意，发散函数不对称</p>
<p>与多分类分类相比，KL散度更常用于逼近复杂函数。我们在使用变分自动编码器（VAE）等深度生成模型时经常使用KL散度</p>
<h3 id="Pytorch实战"><a href="#Pytorch实战" class="headerlink" title="Pytorch实战"></a>Pytorch实战</h3><p>损失函数的基本用法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = LossCriterion()	# 构造函数有自己的参数</span><br><span class="line">loss = criterion(x, y)		# 调用标准时也有参数</span><br></pre></td></tr></table></figure>

<p>得到的loss结果已经对mini-batch数量取了平均值</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/11/24/DANN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/24/DANN/" itemprop="url">DANN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-24T12:37:33+08:00">
                2019-11-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="EANN"><a href="#EANN" class="headerlink" title="EANN"></a>EANN</h2><h3 id="Domain-Adaption"><a href="#Domain-Adaption" class="headerlink" title="Domain Adaption"></a>Domain Adaption</h3><p>主要目的：</p>
<p>想想一个分类任务，假设现在我们手中有源域数据和目标域数据。其中源域数据是丰富并且有标记的，而目标域数据是充足的但是没有标记的，但是源域和目标域的特征空间和标记空间相同。很显然，我们可以利用源域数据建立一个分类器，但是由于目标域数据本身没有标记，那么我们无法通过常规方法为目标域构建分类器，这时候就需要用到Domain Adaption了：</p>
<p><img src="https://pic2.zhimg.com/80/v2-0975878ddcedfaf101b10358317b8691_hd.jpg" alt="img"></p>
<p>Domain Adaptation的思想就是通过消除源域和目标域的分布差异，使得源域数据和目标域数据能同时被分开。</p>
<h3 id="DANN（Domain-Adaptation-Training-of-Neural-Networks）"><a href="#DANN（Domain-Adaptation-Training-of-Neural-Networks）" class="headerlink" title="DANN（Domain-Adaptation Training of Neural Networks）"></a>DANN（Domain-Adaptation Training of Neural Networks）</h3><p>DANN的应用背景：</p>
<p>源域数据充足并且有标记，目标域数据充足但是无标记，源域和目标域的特征空间和标记空间相同，任务是借助源域数据对目标域数据进行分类</p>
<p>DANN结构：</p>
<p><img src="https://pic2.zhimg.com/80/v2-4fc596dfbc7f7ff56b32df7d0f7c0c71_hd.jpg" alt="img"></p>
<p>网络分为三部分：</p>
<ul>
<li>Feature extractor（特征提取器）<ul>
<li>提取后续网络后续完成任务所需要的特征</li>
<li>将源域样本和目标域样本进行映射和混合</li>
</ul>
</li>
<li>Label extractoe（域分类器）<ul>
<li>利用Feature extractor提取的信息对样本进行分类</li>
</ul>
</li>
<li>Domain classifier（域分类器）<ul>
<li>判断feature extractor提取的信息来自源域还是目标域</li>
</ul>
</li>
</ul>
<p>特征提取器提取的信息会传入域分类器，之后域分类器会判断传入的信息到底是来自于源域还是目标域，并计算损失。在反向传播更新参数的过程中，域分类器和特征提取器中间有一个梯度翻转层，也就是说域分类器的训练目标是尽量将输入的信息分到正确的域类别（源域还是目标域），而特征提取与的训练目的恰恰相反（由于梯度翻转层的存在），特征提取器所提取的特征（或者说映射的结果）目的是使域判别器不能正确的判断出信息是来自哪一个域，因此形成一种对抗关系。课件，当域分类器不能将接受的信息正确分为源域样本还是目标域样本时，特征提取器的任务就圆满完成了，因为此时源域样本和目标域样本在某个空间中已经被混合在一起不能分开了。</p>
<p>但是，我们最终的目的是对目标域样本进行分类，那么我们如何保证特征提取器提取的信息是能够用来分类的呢？加入无论输入什么样本给特征提取器，它都输出一个单位向量，这样依旧可以”骗过“域分类器，但是却无法完成后续的分类工作。</p>
<p>这个时候就需要靠Label predictor（类别预测期），因为源域样本是有标记的，所以在提取特征时不仅仅要考虑后面的域判别器的情况，还要利用源域的带标记样本进行有监督训练从而兼顾分类的准确性。</p>
<p>综上，当把特征提取器、域分类器和类别预测器都训练完成后，就可以做到把源域和目标域混合在一起并进行分类了：</p>
<p><img src="https://pic1.zhimg.com/80/v2-fd13bf28230d5e4cacd26d2149a8dd30_hd.jpg" alt="img"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/11/22/选择合适的激活函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/22/选择合适的激活函数/" itemprop="url">选择合适的激活函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-22T14:44:37+08:00">
                2019-11-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="选择合适的激活函数"><a href="#选择合适的激活函数" class="headerlink" title="选择合适的激活函数"></a>选择合适的激活函数</h2><h3 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h3><h4 id="图像"><a href="#图像" class="headerlink" title="图像"></a>图像</h4><p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574406136071.png" alt="1574406136071"></p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><p>他会将输入的数字挤压到0到1区间的范围内。是最先使用的激活函数，以为可以将它解释为一个神经元的激活率，其中0表示没激活，1表示完全激活</p>
<h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><ul>
<li><p>会导致模型的梯度消失</p>
<ul>
<li><p>当神经元的激活状态接近于0或1时，这些区域的梯度会非常接近于0</p>
<p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574406306081.png" alt="1574406306081"></p>
<p>在反向传播期间，该局部梯度会和此处输出的梯度相乘以实现整体的目标。</p>
</li>
</ul>
</li>
<li><p>Sigmoid函数的输出并不是以0为基准，而是从0开始，到1结束。意味着函数的值会为正。因而权重的梯度会全部为正或全部为负，这会造成梯度往完全不同的方向更新，使得优化更困难</p>
</li>
</ul>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><h4 id="图像-1"><a href="#图像-1" class="headerlink" title="图像"></a>图像</h4><p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574406487079.png" alt="1574406487079"></p>
<h4 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h4><p>他会将数字压缩到-1到1区间内。所以他的输出以0为基准，使优化更加容易</p>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>梯度消失</p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><h4 id="图像-2"><a href="#图像-2" class="headerlink" title="图像"></a>图像</h4><p><img src="C:%5CUsers%5C12751%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1574406572298.png" alt="1574406572298"></p>
<h4 id="特点-2"><a href="#特点-2" class="headerlink" title="特点"></a>特点</h4><p>max（input， 0）</p>
<p>不会涉及大量运算，因此它能更快的学习，也避免了梯度消失的问题</p>
<p>但是他<strong>只用在隐藏层</strong>中，<strong>输出层应该用Softmax函数用于分类，因为它能生成不同类别的概率</strong>。在<strong>回归问题中使用线性函数</strong>，因为通过它的数据信号不会变</p>
<h4 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h4><p>部分神经元在训练中会很脆弱，甚至会死掉。这就意味着较大的梯度通过ReLU函数时会导致梯度更新，使得无法再次激活任何数据点</p>
<p>解决方法：Leaky ReLU</p>
<h3 id="经验"><a href="#经验" class="headerlink" title="经验"></a>经验</h3><ul>
<li>深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据（可以经过数据预处理实现）和zero-cintered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度</li>
<li>如果使用ReLU，那么一定要小心设置learning rate，而且注意不要让网络出现很多“dead”神经元，如果这个问题不好解决，可以试试Leaky-ReLU、PReLU或者Maxout</li>
<li>最好不要使用sigmoid，可以试试tanh，不过可以预期他的效果会比不上ReLU和Maxout</li>
</ul>
<h3 id="神经网络的输入为什么要0均值化"><a href="#神经网络的输入为什么要0均值化" class="headerlink" title="神经网络的输入为什么要0均值化"></a>神经网络的输入为什么要0均值化</h3><p>这样做的优点是为了在反向传播中加快网络中每一层权重参数的收敛。</p>
<p>为什么0均值化就会加快收敛呢？</p>
<p><img src="https://pic4.zhimg.com/v2-bc1465b4377a5c1f7762514093a43be7_b.jpg" alt="img"></p>
<p>在梯度下降的时候，假设途中蓝色箭头方向为理想最优向量，根据公式，可以想到：</p>
<p>当x全为正或者全为负时，每次返回的梯度都只会沿着一个方向发生变化，这样就回使得权重收敛效率很低</p>
<p>当正负数量“差不多”时，那么梯度的变化方向就会不确定，这样就能达到上图中的变化效果，加速了权重的收敛</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/11/22/AutoEncoder/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/22/AutoEncoder/" itemprop="url">AutoEncoder</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-22T14:25:00+08:00">
                2019-11-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="AutoEncoder"><a href="#AutoEncoder" class="headerlink" title="AutoEncoder"></a>AutoEncoder</h2><h3 id="压缩与解压"><a href="#压缩与解压" class="headerlink" title="压缩与解压"></a>压缩与解压</h3><p>有一个神经网络，他在做的事情是：接受一张图片，然后给他打码，最后再从打码的图片中还原。</p>
<p><img src="https://pic3.zhimg.com/v2-7370f2fd8e6dabe4b24b71118ebf3b52_b.png" alt="img"></p>
<p>假设刚刚的神经网络是这样，对应上刚刚的图片，可以看出，图片实际上是经过了压缩，再解压的这一道工序，原有的图片质量被缩减，解压时用信息量小却包含了所有关键信息的文件恢复出原本的图片。</p>
<p>为什么要这样做呢？</p>
<p><img src="https://pic2.zhimg.com/v2-948d5ede26f9e6c0e484640b92c1dcad_b.png" alt="img"></p>
<p>有的时候神经网路偶要接受大量的输入信息，比如输入信息是高清图片时。输入信息可能达到上千万，让神经网络直接从上千万个信息源中学习是一件很吃力的事情，所以，何不压缩一下，提取出原图片中的最具代表性的信息，缩减信息量，再把缩减过后的信息放进神经网络学习，这样学习起来就简单轻松很多了，所以，自编码就能在这时发挥作用。</p>
<p>通过将元数据白色的X压缩，解压成黑色的X，然后通过对比黑白，求出预测误差，进行反向传递，逐步提升自编码的准确性，训练好的自编码中间这一部分就是元数据的精髓，可以看出，从头到尾，我们只用到了输入数据X，并没有用到X对应的数据标签，所以也可以说自编码是一种非监督学习</p>
<p>到了真正使用自编码的时候，通常只会用到自编码的前半部分。</p>
<h3 id="编码器Encoder"><a href="#编码器Encoder" class="headerlink" title="编码器Encoder"></a>编码器Encoder</h3><p><img src="https://pic2.zhimg.com/v2-948d5ede26f9e6c0e484640b92c1dcad_b.png" alt="img"></p>
<p>编码器得到原数据的精髓，然后我们只需要再创建一个小的神经网络学习这个精髓的数据，不仅减少了神经网络的负担，而且同样能达到很好的效果</p>
<p><img src="https://pic1.zhimg.com/v2-9032ff2d01cdb7310245c7f397289550_b.png" alt="img"></p>
<p>这是通过一个自编码整理出来的数据，他能从原数据中总结出每种类型数据的特征，如果把这些特征全部放在一张二维的图片上，每种类型都已经被很好的用原数据的精髓区分开来。</p>
<p>PCA主成分分析，在提取主要特征时，自编码和PCA一样，甚至超越了PCA。换句话说自编码可以像PCA一样给特征属性降维。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/11/21/Python可视化数据/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/21/Python可视化数据/" itemprop="url">Python可视化数据</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-21T08:06:51+08:00">
                2019-11-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="绘制热力图"><a href="#绘制热力图" class="headerlink" title="绘制热力图"></a>绘制热力图</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/11/21/Scrapy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/21/Scrapy/" itemprop="url">Scrapy</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-21T07:07:29+08:00">
                2019-11-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h2><p>在安装完Scrapy<code>(pip install scrapy)</code>之后，创建一个scrapy项目</p>
<blockquote>
<p>scrapy startproject tutorial</p>
</blockquote>
<p>该Scrapy项目下都包含了以下内容</p>
<blockquote>
<p>|-tutorial        该项目的python模块。在此放入代码（核心）</p>
<p>|-scrapy.cfg    项目的配置文件</p>
<p>  |-<strong>init</strong>.py</p>
<p>  |-items.py    项目中的item文件（这是创建容器的地方，爬取的信息分别放到不同的容器里面）</p>
<p>  |-middlewares.py</p>
<p>  |-pipelines.py    项目中的设置文件（设置一些基础参数，比如加头文件，设置编码等等）</p>
<p>  |-settings.py</p>
<p>  |-spiders    放置spider的目录（放爬虫的地方）</p>
<p>​    |-<strong>init</strong>.py</p>
</blockquote>
<p>里面的parse方法，有两个作用</p>
<ul>
<li><p>负责解析start_url下载的Response对象，根据item提取数据（解析item数据的前提是parse里全部requests请求都被加入了爬取队列）</p>
</li>
<li><p>如果有新的url则加入爬取队列，负责进一步处理，URL的request对象</p>
</li>
</ul>
<p>启动爬虫</p>
<p>输入命令，启动爬虫</p>
<blockquote>
<p>scrapy crawl dmoz</p>
</blockquote>
<p>启动爬虫的时候发生了什么？</p>
<p>Scrapy为</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/11/14/Transaction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/14/Transaction/" itemprop="url">Transaction</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-14T16:29:16+08:00">
                2019-11-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Mariana</p>
              <p class="site-description motion-element" itemprop="description">a study blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">124</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mariana</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
