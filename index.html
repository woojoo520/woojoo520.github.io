<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="a study blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Celery Fairy">
<meta property="og:url" content="https://woojoo520.github.io/index.html">
<meta property="og:site_name" content="Celery Fairy">
<meta property="og:description" content="a study blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Celery Fairy">
<meta name="twitter:description" content="a study blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://woojoo520.github.io/">





  <title>Celery Fairy</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Celery Fairy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Celery's Blog</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/15/常见的loss函数-TV-Loss/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/15/常见的loss函数-TV-Loss/" itemprop="url">常见的loss函数-TV_Loss</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-15T10:56:54+08:00">
                2019-08-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="TV-Loss"><a href="#TV-Loss" class="headerlink" title="TV Loss"></a>TV Loss</h1><p>在图像复原过程中，图像上的一点点噪声可能就会对复原的结果产生非常大的影响，因为很过复原算法都会放大噪声。这时候我们就需要在最优化问题的模型中添加一些正则项来保持图像的光滑性，TV Loss是常用的一种正则项（<strong>注意是正则项，配合其他loss、</strong></p>
<p><strong>一起使用，约束噪声</strong>）。图片中相邻像素值的差异可以通过降低TV Loss来一定程度上解决。比如降噪，对抗checkboard等等。</p>
<h3 id="1-初始定义"><a href="#1-初始定义" class="headerlink" title="1.初始定义"></a>1.初始定义</h3><p>Rudin等人观察到，受噪声污染的图像的总变分比无噪图像的总变分明显的大。那么最小化TV理论上就可以最小化噪声。图片中相邻像素值的差异可以通过降低TV loss来一定程度上解决。</p>
<p>总编分定义为梯度幅值的积分：</p>
<p>$J_{T_{0}}(u)=\int_{\Omega_{u}}\left|\nabla_{u}\right| d x d y=\int_{D_{u}} \sqrt{u_{x}^{2}+u_{y}^{2}} d x d y$</p>
<p>其中$u_{x}=\frac{\partial u}{\partial x}$， $u_{y}=\frac{\partial u}{\partial y}$，$D_{u}$是图像的支持域。限制总变分就会限制噪声</p>
<h3 id="2-扩展定义"><a href="#2-扩展定义" class="headerlink" title="2.扩展定义"></a>2.扩展定义</h3><p>带阶数的TV loss定义如下：</p>
<p>$\Re_{V^{\beta}}(f)=\int_{\Omega}\left(\frac{\partial f}{\partial u}(u, v)^{2}+\frac{\partial f}{\partial v}(u, v)^{2}\right)^{\frac{\beta}{2}}$</p>
<p>但是在图像中，连续域的积分就变成了像素离散域中求和，所以可以这么算：<br>$\Re_{V^{\beta}}(x)=\sum_{i, j}\left(\left(x_{i, j-1}-x_{i, j}\right)^{2}+\left(x_{i+1, j}-x_{i, j}\right)^{2}\right)^{\frac{\beta}{2}}$</p>
<p>即：求每一个像素和横向下一个像素的差的平方，加上纵向下一个像素的差的平方。然后开β/2次根</p>
<h3 id="3-效果"><a href="#3-效果" class="headerlink" title="3.效果"></a>3.效果</h3><p>总变差（TV）损失促进了生成的图像中的空间平滑性，根据论文的描述，当β &lt; 1时，会出现下图左侧的小点点的artifact, 当β &gt; 1时，图像中的小点点会被消除，但是代价就是图像的清晰度</p>
<p><img src="https://img-blog.csdnimg.cn/20190311152735551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lleGlhb2d1MTEwNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/14/Pytorch-transforms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/14/Pytorch-transforms/" itemprop="url">Pytorch transforms</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-14T14:59:19+08:00">
                2019-08-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Transfroms"><a href="#Transfroms" class="headerlink" title="Transfroms"></a>Transfroms</h2><h3 id="1-裁剪——Crop"><a href="#1-裁剪——Crop" class="headerlink" title="1. 裁剪——Crop"></a>1. 裁剪——Crop</h3><h4 id="1-1-随机裁剪：transforms-RandomCrop"><a href="#1-1-随机裁剪：transforms-RandomCrop" class="headerlink" title="1.1 随机裁剪：transforms.RandomCrop()"></a>1.1 随机裁剪：transforms.RandomCrop()</h4><ul>
<li><p><code>torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=&#39;constant&#39;)</code></p>
<ul>
<li><p>功能：依据给定的size随机裁剪</p>
</li>
<li><p>参数：</p>
<ul>
<li><p><code>size-(sequence or int),若为sequence， 则为(h, w),若为int， 则(size, size)</code></p>
</li>
<li><p><code>padding-(sequence or int, optional)</code>,此参数是设置为多少个pixel，当为int时，图像上下左右均填充int个，例如<code>padding=4</code>，则上下左右均填充4个padding， 若为32 * 32，则会变成40 * 40；当为sequence时，若有两个数，则第一个数表示左右扩充多少，第二个数表示上下的，当有4个数是，则为左、上、右、下</p>
</li>
<li><p><code>fill-(int or tuple)</code>，填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值</p>
</li>
<li><p><code>padding-mode</code>,填充模式（constant（常量）， edge（按照图片边缘的像素值来填充），reflect， symmetric）</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="1-2-中心裁剪：transforms-CenterCrop"><a href="#1-2-中心裁剪：transforms-CenterCrop" class="headerlink" title="1.2 中心裁剪：transforms.CenterCrop()"></a>1.2 中心裁剪：transforms.CenterCrop()</h4><h4 id="1-3-随机长宽比裁剪-transforms-RandomResizedCrop"><a href="#1-3-随机长宽比裁剪-transforms-RandomResizedCrop" class="headerlink" title="1.3 随机长宽比裁剪 transforms.RandomResizedCrop()"></a>1.3 随机长宽比裁剪 transforms.RandomResizedCrop()</h4><h4 id="1-4-上下左右中心裁剪：transforms-FiveCrop"><a href="#1-4-上下左右中心裁剪：transforms-FiveCrop" class="headerlink" title="1.4 上下左右中心裁剪：transforms.FiveCrop()"></a>1.4 上下左右中心裁剪：transforms.FiveCrop()</h4><h4 id="1-5-上下左右中心裁剪后翻转：transform-TenCrop"><a href="#1-5-上下左右中心裁剪后翻转：transform-TenCrop" class="headerlink" title="1.5 上下左右中心裁剪后翻转：transform.TenCrop()"></a>1.5 上下左右中心裁剪后翻转：transform.TenCrop()</h4><h3 id="2-翻转和旋转——Flip-and-Rotation"><a href="#2-翻转和旋转——Flip-and-Rotation" class="headerlink" title="2. 翻转和旋转——Flip and Rotation"></a>2. 翻转和旋转——Flip and Rotation</h3><h4 id="2-1-依概率p水平翻转-transfroms-RandomHorizontalFlip"><a href="#2-1-依概率p水平翻转-transfroms-RandomHorizontalFlip" class="headerlink" title="2.1 依概率p水平翻转 transfroms. RandomHorizontalFlip()"></a>2.1 依概率p水平翻转 transfroms. RandomHorizontalFlip()</h4><h4 id="2-2-依概率p垂直翻转-transforms-RandomVerticalFlip"><a href="#2-2-依概率p垂直翻转-transforms-RandomVerticalFlip" class="headerlink" title="2.2 依概率p垂直翻转 transforms.RandomVerticalFlip()"></a>2.2 依概率p垂直翻转 transforms.RandomVerticalFlip()</h4><h4 id="2-3-随机旋转-transforms-RandomRotation"><a href="#2-3-随机旋转-transforms-RandomRotation" class="headerlink" title="2.3 随机旋转 transforms.RandomRotation()"></a>2.3 随机旋转 transforms.RandomRotation()</h4><h3 id="3-图像变换"><a href="#3-图像变换" class="headerlink" title="3. 图像变换"></a>3. 图像变换</h3><h4 id="3-1-resize-transform-Resize"><a href="#3-1-resize-transform-Resize" class="headerlink" title="3.1 resize  transform.Resize"></a>3.1 resize  transform.Resize</h4><h4 id="3-2-标准化-transform-Normalize"><a href="#3-2-标准化-transform-Normalize" class="headerlink" title="3.2 标准化 transform.Normalize"></a>3.2 标准化 transform.Normalize</h4><h4 id="3-3-转为tensor-transforms-ToTensor"><a href="#3-3-转为tensor-transforms-ToTensor" class="headerlink" title="3.3 转为tensor transforms.ToTensor"></a>3.3 转为tensor transforms.ToTensor</h4><h4 id="3-4-填充-transforms-Pad"><a href="#3-4-填充-transforms-Pad" class="headerlink" title="3.4 填充 transforms.Pad"></a>3.4 填充 transforms.Pad</h4><h4 id="3-5-修改亮度、对比度和饱和度-transforms-ColorJitter"><a href="#3-5-修改亮度、对比度和饱和度-transforms-ColorJitter" class="headerlink" title="3.5 修改亮度、对比度和饱和度 transforms.ColorJitter()"></a>3.5 修改亮度、对比度和饱和度 transforms.ColorJitter()</h4><h4 id="3-6-转灰度图-transforms-GrayScale"><a href="#3-6-转灰度图-transforms-GrayScale" class="headerlink" title="3.6 转灰度图 transforms.GrayScale"></a>3.6 转灰度图 transforms.GrayScale</h4><h4 id="3-7-线性变换-transforms-LinearTransformation"><a href="#3-7-线性变换-transforms-LinearTransformation" class="headerlink" title="3.7 线性变换 transforms.LinearTransformation()"></a>3.7 线性变换 transforms.LinearTransformation()</h4><h4 id="3-8-放射变换-transform-RandomAffine"><a href="#3-8-放射变换-transform-RandomAffine" class="headerlink" title="3.8 放射变换 transform.RandomAffine"></a>3.8 放射变换 transform.RandomAffine</h4><h4 id="3-9-依概率p转为灰度图-transforms-RandomGrayScale"><a href="#3-9-依概率p转为灰度图-transforms-RandomGrayScale" class="headerlink" title="3.9 依概率p转为灰度图 transforms.RandomGrayScale"></a>3.9 依概率p转为灰度图 transforms.RandomGrayScale</h4><h4 id="3-10-将数据转换为PILImage-transforms-ToPILImage"><a href="#3-10-将数据转换为PILImage-transforms-ToPILImage" class="headerlink" title="3.10 将数据转换为PILImage transforms.ToPILImage"></a>3.10 将数据转换为PILImage transforms.ToPILImage</h4><h4 id="3-11-transforms-Lambda"><a href="#3-11-transforms-Lambda" class="headerlink" title="3.11 transforms.Lambda"></a>3.11 transforms.Lambda</h4><h3 id="4-对transforms操作，使数据增强更灵活"><a href="#4-对transforms操作，使数据增强更灵活" class="headerlink" title="4. 对transforms操作，使数据增强更灵活"></a>4. 对transforms操作，使数据增强更灵活</h3><h4 id="4-1-transforms-RandomChoice-transfroms"><a href="#4-1-transforms-RandomChoice-transfroms" class="headerlink" title="4.1 transforms.RandomChoice(transfroms)"></a>4.1 transforms.RandomChoice(transfroms)</h4><h4 id="4-2-transforms-RandomApply-transforms-p-0-5"><a href="#4-2-transforms-RandomApply-transforms-p-0-5" class="headerlink" title="4.2 transforms.RandomApply(transforms, p=0.5)"></a>4.2 transforms.RandomApply(transforms, p=0.5)</h4><h4 id="4-3-transforms-RandomOrder"><a href="#4-3-transforms-RandomOrder" class="headerlink" title="4.3 transforms.RandomOrder"></a>4.3 transforms.RandomOrder</h4><h4 id><a href="#" class="headerlink" title=" "></a> </h4>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/14/Something-about-PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/14/Something-about-PyTorch/" itemprop="url">Something about PyTorch</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-14T14:50:32+08:00">
                2019-08-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Something-about-PyTorch"><a href="#Something-about-PyTorch" class="headerlink" title="Something about PyTorch"></a>Something about PyTorch</h2><ul>
<li><p><code>torchvision.transfroms.Compose(trandforms)</code></p>
<p>将多个transform组合起来使用</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transforms.Compose([</span><br><span class="line">	transfroms.CenterCrop(<span class="number">10</span>), </span><br><span class="line">	transforms.ToTensor(),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
</li>
<li></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/14/parser-action/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/14/parser-action/" itemprop="url">parser-action</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-14T14:33:27+08:00">
                2019-08-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h2><p>举例1：</p>
<p><code>self.parser.add_argument(--&#39;lr_use&#39;, action=&#39;store_true&#39;, default=False, help=&#39;if or not use lr_loss&#39;)</code></p>
<p>当在终端运行的时候，如果不加入<code>--lr_use</code>， 那么程序running的时候，<code>lr_use</code>的值为<code>default:False</code></p>
<p><code>self.parser.add_argument(&#39;--no_flip&#39;, action=&#39;store_false&#39;, help=&#39;....&#39;)</code></p>
<p>当在终端运行的时候，并没有加入<code>no_flip</code>，数据集中的图片并不会翻转，打印出来看到<code>no_flip</code>的值为True</p>
<p><strong>Note:</strong></p>
<p>有default值的时候，running时不声明就为默认值</p>
<p>没有的话，如果是<code>store_false</code>，则默认值是True， 如果是<code>store_true</code>，则默认值是False</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/11/CenterLossTest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/11/CenterLossTest/" itemprop="url">CenterLossTest</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-11T15:26:38+08:00">
                2019-08-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="CenterLossTest"><a href="#CenterLossTest" class="headerlink" title="CenterLossTest"></a>CenterLossTest</h2><ul>
<li><p><code>class torch.nn.PReLU(num_paramenter=1, init=0.25)</code></p>
<p>对输入的每一个元素运用函数 $PReLU(x) = max(0,x) + a*min(0, x)$，<code>a</code>是一个可学习参数。当没有声明时，<code>nn.PReLU()</code>在所有的输入中只有一个参数a；如果是<code>nn.PReLU(nChannels)</code>，a将应用到每个输入。</p>
<p><strong>注意：</strong>当为了表现更加的模型而学习参数a时不要使用权重衰减</p>
<p>参数：</p>
<ul>
<li>num_parameters：需要学习的a的个数，默认等于1</li>
<li>init：a的初始值，默认等于0.25</li>
</ul>
<p>shape：</p>
<ul>
<li>输入：$(N, )$，代表任意数目附加维度</li>
<li>输出：$(N,*)$，与输入拥有同样的shape属性</li>
</ul>
</li>
<li><p><code>nn.Linear(in_features, out_features, bias=True)</code></p>
<p>具体形式为：<code>y = wx + b</code></p>
<p><code>weight = Parameter(torch.Tensor(out_features, in_features))</code></p>
<p><code>bias = Parameter(torch.Tensor(out_features))</code></p>
<p><code>bias</code>如果设置为False，则图层不会学习附加偏差。默认值：True</p>
</li>
<li><p><code>self.v = torch.nn.Parameters()</code></p>
<p>可以把这个函数理解为类型转换函数，讲一个不可训练的类型<code>Tensor</code>转换成可以训练的类型<code>parameter</code>，并将这个<code>parameter</code>绑定到这个<code>module</code>里面(<code>net.parameter()</code>中就有这个绑定的<code>parameter</code>，所以在参数优化的时候可以进行优化的)，所以经过类型转换这个<code>self.v</code>变成了模型的一部分，成为了模型中根据训练可以改动的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化</p>
</li>
<li><p><code>torch.pow()</code></p>
<p>这里对应的矩阵乘法只是每一位上的乘法</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(a)</span><br><span class="line">b = torch.pow(a)</span><br><span class="line">print(b)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output：</span></span><br><span class="line"><span class="string">tensor([[-0.6702,  0.1811],</span></span><br><span class="line"><span class="string">        [-0.7064, -0.3418]])</span></span><br><span class="line"><span class="string">tensor([[0.4491, 0.0328],</span></span><br><span class="line"><span class="string">        [0.4990, 0.1168]])</span></span><br><span class="line"><span class="string">0.4491 = (-0.6702)*(-0.6702)        </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.sum(input, dim, out=None)</code>  <code>----&gt;Tensor</code></p>
<p>返回输入张量给定维度上每行的和。输出形状与输入相同，除了给定维度上为1.</p>
<p>参数：</p>
<ul>
<li>input(Tensor)——输入张量</li>
<li>dim(int)——缩减的维度</li>
<li>out(Tensor, optional)——结果张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(torch.sum(a, <span class="number">1</span>))</span><br><span class="line">print(torch.sum(a))</span><br><span class="line">print(torch.sum(a, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[-0.9850, -0.6207, -0.6559, -0.1220],</span></span><br><span class="line"><span class="string">        [-1.0619,  0.0158, -1.0086,  0.3370],</span></span><br><span class="line"><span class="string">        [ 0.5729, -1.7753,  1.2464, -1.6284],</span></span><br><span class="line"><span class="string">        [-0.3275, -0.5711, -0.6691,  1.2357]])</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">tensor([-2.3836, -1.7177, -1.5843, -0.3320])	# 变成了行向量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor(-6.0177)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[-2.3836],</span></span><br><span class="line"><span class="string">        [-1.7177],</span></span><br><span class="line"><span class="string">        [-1.5843],</span></span><br><span class="line"><span class="string">        [-0.3320]])	# 仍然保持了列向量</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>expand</code></p>
<p>扩展某个size为1的维度。如(2, 2, 1)扩展为(2, 2, 3)</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = x.expand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(y)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[[ 0.3814],</span></span><br><span class="line"><span class="string">         [ 1.4493]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[-0.0204],</span></span><br><span class="line"><span class="string">         [-0.9141]]])</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">tensor([[[ 0.3814,  0.3814,  0.3814],</span></span><br><span class="line"><span class="string">         [ 1.4493,  1.4493,  1.4493]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[-0.0204, -0.0204, -0.0204],</span></span><br><span class="line"><span class="string">         [-0.9141, -0.9141, -0.9141]]])     </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.squeeze()</code></p>
<p>将维度为1的压缩掉。如size为(3, 1, 1, 2)，压缩之后为(3, 2)</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.squeeze())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[[[ 2.2045,  0.2968,  0.2945]]],</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        [[[ 0.2579,  0.9719, -0.8220]]]])</span></span><br><span class="line"><span class="string">tensor([[ 2.2045,  0.2968,  0.2945],</span></span><br><span class="line"><span class="string">        [ 0.2579,  0.9719, -0.8220]])        </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.unsqueeze(input, dim, out=None)</code></p>
<p>返回一个新的张量，对输入的指定位置插入维度1</p>
<p><strong>注意：</strong>返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个</p>
<p>如果dim为负，则会被转换为<code>dim + input.dim() + 1</code></p>
<p>参数：</p>
<ul>
<li>tensor(Tensor)——输入张量</li>
<li>dim(int)——插入维度的索引</li>
<li>out(Tensor, optional)——结果张量</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.randn(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(a)</span><br><span class="line">b = a.unsqueeze(<span class="number">1</span>)</span><br><span class="line">print(b)</span><br><span class="line">print(b.size())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[-0.2329,  0.0805]])</span></span><br><span class="line"><span class="string">tensor([[[-0.2329,  0.0805]]])</span></span><br><span class="line"><span class="string">torch.Size([1, 1, 2])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>t()</code>     转置</p>
</li>
<li><p><code>torch.addmm(beta-1, mat, alpha=1, mat1, mat2, out=None)    ----&gt;Tensor</code></p>
<p>对矩阵mat1和mat2进行矩阵乘操作，矩阵mat加到最终结果。alpha和beta分别是两个矩阵mat1×mat2和mat的比例因子，即$out=(beta * M) + (alpha * mat1 × mat2)$</p>
<p>对类型为FloatTensor或DoubleTensor的输入，beta和alpha必须为实数，否则两个参数必须为整数</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">b = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">c = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line">print(c)</span><br><span class="line">print(a.addmm(<span class="number">1</span>, <span class="number">2</span>, b, c))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[ 1.2774,  0.2344],</span></span><br><span class="line"><span class="string">        [-0.2572,  0.0019]])</span></span><br><span class="line"><span class="string">tensor([[0.4277, 0.8812, 0.7919],</span></span><br><span class="line"><span class="string">        [0.5476, 0.2299, 0.9781]])</span></span><br><span class="line"><span class="string">tensor([[-1.2772, -0.9458],</span></span><br><span class="line"><span class="string">        [ 1.6094,  0.7200],</span></span><br><span class="line"><span class="string">        [ 0.0633,  0.0571]])</span></span><br><span class="line"><span class="string">tensor([[ 3.1216,  0.7847],</span></span><br><span class="line"><span class="string">        [-0.7920, -0.5911]])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.clamp(input, min, max, out=None)   ---&gt;Tensor</code></p>
<p>将输入input张量每个元素都夹紧到区间[min, max]，并返回结果到一个新张量。</p>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">a = torch.randint(low=<span class="number">0</span>, high=<span class="number">10</span>, size=(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">print(a)</span><br><span class="line">a = torch.clamp(a, <span class="number">3</span>, <span class="number">7</span>)</span><br><span class="line">print(a)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[1, 1, 4],</span></span><br><span class="line"><span class="string">        [8, 9, 2]])</span></span><br><span class="line"><span class="string">tensor([[3, 3, 4],</span></span><br><span class="line"><span class="string">        [7, 7, 3]])        </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

<p>$y_{i}=\left{\begin{array}{l}{ min, x_{i} &lt; min} \ {x_{i}, min \leq x_{i} \leq max} \ {max, x_{i} &gt; max}\end{array}\right.$</p>
</li>
<li><p><code>torch.eq()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">outputs = torch.FloatTensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">targets = torch.FloatTensor([[<span class="number">0</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">print(targets.eq(outputs.data))	<span class="comment"># 比较相等</span></span><br><span class="line">print(targets.eq(outputs.data).cpu().sum())	<span class="comment"># 统计相等的个数</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">tensor([[False],</span></span><br><span class="line"><span class="string">        [ True],</span></span><br><span class="line"><span class="string">        [ True]])</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">tensor(2)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>

</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/NormFace/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/NormFace/" itemprop="url">NormFace</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T16:29:48+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="人脸识别：NormFace"><a href="#人脸识别：NormFace" class="headerlink" title="人脸识别：NormFace"></a>人脸识别：NormFace</h2><p>参考链接：<a href="https://blog.csdn.net/wfei101/article/details/82890444" target="_blank" rel="noopener">https://blog.csdn.net/wfei101/article/details/82890444</a></p>
<h3 id="提出问题"><a href="#提出问题" class="headerlink" title="提出问题"></a>提出问题</h3><p>之前的人脸识别工作，在特征比较阶段，通常使用的都是特征的余弦距离</p>
<p>$cos \theta = \frac {a * b} {||a|| ||b||}$</p>
<p>而余弦距离等价于L2归一化后的内积，也等价于L2归一化后的欧氏距离（欧氏距离表示超球面上的弦长，两个向量之间的夹角越大，弦长也越大）</p>
<p>然而，实际上训练的时候用的都是没有L2归一化的内积</p>
<p>关于这一点，可以这样解释，softmax函数是：</p>
<p>$P_{k}= \frac {e^{W_{k}x}} {\sum_{j=0}^{d} e^{W_{j}x}}$</p>
<p>可以理解为$W_{k}$ 和特征向量 $x$的内积越大，x属于第k类概率也就越大，训练过程就是最大化x与其标签项所对应项的权值$W_{label(x)}$的过程</p>
<p><img src="https://img-blog.csdn.net/20180318154156748?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L0ZpcmVfTGlnaHRf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>也就是说在训练时使用的距离度量与在测试时使用的度量是不一样的</p>
<h3 id="测试时是否需要归一化？"><a href="#测试时是否需要归一化？" class="headerlink" title="测试时是否需要归一化？"></a>测试时是否需要归一化？</h3><p>事实证明，进行人脸验证时，使用归一化后的内积或者欧氏距离效果会优于直接计算两个特征向量的内积或者欧氏距离，实验结果如下</p>
<p><img src="https://img-blog.csdn.net/20180318154219273?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L0ZpcmVfTGlnaHRf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<ul>
<li>注意这个Normalization不同于batch normalization，一个是对L2翻书进行归一化，一个是均值归0，方差归1</li>
</ul>
<h3 id="那么是否可以直接在训练时也对特征向量归一化？"><a href="#那么是否可以直接在训练时也对特征向量归一化？" class="headerlink" title="那么是否可以直接在训练时也对特征向量归一化？"></a>那么是否可以直接在训练时也对特征向量归一化？</h3><p>针对上面的问题，作者设计实验，通过归一化Softmax所有的特征和权重来创建一个cosine layer，实验结果是<strong>网络不收敛了</strong>。</p>
<h3 id="本论文要解决的四大问题："><a href="#本论文要解决的四大问题：" class="headerlink" title="本论文要解决的四大问题："></a>本论文要解决的四大问题：</h3><ul>
<li>为什么在测试时必须要归一化？</li>
<li>为什么直接优化余弦相似度会导致网络不收敛？</li>
<li>怎么样使用softmaxloss优化余弦相似度？</li>
<li>既然softmax loss在优化余弦相似度时不能收敛，那么其他的损失函数可以收敛吗？</li>
</ul>
<h3 id="L2归一化"><a href="#L2归一化" class="headerlink" title="L2归一化"></a>L2归一化</h3><h4 id="为什么要归一化"><a href="#为什么要归一化" class="headerlink" title="为什么要归一化"></a>为什么要归一化</h4><p>全连接层特征降至二维的MNIST特征图</p>
<p>左图中，f2f3是同一类的两个特征，但是可以看到f1和f2的距离明显小于f2和f3的距离，因此，加入不对特征进行归一化再比较距离的话，可能就会误判f1f2为同一类</p>
<p><img src="https://img-blog.csdn.net/20180318154315787?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L0ZpcmVfTGlnaHRf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<h4 id="为什么特征会呈辐射状分布？"><a href="#为什么特征会呈辐射状分布？" class="headerlink" title="为什么特征会呈辐射状分布？"></a>为什么特征会呈辐射状分布？</h4><p>Softmax实际上是一种（Soft）软的max（最大化）操作,考虑Softmax的概率</p>
<p>$P_{i}(f)= \frac {e^{W_{i}^{T}f}} {\sum_{j=1}^{n} e^{W_{j}^{T}f}}$</p>
<p>假设是一个十个分类问题，那么每个类都会对应一个权值向量W0,W1…W9,某个特征f会被分为哪一类，<strong>取决f和哪一个权值向量的内积最大</strong>。</p>
<p>也就是说，靠近W0的向量会被归为第一类，靠近W1的向量会归为第二类，以此类推。网络在训练过程中，为了使得各个分类更明显，会让各个权值向量W逐渐分散开，相互之间有一定的角度，而<strong>靠近某一权值向量的特征就会被归为相应的类别</strong>，因此特征最终会呈辐射状分布。</p>
<h4 id="如果添加了偏置，结果会是怎么样的呢？"><a href="#如果添加了偏置，结果会是怎么样的呢？" class="headerlink" title="如果添加了偏置，结果会是怎么样的呢？"></a>如果添加了偏置，结果会是怎么样的呢？</h4><p>$L_{s}= - \frac{1}{m} \sum_{i = 1}^{m} log\frac {e^{W_{y_{i}}^{T} f_{i} + b_{y_{i}}}} {\sum_{j=1}^{n} e^{W_{j}^{T}f_{i} + b_{j}}}$</p>
<p>如果添加了骗纸，不同类的b不同，则会造成有的类w角度近似相等，而依据b来区分的情况。如下图：</p>
<p><img src="https://img-blog.csdn.net/20180318154405545?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L0ZpcmVfTGlnaHRf/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>在这种情况下，如果再对w进行归一化，那个中间这些类会散步在单位圆上各个方向，造成错误分类。</p>
<p>所以添加偏置对我们通过余弦距离来分类没有帮助，弱化了网络的学习能力，所以我们不添加偏置</p>
<h4 id="网络为何不收敛？"><a href="#网络为何不收敛？" class="headerlink" title="网络为何不收敛？"></a>网络为何不收敛？</h4>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/卷积/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/卷积/" itemprop="url">卷积在图像处理中的应用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T15:09:44+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>卷积的用途：</p>
<p>将图像相邻子区域的像素值与卷积核执行“卷积”操作，可以获取相邻数据之间的统计关系，从而可挖掘图像中的某些重要特征。</p>
<p>比较抽象…所以特征到底是什么？用图像来形象的说明一下</p>
<p><img src="https://pic4.zhimg.com/v2-b0f198aa46872eb91cb7f1f593073f13_b.jpg" alt="img"></p>
<p>下面我们简单介绍一下常用的“久经考验”的卷积核。<br>（1）同一化核（Identity）。从图13-6可见，这个滤波器什么也没有做，卷积后得到的图像和原图一样。因为这个核只有中心点的值是1。邻域点的权值都是0，所以对滤波后的取值没有任何影响。<br>（2）边缘检测核（Edge Detection），也称为高斯-拉普拉斯算子。需要注意的是，这个核矩阵的元素总和为0（即中间元素为8，而周围8个元素之和为-8），所以滤波后的图像会很暗，而只有边缘位置是有亮度的。<br>（3）图像锐化核（Sharpness Filter）。图像的锐化和边缘检测比较相似。首先找到边缘，然后再把边缘加到原来的图像上面，如此一来，就强化了图像的边缘，使得图像看起来更加锐利。<br>（4）均值模糊（Box Blur /Averaging）。这个核矩阵的每个元素值都是1，它将当前像素和它的四邻域的像素一起取平均，然后再除以9。均值模糊比较简单，但图像处理得不够平滑。因此，还可以采用高斯模糊核（Gaussian Blur），这个核被广泛用在图像降噪上。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/center-loss/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/center-loss/" itemprop="url">center loss</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T14:31:07+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Center-Loss"><a href="#Center-Loss" class="headerlink" title="Center Loss"></a>Center Loss</h2><h3 id="一-简介"><a href="#一-简介" class="headerlink" title="一. 简介"></a>一. 简介</h3><p>论文链接：<a href="http://ydwen.github.io/papers/WenECCV16.pdf" target="_blank" rel="noopener">http://ydwen.github.io/papers/WenECCV16.pdf</a> </p>
<h3 id="二-为什么要使用Center-Loss？"><a href="#二-为什么要使用Center-Loss？" class="headerlink" title="二. 为什么要使用Center Loss？"></a>二. 为什么要使用Center Loss？</h3><p>简单的来说，我们在做分类的时候，不光需要学得separable的特征，更想要这些特征是discriminative的，这就意味着我们需要在loss上做更多的约束。</p>
<p>仅仅使用softmax作为监督信号的输出处理就只能做到seperable而不是discriminative，如下图:</p>
<p><img src="https://img-blog.csdn.net/20180727140845416?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<h3 id="三-如何使学到的特征差异化更大——Center-Loss"><a href="#三-如何使学到的特征差异化更大——Center-Loss" class="headerlink" title="三. 如何使学到的特征差异化更大——Center Loss"></a>三. 如何使学到的特征差异化更大——Center Loss</h3><p>融合Softmax Loss与Center loss</p>
<p><strong>Softmax Loss（保证类之间的feature距离最大）与Center Loss（保证类内的feature距离最小，更接近于类中心）</strong></p>
<p><img src="https://img-blog.csdn.net/20180727150130651?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p><img src="https://img-blog.csdn.net/2018072715022915?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>m是mini-batch、n是class。在Lc公式中有一个缺陷，就是$C_{y_{i}}$是i这个样本对应的类别yi所属于的类中心C∈ Rd，d代表d维。</p>
<p>理想情况下，Cyi需要随着学到的feature变化而实时更新，也就是要在每一次迭代中用整个数据集的feature来算每个类的中心。</p>
<p>但这显然不现实，做以下两个修改：</p>
<p>1、由整个训练集更新center改为mini-batch更改center  </p>
<p>2、避免错误分类的样本的干扰，使用scalar α 来控制center的学习率  </p>
<p>因此求算梯度的公式如下：</p>
<p><img src="https://img-blog.csdn.net/20180727153311160?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>即：当yi = j，也就是mini-batch中某一个sample是对应要更新的那一个类的center的时候就累加起来除以某类的个数+1。</p>
<p><img src="https://img-blog.csdn.net/2018072715370270?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>最终loss联立起来如上图，λ用于平衡softmax loss与center loss，越大则区分度 越大，如下图效果：</p>
<p><img src="https://img-blog.csdn.net/20180727150702547?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<h3 id="四-Center-Loss的实现"><a href="#四-Center-Loss的实现" class="headerlink" title="四. Center Loss的实现"></a>四. Center Loss的实现</h3><p>pytorch实现：<a href="https://github.com/jxgu1016/MNIST_center_loss_pytorch" target="_blank" rel="noopener">https://github.com/jxgu1016/MNIST_center_loss_pytorch</a></p>
<ul>
<li>网络结构</li>
</ul>
<p><img src="https://img-blog.csdn.net/20180727162522989?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>即在特征层输出（classification前最后一层）引入center loss：</p>
<p><img src="https://img-blog.csdn.net/20180727162755579?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0x1Y2lmZXJfenpx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<h4 id="fully-connected-和-local-connected"><a href="#fully-connected-和-local-connected" class="headerlink" title="fully-connected 和 local-connected"></a>fully-connected 和 local-connected</h4><h5 id="判断fully-connected的方法："><a href="#判断fully-connected的方法：" class="headerlink" title="判断fully-connected的方法："></a>判断fully-connected的方法：</h5><ul>
<li><p>对于neuron的链接（点对点的链接）都是fully connected（这里其实就是MLP）</p>
</li>
<li><p>对于有filter的network，不是看filter的size，而是看output的feature map的size。如果output feature map的size还是1 * 1 * N的话，这个layer就是fully connected layer</p>
<p><strong>解释第二个判断方法：</strong></p>
<ul>
<li>1 * 1的filter size不一定是fully connected。比如input size是10 * 10 * 100， filter size是1 * 1 * 100， 重复50次，则该layer的总weights是：1 * 1 * 100 * 50</li>
<li>1 * 1的filter size如果要是 fully connected， 则input size必须是1 * 1</li>
<li>input size是10x10的时候却是fully connected的情况：这里我们的output size肯定是1x1，且我们的filter size肯定是10x10。</li>
</ul>
<p><strong>总结：filter size等于input size则是fully connected</strong></p>
</li>
</ul>
<p>综上：</p>
<ul>
<li>fully connected没有weight share</li>
<li>对于neuron的连接（点对点的链接）都是fully connected（MLP——多层感知器）</li>
<li>Convolution中当filter size等于input size时，就是fully connected，此时的output size为1 * 1 * N</li>
<li>当1 *1不等于input size时，1 * 1一样具备weights share的能力。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/softmax-and-softmax-loss-and-BP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/softmax-and-softmax-loss-and-BP/" itemprop="url">softmax and softmax-loss and BP</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T13:57:43+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/" target="_blank" rel="noopener">http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/</a> </p>
<p><img src="https://img-blog.csdn.net/20170504203817251?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>联想逻辑回归的重要公式就是<img src="https://img-blog.csdn.net/20170504203942767?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img">，得到预测结果，然后再经过sigmoid转换成0到1的概率值，而在softmax中则通过取exponential的方式并进行归一化得到某个样本属于某类的概率。非负的意义不用说，就是避免正负值抵消。</p>
<p><img src="https://img-blog.csdn.net/20170504204409994?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>逻辑回归的推导可以用最大似然或最小损失函数，本质是一样的，可以简单理解成加了一个负号，这里的y指的是真实类别。注意下softmax-loss可以看做是softmax和multinomial logistic loss两步，正如上述所写公式，把变量展开即softmax-loss。</p>
<p><img src="https://img-blog.csdn.net/20170504204847454?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>原博客的重点在于介绍softmax-loss是分成两步还是一步到位比较好，而我这则重点说下BP。上面这个神经网络的图应该不陌生，这个公式也是在逻辑回归的核心（通过迭代得到w，然后在测试时按照上面这个公式计算类别概率）</p>
<p><img src="https://img-blog.csdn.net/20170504205237865?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>这里第一个公式是损失函数对权重w求导，其实就是梯度，红色那部分可以看前面O是怎么算出来的，就知道其导数的形式非常简单，就是输入I。蓝色部分就是BP的核心，回传就是通过这个达到的，回传的东西就是损失函数对该层输出的导数，只有把这个往前回传，才能计算前面的梯度。所以回传的不是对权重的求导，对每层权重的求导的结果会保留在该层，等待权重更新时候使用。具体看上面最后一个公式。</p>
<p><img src="https://img-blog.csdn.net/20170504210106340?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>这部分的求导：l(y,z)函数是log函数，log（x）函数求导是取1/x，后面的那个数是zy对zk的导数，当k=y时，那就是1，k不等于y时就是两个不同的常数求导，就是0。</p>
<p><img src="https://img-blog.csdn.net/20170504210625060?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxNDM4MDE2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>这一部分就是把softmax-loss分成两步来做，第一个求导可以先找到最前面l(y,o)的公式，也是log函数，所以求导比较简单。第二个求导也是查看前面Oi的公式，分母取平方的那种求导。最后链式相乘的结果和原来合并算的结果一样。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://woojoo520.github.io/2019/08/09/PyTorch-中的-dim/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mariana">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Celery Fairy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/09/PyTorch-中的-dim/" itemprop="url">PyTorch 中的 dim</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-09T10:24:23+08:00">
                2019-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="PyTorch中的dim"><a href="#PyTorch中的dim" class="headerlink" title="PyTorch中的dim"></a>PyTorch中的dim</h2><h3 id="dim概念"><a href="#dim概念" class="headerlink" title="dim概念"></a>dim概念</h3><p>dim的不同值表示不同维度。特别的在dim=0表示二维中的行，dim=1在二维矩阵中表示行。广泛的来说，我们不管一个矩阵是几维的，比如一个矩阵维度如下：${d_{0},d_{1},…,d_{n-1}}$，那么dim=0就表示对应到$d_{0}$也就是第一个维度，dim=1,表示对应到$d_{1}$也就是第二个维度，以此类推</p>
<h3 id="dim在函数中的作用"><a href="#dim在函数中的作用" class="headerlink" title="dim在函数中的作用"></a>dim在函数中的作用</h3><h4 id="例一-torch-argmax"><a href="#例一-torch-argmax" class="headerlink" title="例一. torch.argmax()"></a>例一. torch.argmax()</h4><p>函数中dim表示该维度会消失。</p>
<p>这个消失是什么意思？官方英文解释是：dim (int) – the dimension to reduce.</p>
<p>我们知道argmax就是得到最大值的序号索引，对于一个维度为$(d_0,d_1)$的矩阵来说，我们想要求每一行中最大数的在该行中的列号，最后我们得到的就是一个维度为$(d_0,1)$的一矩阵。这时候，列就要消失了。</p>
<p>因此，我们想要求每一行最大的列标号，我们就要指定dim=1，表示我们不要列了，保留行的size就可以了。<br>假如我们想求每一列的最大行标，就可以指定dim=0，表示我们不要行了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">a = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a.size())</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">b = torch.argmax(a, dim=<span class="number">1</span>)</span><br><span class="line">print(b)</span><br><span class="line">print(b.size())</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output:</span></span><br><span class="line"><span class="string">torch.Size([3, 4])</span></span><br><span class="line"><span class="string">tensor([[0.9120, 0.4805, 0.6701, 0.5446],</span></span><br><span class="line"><span class="string">        [0.6273, 0.1295, 0.3416, 0.2213],</span></span><br><span class="line"><span class="string">        [0.6068, 0.8448, 0.8452, 0.4931]])</span></span><br><span class="line"><span class="string">tensor([0, 0, 2])</span></span><br><span class="line"><span class="string">torch.Size([3])</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 可以看到，指定dim=1时，列的size没有了</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.argmax(input, dim=<span class="literal">None</span>, keepdim=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>返回指定维度最大的序号</p>
<p>dim给定的定义是：the dimention to reduce.也就是吧dim这个维度的，变成这个维度的最大值</p>
<p>如果上面的代码改成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">b = torch.argmax(a, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">output：</span></span><br><span class="line"><span class="string">tensor([[0],</span></span><br><span class="line"><span class="string">		[0],</span></span><br><span class="line"><span class="string">        [2]])</span></span><br><span class="line"><span class="string">torch.Size([3, 1])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Mariana</p>
              <p class="site-description motion-element" itemprop="description">a study blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mariana</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
